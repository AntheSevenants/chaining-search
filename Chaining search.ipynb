{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaining search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library functions: Search\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T16:42:06.788517Z",
     "start_time": "2019-01-16T16:42:06.541144Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import urllib\n",
    "from IPython.display import FileLink, FileLinks\n",
    "AVAILABLE_CORPORA = ['chn', 'opensonar', 'zeebrieven', 'gysseling', 'nederlab']\n",
    "RECORDS_PER_PAGE = 1000\n",
    "\n",
    "# Get rid of ellipsis in display (otherwise relevant data might not be shown)\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "\n",
    "# Search methods\n",
    "\n",
    "def search_corpus_allwords(corpus):\n",
    "    query = r'[word=\".*\"]'\n",
    "    return search_corpus(query,corpus)\n",
    "\n",
    "def search_corpus(query, corpus, start_position=1):\n",
    "    if corpus not in AVAILABLE_CORPORA:\n",
    "        raise ValueError(\"Unknown corpus: \" + corpus)\n",
    "    # Do request to federated content search corpora, so we get same output format for every corpus\n",
    "    url = \"http://portal.clarin.inl.nl/fcscorpora/clariah-fcs-endpoints/sru?operation=searchRetrieve&queryType=fcs&maximumRecords=\" + str(RECORDS_PER_PAGE) + \"&startRecord=\" + str(start_position) + \"&x-fcs-context=\" + corpus + \"&query=\" + urllib.parse.quote(query)\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    response_text = response.text    \n",
    "    df, next_page = parse_xml(response_text)\n",
    "    # If there are next pages, call search_corpus recursively\n",
    "    if next_page > 0:\n",
    "        df_more = search_corpus(query,corpus, start_position=next_page)\n",
    "        df = df.append(df_more, ignore_index=True)\n",
    "    # show message out of xml, if some error has occured (prevents empty output)\n",
    "    show_error_if_any(response_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "def search_corpus_multiple(queries, corpus):\n",
    "    result_dict = {}\n",
    "    for query in queries:\n",
    "        result_dict[query] = search_corpus(query,corpus)\n",
    "    return result_dict\n",
    "   \n",
    "\n",
    "def search_lexicon_allwords(lexicon):\n",
    "    query = lexicon_query_allwords(lexicon)\n",
    "    return search_lexicon(query, lexicon)\n",
    "\n",
    "def search_lexicon(query, lexicon):\n",
    "    endpoint = \"http://172.16.4.56:8890/sparql\"\n",
    "    if (lexicon==\"diamant\"):\n",
    "        endpoint = \"http://svprre02:8080/fuseki/tdb/sparql\"\n",
    "    \n",
    "    # Accept header is needed for virtuoso, it isn't otherwise!\n",
    "    response = requests.post(endpoint, data={\"query\":query}, headers = {\"Accept\":\"application/sparql-results+json\"})\n",
    "        \n",
    "    response_json = json.loads(response.text)\n",
    "    records_json = response_json[\"results\"][\"bindings\"]\n",
    "    records_string = json.dumps(records_json)    \n",
    "    df = pd.read_json(records_string, orient=\"records\")\n",
    "    \n",
    "    # make sure cells containing NULL are added too, otherwise we'll end up with ill-formed data\n",
    "    # TODO: maybe this can be replaced by:\n",
    "    # df = df.fillna('')\n",
    "    df = df.applymap(lambda x: '' if pd.isnull(x) else x[\"value\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Processing methods\n",
    "\n",
    "def column_difference(df_column1, df_column2):\n",
    "    set_df1 = set(df_column1)\n",
    "    set_df2 = set(df_column2)\n",
    "    diff_left = set_df1.difference(set_df2)\n",
    "    diff_right = set_df2.difference(set_df1)\n",
    "    intersec = set_df1.intersection(set_df2)\n",
    "    return diff_left, diff_right, intersec\n",
    "\n",
    "def diamant_get_synonyms(df):\n",
    "    # Depending on the result type, we return the lemma or the definition text\n",
    "    lemmas = set(df[df[\"inputMode\"]==\"defText\"][\"n_ontolex_writtenRep\"])\n",
    "    defTexts = set(df[df[\"inputMode\"]==\"lemma\"][\"n_syndef_definitionText\"])\n",
    "    return lemmas|defTexts\n",
    "\n",
    "def parse_xml(text):\n",
    "    # TODO: should we secure against untrusted XML?\n",
    "    root = ET.fromstring(text)\n",
    "    records = []\n",
    "    n_words_in_hit = 0\n",
    "    computed_nwih = False\n",
    "    for entry in root.iter(\"{http://clarin.eu/fcs/resource}ResourceFragment\"):\n",
    "        for dataView in entry.findall(\"{http://clarin.eu/fcs/resource}DataView\"):\n",
    "            # We only take into account hits, ignore metadata and segmenting dataViews\n",
    "            if (dataView.get(\"type\")==\"application/x-clarin-fcs-hits+xml\"):\n",
    "                result = dataView.find(\"{http://clarin.eu/fcs/dataview/hits}Result\")\n",
    "                left_context = result.text if result.text is not None else ''\n",
    "                hits = list(result)\n",
    "                if len(hits)==0:\n",
    "                    print([w for w in result.itertext()])\n",
    "                    print(\"not hit in kwic, skip\")\n",
    "                    continue\n",
    "                last_hit = hits[-1]\n",
    "                right_context = last_hit.tail if last_hit.tail is not None else ''\n",
    "                hit_words = [hit.text for hit in hits]\n",
    "                \n",
    "                if not computed_nwih:\n",
    "                    n_words_in_hit = len(hits)\n",
    "                    computed_nwih=True\n",
    "                kwic = [left_context] + hit_words + [right_context]\n",
    "                records.append(kwic)\n",
    "    columns = [\"left context\"] + [\"word \" + str(n) for n in range(n_words_in_hit)] + [\"right context\"]\n",
    "    \n",
    "    next_pos = 0\n",
    "    next_record_position = root.find(\"{http://docs.oasis-open.org/ns/search-ws/sruResponse}nextRecordPosition\")\n",
    "    if (next_record_position is not None):\n",
    "        next_pos = int(next_record_position.text)\n",
    "        \n",
    "    return pd.DataFrame(records, columns = columns), next_pos\n",
    "\n",
    "def show_error_if_any(text):\n",
    "    # get error message out of xml and print it on screen\n",
    "    root = ET.fromstring(text)\n",
    "    msgs = []\n",
    "    for diagnostic in root.iter(\"{http://docs.oasis-open.org/ns/search-ws/diagnostic}diagnostic\"):\n",
    "        for msg in diagnostic.findall(\"{http://docs.oasis-open.org/ns/search-ws/diagnostic}message\"):\n",
    "            msg_text = msg.text if msg.text is not None else ''\n",
    "            msgs.append(msg_text)\n",
    "    if len(msgs) > 0:\n",
    "        print(\"; \".join(msgs))\n",
    "\n",
    "# View methods\n",
    "\n",
    "# results: dict of df's\n",
    "# labels: list of label corresponding to the df's in results\n",
    "def view_multiple_results(results, labels):\n",
    "    assert len(labels)==len(results)\n",
    "    for n,query in enumerate(results):\n",
    "        df = results[query]\n",
    "        if not df.empty:\n",
    "            display(HTML('Resultaten voor <b>' + labels[n] + \"</b>:\"))\n",
    "            display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library functions: UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T16:42:06.810560Z",
     "start_time": "2019-01-16T16:42:06.790502Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from pathlib import Path\n",
    "from IPython.display import Javascript\n",
    "DEFAULT_QUERY = r'[lemma=\"boek\" & pos=\"verb\"]' #r'[lemma=\"boeken\" pos=\"verb\"]'\n",
    "DEFAULT_CORPUS = \"chn\"\n",
    "\n",
    "def create_run_cell_ui(cell_id):\n",
    "    executebutton = widgets.Button(\n",
    "        description='Uitvoeren',\n",
    "        disabled=False,\n",
    "        button_style='info', \n",
    "        tooltip='Voer de volgende code uit',  \n",
    "        icon=''\n",
    "    )\n",
    "    executebutton.cell_id = cell_id\n",
    "    executebutton.on_click( run_cell )    \n",
    "    executebuttonBox = widgets.HBox([executebutton])\n",
    "    display(executebuttonBox)    \n",
    "\n",
    "def run_cell(button):\n",
    "    cell_id = button.cell_id\n",
    "    # https://stackoverflow.com/questions/47567834/execute-a-jupyter-notebook-cell-programmatically\n",
    "    Javascript(\"Jupyter.notebook.execute_cells([\"+cell_id+\"])\")\n",
    "\n",
    "def create_corpus_ui():\n",
    "    # Create UI elements\n",
    "    corpusQueryField = widgets.Text(description=\"<b>CQL query:</b>\", value=DEFAULT_QUERY)\n",
    "    corpusField = widgets.Dropdown(\n",
    "        options=AVAILABLE_CORPORA,\n",
    "        value=DEFAULT_CORPUS,\n",
    "        description='<b>Corpus:</b>',\n",
    "    )\n",
    "    '''corpusSearchButton = widgets.Button(\n",
    "        description='Search',\n",
    "        button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Search',\n",
    "    )\n",
    "    # Handle events\n",
    "    corpusSearchButton.on_click(corpus_search)'''\n",
    "    \n",
    "    # Stack UI elements in vertical box and display\n",
    "    corpusUiBox = widgets.VBox([corpusQueryField,corpusField])\n",
    "    display(corpusUiBox)\n",
    "    \n",
    "    # Return fields, so their contents are accessible from the global namespace of the Notebook\n",
    "    return corpusQueryField, corpusField\n",
    "\n",
    "def create_lexicon_ui():\n",
    "    DEFAULT_SEARCHWORD = 'boek'\n",
    "    DEFAULT_LEXICON = \"diamant\"\n",
    "\n",
    "    # Create UI elements\n",
    "    searchWordField = widgets.Text(description=\"<b>Word:</b>\", value=DEFAULT_SEARCHWORD)\n",
    "    lexiconField = widgets.Dropdown(\n",
    "        options=['anw', 'celex', 'diamant', 'duelme', 'molex'],\n",
    "        value=DEFAULT_LEXICON,\n",
    "        description='<b>Lexicon:</b>',\n",
    "    )\n",
    "    '''lexSearchButton = widgets.Button(\n",
    "        description='Search',\n",
    "        button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Search',\n",
    "    )\n",
    "    # Handle events\n",
    "    lexSearchButton.on_click(lexicon_search)'''\n",
    "    # Stack UI elements in vertical box and display\n",
    "    lexUiBox = widgets.VBox([searchWordField,lexiconField])\n",
    "    display(lexUiBox)\n",
    "    return searchWordField, lexiconField\n",
    "\n",
    "def create_save_results_ui(df):\n",
    "    # build ui for saving results\n",
    "    DEFAULT_FILENAME = 'mijn_resultaten.csv'\n",
    "    saveResultsCaption = widgets.Label(value='Sla uw resultaten op:')\n",
    "    fileNameField = widgets.Text(value=DEFAULT_FILENAME)\n",
    "    savebutton = widgets.Button(\n",
    "        description='Bestand opslaan',\n",
    "        disabled=False,\n",
    "        button_style='warning', \n",
    "        tooltip=DEFAULT_FILENAME,  # trick to pass filename to button widget\n",
    "        icon=''\n",
    "    )\n",
    "    # inject dataframe into button object\n",
    "    savebutton.df = df\n",
    "    # when the user types a new filename, it will be passed to the button tooltip property straight away\n",
    "    fileNameLink = widgets.jslink((fileNameField, 'value'), (savebutton, 'tooltip'))\n",
    "    # click event with callback\n",
    "    savebutton.on_click( save_results )    \n",
    "    saveResultsBox = widgets.HBox([saveResultsCaption, fileNameField, savebutton])\n",
    "    display(saveResultsBox)    \n",
    "    \n",
    "def save_results(button):\n",
    "    # The result files can be saved locally or on the server:\n",
    "    # If result files are to be offered as downloads, set to True; otherwise set to False    \n",
    "    fileDownloadable = False\n",
    "    # specify paths here, if needed:\n",
    "    filePath_onServer = ''  # could be /path/to\n",
    "    filePath_default = ''\n",
    "    # compute full path given chose mode\n",
    "    fileName = (filePath_onServer if fileDownloadable else filePath_default ) + button.tooltip\n",
    "        \n",
    "    button.df.to_csv( fileName, index=False)\n",
    "    # confirm it all went well\n",
    "    print(button.tooltip + \" opgeslagen\")    \n",
    "    button.button_style = 'success'\n",
    "    button.icon = 'check'\n",
    "    # trick: https://stackoverflow.com/questions/31893930/download-csv-from-an-ipython-notebook\n",
    "    if (fileDownloadable):\n",
    "        downloadableFiles = FileLinks(filePath_onServer)\n",
    "        display(downloadableFiles)\n",
    "\n",
    "    \n",
    "def create_load_results_ui():\n",
    "    # https://stackoverflow.com/questions/9319317/quick-and-easy-file-dialog-in-python\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    # build ui for loading saved results\n",
    "    DEFAULT_FILENAME = 'mijn_resultaten.csv'\n",
    "    loadResultsCaption = widgets.Label(value='Laad uw opgeslagen resultaten op:')\n",
    "    loadbutton = widgets.Button(\n",
    "        description='Bestand laden',\n",
    "        disabled=False,\n",
    "        button_style='warning', \n",
    "        tooltip=DEFAULT_FILENAME,  # trick to pass filename to button widget\n",
    "        icon='check'\n",
    "    )\n",
    "    # click event with callback\n",
    "    df = loadbutton.on_click( load_results )    \n",
    "    loadResultsBox = widgets.HBox([loadResultsCaption, loadbutton])\n",
    "    display(loadResultsBox)  \n",
    "    return df\n",
    "\n",
    "def load_results(button):\n",
    "    # https://stackoverflow.com/questions/9319317/quick-and-easy-file-dialog-in-python\n",
    "    filepath = filedialog.askopenfilename(initialdir=\"/\", title=\"Select file\")\n",
    "    df = load_dataframe(filepath)    \n",
    "    # confirm it all went well\n",
    "    button.button_style = 'success'\n",
    "    button.icon = 'check'\n",
    "    return df\n",
    "\n",
    "def load_dataframe(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(filepath + \" ingelezen\")    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Library functions: Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T16:42:06.826492Z",
     "start_time": "2019-01-16T16:42:06.812237Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def containsRegex(word):\n",
    "    return ( word.find('^')>-1 or\n",
    "            word.find('$')>-1 or \n",
    "            re.match(\"\\(.+?\\)\", word) or\n",
    "            re.match(\"\\[.+?\\]\", word) or\n",
    "            re.match(\"[\\+*]\", word) )\n",
    "                     \n",
    "def lexicon_query(word, pos, lexicon):\n",
    "    if (lexicon==\"anw\"):\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart = \"\"\"FILTER ( regex(?lemma, \\\"\"\"\"+word+\"\"\"\\\") || regex(?definition, \\\"\"\"\"+word+\"\"\"\\\") ) . \"\"\"\n",
    "        if (exactsearch == True):\n",
    "              subpart =  \"\"\"\n",
    "                { { ?lemId rdfs:label ?lemma .  \n",
    "                values ?lemma { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } }                 \n",
    "                UNION\n",
    "                { ?definitionId lemon:value ?definition .\n",
    "                values ?definition { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } } .\n",
    "                \"\"\"               \n",
    "        query = \"\"\"PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "                  PREFIX anw: <http://rdf.ivdnt.org/lexica/anw>\n",
    "                  PREFIX anwsch: <http://rdf.ivdnt.org/schema/anw/>\n",
    "                  PREFIX lemon: <http://lemon-model.net/lemon#>\n",
    "                  \n",
    "                  SELECT ?lemId ?lemma ?writtenForm ?definition concat('', ?definitionComplement) as ?definitionComplement\n",
    "                  FROM <http://rdf.ivdnt.org/lexica/anw>\n",
    "                  WHERE {\n",
    "                      ?lemId rdfs:label ?lemma .\n",
    "                      ?lemId ontolex:sense ?senseId .\n",
    "                      ?senseId lemon:definition ?definitionId .\n",
    "                      ?definitionId lemon:value ?definition .\n",
    "                      OPTIONAL { ?definitionId anwsch:definitionComplement ?definitionComplement .}\n",
    "                      OPTIONAL { ?lemId ontolex:canonicalForm ?lemCFId . \n",
    "                          ?lemCFId ontolex:writtenRepresentation ?writtenForm . }\n",
    "                      \"\"\"+subpart+\"\"\"\n",
    "                      }\"\"\"\n",
    "    elif (lexicon==\"diamant\"):\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart1 = \"\"\"?n_form ontolex:writtenRep ?n_ontolex_writtenRep . \n",
    "            FILTER regex(?n_ontolex_writtenRep, \\\"\"\"\"+word+\"\"\"\\\") . \"\"\"\n",
    "        subpart2 = \"\"\"?n_syndef diamant:definitionText ?n_syndef_definitionText .  \n",
    "            FILTER regex(?n_ontolex_writtenRep, \\\"\"\"\"+word+\"\"\"\\\") . \"\"\"\n",
    "        if (exactsearch == True):\n",
    "            subpart1 =  \"\"\"\n",
    "                { ?n_form ontolex:writtenRep ?n_ontolex_writtenRep . \n",
    "                values ?n_ontolex_writtenRep { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } \n",
    "                \"\"\"                \n",
    "            subpart2 = \"\"\"\n",
    "                { ?n_syndef diamant:definitionText ?n_syndef_definitionText . \n",
    "                values ?n_syndef_definitionText { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } \n",
    "                \"\"\"\n",
    "        query = \"\"\"\n",
    "        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "        prefix prov: <http://www.w3.org/ns/prov#>\n",
    "        prefix diamant: <http://rdf.ivdnt.org/schema/diamant#>\n",
    "        prefix lexinfo: <http://www.lexinfo.net/ontology/2.0/lexinfo#>\n",
    "        prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "        prefix lemon: <http://lemon-model.net/lemon#>\n",
    "        prefix ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "        prefix ud: <http://universaldependencies.org/u/pos/>\n",
    "        prefix skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "        prefix dcterms: <http://purl.org/dc/terms/>\n",
    "        prefix dc: <http://purl.org/dc/terms/>\n",
    "\n",
    "        select ?n_entry ?n_form ?n_ontolex_writtenRep ?n_syndef ?n_sensedef ?n_sensedef_definitionText ?n_syndef_definitionText ?n_sense ?inputMode ?wy_f_show ?wy_t_show\n",
    "        where\n",
    "        {\n",
    "        graph ?g\n",
    "        {\n",
    "        {\n",
    "            \"\"\" + subpart1 + \"\"\"\n",
    "            { ?n_entry a ontolex:LexicalEntry} .\n",
    "            { ?n_form a ontolex:Form} .\n",
    "            { ?n_sense a ontolex:LexicalSense} .\n",
    "            { ?n_syndef a diamant:SynonymDefinition} .\n",
    "            { ?n_sensedef a lemon:SenseDefinition} .\n",
    "            { ?n_syndef diamant:definitionText ?n_syndef_definitionText } .\n",
    "            { ?n_sensedef diamant:definitionText ?n_sensedef_definitionText } .\n",
    "            { ?n_entry ontolex:canonicalForm ?n_form } .\n",
    "            { ?n_entry ontolex:sense ?n_sense } .\n",
    "            { ?n_sense lemon:definition ?n_syndef } .\n",
    "            { ?n_sense lemon:definition ?n_sensedef } .\n",
    "              ?n_sense diamant:attestation ?n_attest_show .\n",
    "              ?n_sense diamant:attestation ?n_attest_filter .\n",
    "              ?n_attest_show diamant:text ?n_q_show .\n",
    "              ?n_attest_filter diamant:text ?n_q_filter .\n",
    "              ?n_attest_show a diamant:Attestation .\n",
    "              ?n_attest_filter a diamant:Attestation .\n",
    "              ?n_q_filter a diamant:Quotation .\n",
    "              ?n_q_show a diamant:Quotation .\n",
    "              ?n_q_filter diamant:witnessYearFrom ?wy_f_filter .\n",
    "              ?n_q_filter diamant:witnessYearTo ?wy_t_filter .\n",
    "              ?n_q_show diamant:witnessYearFrom ?wy_f_show .\n",
    "              ?n_q_show diamant:witnessYearTo ?wy_t_show .\n",
    "              FILTER (xsd:integer(?wy_f_show) >= 1200)\n",
    "              FILTER (xsd:integer(?wy_t_show) >= 1200)\n",
    "              FILTER (xsd:integer(?wy_f_show) <= 2018)\n",
    "              FILTER (xsd:integer(?wy_t_show) <= 2018)\n",
    "            { bind(\"lemma\" as ?inputMode) } .\n",
    "            } UNION\n",
    "          {\n",
    "            \"\"\" + subpart2 + \"\"\"\n",
    "            { ?n_sense a ontolex:LexicalSense} .\n",
    "            { ?n_syndef a diamant:SynonymDefinition} .\n",
    "            { ?n_sensedef a lemon:SenseDefinition} .\n",
    "            { ?n_form a ontolex:Form} .\n",
    "            { ?n_form ontolex:writtenRep ?n_ontolex_writtenRep } .  { ?n_entry a ontolex:LexicalEntry} .\n",
    "            { ?n_entry ontolex:sense ?n_sense } .\n",
    "            { ?n_sense lemon:definition ?n_syndef } .\n",
    "            { ?n_sense lemon:definition ?n_sensedef } .\n",
    "            { ?n_sensedef diamant:definitionText ?n_sensedef_definitionText } .\n",
    "            { ?n_entry ontolex:canonicalForm ?n_form } .\n",
    "            ?n_sense diamant:attestation ?n_attest_show .\n",
    "            ?n_sense diamant:attestation ?n_attest_filter .\n",
    "            ?n_attest_filter diamant:text ?n_q_filter .\n",
    "            ?n_attest_show diamant:text ?n_q_show .\n",
    "            ?n_q_filter diamant:witnessYearFrom ?wy_f_filter .\n",
    "            ?n_q_filter diamant:witnessYearTo ?wy_t_filter .\n",
    "            ?n_q_show diamant:witnessYearFrom ?wy_f_show .\n",
    "            ?n_q_show diamant:witnessYearTo ?wy_t_show .\n",
    "            ?n_attest_show a diamant:Attestation .\n",
    "            ?n_attest_filter a diamant:Attestation .\n",
    "            ?n_q_filter a diamant:Quotation .\n",
    "            ?n_q_show a diamant:Quotation .\n",
    "            FILTER (xsd:integer(?wy_f_show) >= 1200)\n",
    "            FILTER (xsd:integer(?wy_t_show) >= 1200)\n",
    "            FILTER (xsd:integer(?wy_f_show) <= 2018)\n",
    "            FILTER (xsd:integer(?wy_t_show) <= 2018)\n",
    "          { bind(\"defText\" as ?inputMode) } .\n",
    "            }\n",
    "        }\n",
    "        }\"\"\"\n",
    "    elif (lexicon==\"molex\"):\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart1 = \"\"\"\"\"\"\n",
    "        subpart2 = \"\"\"\"\"\"\n",
    "        subpartPos = \"\"\"\"\"\"\n",
    "        if (word != ''):\n",
    "            if (exactsearch == True):\n",
    "                subpart1 =  \"\"\"\n",
    "                    { ?lemCFId ontolex:writtenRep ?lemma . \n",
    "                    values ?lemma { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } \n",
    "                    UNION\n",
    "                    { ?wordformId ontolex:writtenRep ?wordform . \n",
    "                    values ?wordform { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } .\n",
    "                    \"\"\"        \n",
    "            else:\n",
    "                subpart2 = \"\"\"FILTER ( regex(?lemma, \\\"\"\"\"+word+\"\"\"\\\") || regex(?wordform, \\\"\"\"\"+word+\"\"\"\\\") ) . \"\"\"\n",
    "        if (pos != ''):\n",
    "            subpartPos = \"\"\"FILTER ( regex(?lemPos, \\\"\"\"\"+pos+\"\"\"$\\\") ) .\"\"\"\n",
    "        query = \"\"\"\n",
    "            PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "            PREFIX UD: <http://universaldependencies.org/u/>\n",
    "            PREFIX diamant: <http://rdf.ivdnt.org/schema/diamant#>\n",
    "            \n",
    "            SELECT ?lemEntryId ?lemma ?lemPos ?wordformId ?wordform ?hyphenation ?wordformPos ?Gender ?Number\n",
    "            FROM <http://rdf.ivdnt.org/lexica/molex>\n",
    "            WHERE\n",
    "            {\n",
    "            ?lemEntryId ontolex:canonicalForm ?lemCFId .\n",
    "            ?lemCFId ontolex:writtenRep ?lemma .\n",
    "            \"\"\"+subpart1+\"\"\"\n",
    "            OPTIONAL {?lemEntryId UD:Gender ?Gender .}\n",
    "            OPTIONAL {?lemEntryId UD:VerbForm ?verbform .}\n",
    "            ?lemEntryId UD:pos ?lemPos .\n",
    "            \"\"\"+subpartPos+\"\"\"\n",
    "            ?lemEntryId ontolex:lexicalForm ?wordformId .\n",
    "            ?wordformId UD:pos ?wordformPos .\n",
    "            OPTIONAL {?wordformId UD:Number ?Number .}\n",
    "            OPTIONAL {?wordformId ontolex:writtenRep ?wordform .}\n",
    "            OPTIONAL {?wordformId diamant:hyphenation ?hyphenation .}\n",
    "            \"\"\"+subpart2+\"\"\"\n",
    "            }\n",
    "        \"\"\"\n",
    "    elif (lexicon==\"duelme\"):\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart = \"\"\"FILTER ( regex(?lemma, \\\"\"\"\"+word+\"\"\"\\\") || regex(?wordform, \\\"\"\"\"+word+\"\"\"\\\") ) .\"\"\"\n",
    "        if (exactsearch == True):\n",
    "            subpart =  \"\"\"\n",
    "                { ?y lmf:hasLemma ?dl .  \n",
    "                values ?dl { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } }                 \n",
    "                \"\"\"        \n",
    "        query = \"\"\"\n",
    "            PREFIX duelme: <http://rdf.ivdnt.org/lexica/duelme>\n",
    "            PREFIX intskos: <http://ivdnt.org/schema/lexica#>\n",
    "            PREFIX lmf: <http://www.lexinfo.net/lmf>\n",
    "            PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "            PREFIX UD: <http://rdf.ivdnt.org/vocabs/UniversalDependencies2#>\n",
    "            \n",
    "            SELECT ?exampleSentence ?lemma ?gender ?number\n",
    "            WHERE  {\n",
    "                  ?d intskos:ExampleSentence ?exampleSentence .\n",
    "                  ?d lmf:ListOfComponents [lmf:Component ?y] .\n",
    "                  ?y lmf:hasLemma ?lemma . \n",
    "                  OPTIONAL {?y UD:Gender ?gender}\n",
    "                  OPTIONAL {?y UD:Number ?number}\n",
    "            \"\"\"+subpart+\"\"\"\n",
    "            }\n",
    "        \"\"\"\n",
    "    elif (lexicon==\"celex\"):\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart = \"\"\"FILTER ( regex(?lemma, \\\"\"\"\"+word+\"\"\"\\\") ) . \"\"\"\n",
    "        if (exactsearch == True):\n",
    "            subpart =  \"\"\"\n",
    "                { ?lemmaId ontolex:canonicalForm [ontolex:writtenRep ?lemma] .  \n",
    "                values ?lemma { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } }                 \n",
    "                \"\"\"        \n",
    "        query = \"\"\"\n",
    "            PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "            PREFIX celex: <http://rdf.ivdnt.org/lexica/celex>\n",
    "            PREFIX UD: <http://rdf.ivdnt.org/vocabs/UniversalDependencies2#>\n",
    "            PREFIX decomp: <http://www.w3.org/ns/lemon/decomp#>\n",
    "            \n",
    "            SELECT DISTINCT ?lemmaId ?lemma ?wordformId ?wordform ?number ?gender concat('',?subLemmata) AS ?subLemmata\n",
    "            WHERE  {\n",
    "                ?lemmaId ontolex:canonicalForm [ontolex:writtenRep ?lemma] .\n",
    "                \"\"\"+subpart+\"\"\"\n",
    "                BIND( ?lemmaId AS ?lemmaIdIRI ).\n",
    "                ?lemmaId ontolex:lexicalForm ?wordformId .\n",
    "                ?wordformId ontolex:writtenRep ?wordform .\n",
    "                OPTIONAL {?wordformId UD:Number ?number} .\n",
    "                OPTIONAL {\n",
    "                    ?lemmaId UD:Gender ?g . \n",
    "                        bind( \n",
    "                            if(?g = UD:Fem_Gender, \n",
    "                            UD:Com_Gender, \n",
    "                                if(?g = UD:Masc_Gender,\n",
    "                                    UD:Com_Gender,\n",
    "                                    UD:Neut_Gender\n",
    "                                )\n",
    "                            )\n",
    "                            AS ?gender\n",
    "                        )\n",
    "                }\n",
    "                OPTIONAL {\n",
    "                    SELECT ?lemmaIdIRI (group_concat(DISTINCT concat(?partNr,\":\",?subLemma);separator=\" + \") as ?subLemmata)\n",
    "                    WHERE {\n",
    "                        SELECT ?lemmaIdIRI ?celexComp ?aWordformId ?subLemma ?partNr\n",
    "                        WHERE {\n",
    "                                {\n",
    "                                ?lemmaIdIRI decomp:constituent ?celexComp .\n",
    "                                ?celexComp decomp:correspondsTo ?subLemmaId .\n",
    "                                BIND( ?subLemmaId AS ?subLemmaIdIRI ) .\n",
    "                                ?subLemmaIdIRI ontolex:canonicalForm [ontolex:writtenRep ?subLemma] .\n",
    "                                BIND( ?lemmaIdIRI AS ?mainLemmaId ) .\n",
    "                                ?mainLemmaId ontolex:lexicalForm ?aWordformId .\n",
    "                                }\n",
    "                                {\n",
    "                                    {\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_1> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_2> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_3> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_4> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_5> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_6> ?celexComp .}                                        \n",
    "                                    }\n",
    "                                ?lemmaIdIRI ?rdfsynt ?celexComp .\n",
    "                                BIND(IF(STRSTARTS(str(?rdfsynt), \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"), replace(STRAFTER(str(?rdfsynt), \"#\"), \"_\", \"\"), \"999\") AS ?partNr) .\n",
    "                                MINUS {\n",
    "                                    ?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#0> ?celexComp .\n",
    "                                    }\n",
    "                                }\n",
    "                            FILTER (?partNr != \"999\") .\n",
    "                            }\n",
    "                            ORDER BY ?partNr\n",
    "                            }\n",
    "                        GROUP BY ?aWordformId ?lemmaIdIRI\n",
    "                    }\n",
    "            }\n",
    "        \"\"\"\n",
    "        \n",
    "    return query\n",
    "\n",
    "def corpus_query_lemma(word):\n",
    "    return r'[lemma=\"'+ word + r'\"]'\n",
    "\n",
    "def lexicon_query_allwords(lexicon):\n",
    "    if (lexicon==\"diamant\"):\n",
    "        query = \"\"\"\n",
    "        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "        prefix prov: <http://www.w3.org/ns/prov#>\n",
    "        prefix diamant: <http://rdf.ivdnt.org/schema/diamant#>\n",
    "        prefix lexinfo: <http://www.lexinfo.net/ontology/2.0/lexinfo#>\n",
    "        prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "        prefix lemon: <http://lemon-model.net/lemon#>\n",
    "        prefix ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "        prefix ud: <http://universaldependencies.org/u/pos/>\n",
    "        prefix skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "        prefix dcterms: <http://purl.org/dc/terms/>\n",
    "        prefix dc: <http://purl.org/dc/terms/>\n",
    "\n",
    "        select ?n_ontolex_writtenRep\n",
    "        where\n",
    "        {\n",
    "        graph ?g\n",
    "        {\n",
    "        {\n",
    "            { ?n_form ontolex:writtenRep ?n_ontolex_writtenRep} .\n",
    "            { ?n_form a ontolex:Form} .\n",
    "        }\n",
    "        }\n",
    "        }\n",
    "        LIMIT 10000\"\"\"\n",
    "    elif (lexicon==\"anw\"):\n",
    "        query = \"\"\"PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "                  PREFIX anw: <http://rdf.ivdnt.org/lexica/anw>\n",
    "                  \n",
    "                  SELECT ?writtenForm\n",
    "                  FROM <http://rdf.ivdnt.org/lexica/anw>\n",
    "                  WHERE {\n",
    "                      ?lemId rdfs:label ?lemma .\n",
    "                      ?lemId ontolex:canonicalForm ?lemCFId . \n",
    "                      ?lemCFId ontolex:writtenRepresentation ?writtenForm .\n",
    "                      }\"\"\"\n",
    "    else:\n",
    "        raise ValueError(\"Lexicon \" + lexicon + \" not supported for querying all words.\")\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the cell below to show the UI, and fill in your search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T16:42:06.847928Z",
     "start_time": "2019-01-16T16:42:06.828765Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from chaininglib import ui\n",
    "\n",
    "# Create corpus UI, creates references to field contents\n",
    "corpusQueryField, corpusField = create_corpus_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Click the cell below and press Run to perform the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T16:42:07.402740Z",
     "start_time": "2019-01-16T16:42:06.849675Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from chaininglib import search\n",
    "query= corpusQueryField.value\n",
    "corpus = corpusField.value\n",
    "df_corpus = search_corpus(query,corpus)\n",
    "#df_corpus = load_dataframe('mijn_resultaten.csv')\n",
    "display(df_corpus)\n",
    "create_save_results_ui(df_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Lexicon search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Run the cell below to show the UI, and fill in your search query in the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T16:42:07.420257Z",
     "start_time": "2019-01-16T16:42:07.404330Z"
    },
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from chaininglib import ui\n",
    "searchWordField, lexiconField = create_lexicon_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " * Click the cell below and press Run to perform the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T16:42:07.805919Z",
     "start_time": "2019-01-16T16:42:07.421680Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#from chaininglib import queries, search\n",
    "\n",
    "search_word = searchWordField.value\n",
    "lexicon = lexiconField.value\n",
    "# USER: can replace this by own custom query\n",
    "query = lexicon_query(word=search_word, pos= '', lexicon=lexicon)\n",
    "\n",
    "df_lexicon = search_lexicon(query, lexicon)\n",
    "df_columns_list = list(df_lexicon.columns.values)\n",
    "df_lexicon_in_columns = df_lexicon[df_columns_list]\n",
    "display(df_lexicon_in_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Case study 1 (parallel): Frequency of *puur*+verb and *zuiver*+verb compared\n",
    "* Below cell searches for *puur*+verb and for *zuiver*+verb in the CHN corpus\n",
    "* Compare frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T16:42:08.692517Z",
     "start_time": "2019-01-16T16:42:07.809209Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#from chaininglib import search\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Word 1: puur\n",
    "word1= \"puur\"\n",
    "df_corpus1 = search_corpus(r'[word=\"' + word1 + r'\"][pos=\"verb\"]',corpus=\"chn\")\n",
    "display(HTML('<b>' + word1 + '</b>'))\n",
    "display(df_corpus1)\n",
    "\n",
    "# Word 2: zuiver\n",
    "word2 = \"zuiver\"\n",
    "df_corpus2 = search_corpus(r'[word=\"' + word2 + r'\"][pos=\"verb\"]',\"chn\")\n",
    "display(HTML('<b>' + word2 + '</b>'))\n",
    "display(df_corpus2)\n",
    "\n",
    "# Compute difference\n",
    "diff_left, diff_right, intersec = column_difference(df_corpus1[\"word 1\"], df_corpus2[\"word 1\"])\n",
    "# Elements of 1 that are not in 2\n",
    "display(HTML('Werkwoorden voor <b>' + word1 + '</b> niet in <b>' + word2 + '</b>: ' + \", \".join(diff_left)))\n",
    "# Elements of 2 that are not in 1\n",
    "display(HTML('Werkwoorden voor <b>' + word1 + '</b> niet in <b>' + word2 + '</b>: ' + \", \".join(diff_right)))\n",
    "# Elements both in 1 and 2\n",
    "display(HTML('Werkwoorden zowel voor <b>' + word1 + '</b> als voor <b>' + word2 + '</b>: ' + \", \".join(intersec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 2 (sequential): Retrieve synonyms from DiaMaNT, look up in Gysseling\n",
    "* Below cell searches for term \"boek\" in DiaMaNT, and looks up all variants in Gysseling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T16:42:18.713352Z",
     "start_time": "2019-01-16T16:42:08.694085Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_word = \"boek\"\n",
    "lexicon = \"diamant\"\n",
    "corpus= \"gysseling\"\n",
    "\n",
    "# First, lookup synonyms in DiaMaNT\n",
    "query = lexicon_query(word=search_word, pos= '', lexicon=lexicon)\n",
    "df_lexicon = search_lexicon(query, lexicon)\n",
    "syns = diamant_get_synonyms(df_lexicon) \n",
    "syns.add(search_word) # Also add search word itself\n",
    "display(HTML('Synoniemen voor <b>' + search_word + '</b>: ' + \", \".join(syns)))\n",
    "\n",
    "# Search for all synonyms in corpus\n",
    "## Create queries: search by lemma\n",
    "syns_queries = [corpus_query_lemma(syn) for syn in syns]\n",
    "## Search for all synonyms in corpus\n",
    "result_dict = search_corpus_multiple(syns_queries, corpus)\n",
    "view_multiple_results(result_dict, labels=list(syns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T16:24:19.655999Z",
     "start_time": "2019-01-11T16:24:19.645252Z"
    }
   },
   "source": [
    "## Case study (parallel) 3: Find corpus words not in lexicon; list most frequent ones.\n",
    "* Only parallel if you can ask the lexicon a list of all words.\n",
    "* Currently only working: ask DiaMaNT list of words (limited at 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T16:42:24.722526Z",
     "start_time": "2019-01-16T16:42:18.714869Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Query lexicon to give list of all words\n",
    "lexicon=\"anw\"\n",
    "#df_lexicon = search_lexicon_allwords(lexicon)\n",
    "## TODO: Why do double words appear?\n",
    "#lexicon_set = set([w.lower() for w in df_lexicon[\"writtenForm\"]])\n",
    "#display(lexicon_set)\n",
    "\n",
    "df_corpus = search_corpus_allwords(\"gysseling\")\n",
    "display(df_corpus)\n",
    "len(df_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T15:46:20.519833Z",
     "start_time": "2019-01-16T15:46:20.516208Z"
    }
   },
   "source": [
    "## Case study (sequential) 4: Find occurences of attributive adjectives not ending with -e, even though they are preceeded by a definite article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T16:42:36.088787Z",
     "start_time": "2019-01-16T16:42:24.724312Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_to_search=\"opensonar\"\n",
    "lexicon_to_search=\"molex\"\n",
    "\n",
    "# CORPUS: get [article + attributive adjective + nouns] combinations in which the adjective does not end with -e\n",
    "print('Searching '+corpus_to_search+' corpus')\n",
    "df_corpus = search_corpus(r'[lemma=\"de|het\"][word=\"^g(.+)[^e]$\" & pos=\"ADJ\"][pos=\"NOUN\"]', corpus=corpus_to_search)\n",
    "display(df_corpus)\n",
    "\n",
    "# LEXICON: get adjectives the lemma of which does not end with -e\n",
    "print('Searching '+lexicon_to_search+' lexicon')\n",
    "query=lexicon_query('^g(.+)[^e]$', 'ADJ', lexicon_to_search)\n",
    "df_lexicon = search_lexicon(query, lexicon_to_search)\n",
    "display(df_lexicon)\n",
    "\n",
    "# LEXICON: get adjectives having a final -e in definite attributive use\n",
    "print('Filtering lexicon results')\n",
    "condition=df_lexicon.wordform.str.contains('e$')\n",
    "df = df_lexicon[condition]\n",
    "display(df)\n",
    "\n",
    "# RESULT: get the records out of our first list in which the -e-less-adjectives match the lemma form of our last list\n",
    "print('Wanted list:')\n",
    "eless_forms = list(df.lemma)\n",
    "adj_without_e = df_corpus['word 1'].isin(eless_forms)\n",
    "display( df_corpus[adj_without_e] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs_env",
   "language": "python",
   "name": "cs_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
