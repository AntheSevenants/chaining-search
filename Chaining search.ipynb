{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaining search\n",
    "\n",
    "\n",
    "\n",
    "## Sphinx documentatie: https://pythonhosted.org/an_example_pypi_project/sphinx.html\n",
    "## in voorbeelden handige python functies opnemen\n",
    "## zoals ; .sort_values(ascending=False,by=['raw_freq']));  list enz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the cell below to show the UI, and fill in your search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T15:54:55.508244Z",
     "start_time": "2019-02-18T15:54:55.297822Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.ui.search import create_corpus_ui\n",
    "\n",
    "# Create corpus UI, creates references to field contents\n",
    "corpusQueryField, corpusField = create_corpus_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Click the cell below and press Run to perform the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T16:35:20.155128Z",
     "start_time": "2019-02-18T16:35:19.420420Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "#from chaininglib import search\n",
    "query= corpusQueryField.value\n",
    "corpus_name = corpusField.value\n",
    "df_corpus = create_corpus(corpus_name).pattern(query).method(\"blacklab\").results()\n",
    "#df_corpus = load_dataframe('mijn_resultaten.csv')\n",
    "display_df(df_corpus, labels=\"Results\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the cell below to show the UI, and fill in your search query in the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T16:38:26.757646Z",
     "start_time": "2019-02-13T16:38:26.413068Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from chaininglib.ui.search import create_lexicon_ui\n",
    "\n",
    "#from chaininglib import ui\n",
    "searchWordField, lexiconField = create_lexicon_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Click the cell below and press Run to perform the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T16:38:28.137836Z",
     "start_time": "2019-02-13T16:38:28.064277Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.LexiconQuery import *\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "search_word = searchWordField.value\n",
    "lexicon_name = lexiconField.value\n",
    "# USER: can replace this by own custom query\n",
    "df_lexicon = create_lexicon(lexicon_name).lemma(search_word).results()\n",
    "display_df(df_lexicon)\n",
    "#df_columns_list = list(df_lexicon.columns.values)\n",
    "#df_lexicon_in_columns = df_lexicon[df_columns_list]\n",
    "#display(df_lexicon_in_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 1 (parallel): Frequency of *puur*+verb and *zuiver*+verb compared\n",
    "* Below cell searches for *puur*+verb and for *zuiver*+verb in the CHN corpus\n",
    "* Compare frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T16:25:46.948392Z",
     "start_time": "2019-02-15T16:25:45.392228Z"
    }
   },
   "outputs": [],
   "source": [
    "#from chaininglib import search\n",
    "from IPython.core.display import display, HTML\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.ui.dfui import display_df\n",
    "from chaininglib.utils.dfops import column_difference\n",
    "\n",
    "# Word 1: puur\n",
    "word1= \"puur\"\n",
    "cq1 = create_corpus(\"chn\").pattern(r'[word=\"' + word1 + r'\"][pos=\"verb\"]')\n",
    "df_corpus1 = cq1.results()\n",
    "display_df(df_corpus1, word1)\n",
    "\n",
    "# Word 2: zuiver\n",
    "word2 = \"zuiver\"\n",
    "cq2 = create_corpus(\"chn\").pattern(r'[word=\"' + word2 + r'\"][pos=\"verb\"]')\n",
    "df_corpus2 = cq2.results()\n",
    "display_df(df_corpus2, word2)\n",
    "\n",
    "# Compute difference\n",
    "diff_left, diff_right, intersec = column_difference(df_corpus1[\"word 1\"], df_corpus2[\"word 1\"])\n",
    "# Elements of 1 that are not in 2\n",
    "display(HTML('Werkwoorden voor <b>' + word1 + '</b> niet in <b>' + word2 + '</b>: ' + \", \".join(diff_left)))\n",
    "# Elements of 2 that are not in 1\n",
    "display(HTML('Werkwoorden voor <b>' + word1 + '</b> niet in <b>' + word2 + '</b>: ' + \", \".join(diff_right)))\n",
    "# Elements both in 1 and 2\n",
    "display(HTML('Werkwoorden zowel voor <b>' + word1 + '</b> als voor <b>' + word2 + '</b>: ' + \", \".join(intersec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 2 (sequential): Retrieve synonyms from DiaMaNT, look up in Gysseling\n",
    "* Below cell searches for term \"boek\" in DiaMaNT, and looks up all variants in Gysseling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T16:26:20.814487Z",
     "start_time": "2019-02-15T16:26:10.020960Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.search.LexiconQuery import *\n",
    "from IPython.core.display import display, HTML\n",
    "from chaininglib.search.corpusQueries import corpus_query\n",
    "from chaininglib.process.lexicon import diamant_get_synonyms\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "search_word = \"boek\"\n",
    "lexicon_name = \"diamant\"\n",
    "corpus= \"gysseling\"\n",
    "\n",
    "# First, lookup synonyms in DiaMaNT\n",
    "lq = create_lexicon(lexicon_name).lemma(search_word)\n",
    "df_lexicon = lq.results()\n",
    "syns = diamant_get_synonyms(df_lexicon) \n",
    "syns.add(search_word) # Also add search word itself\n",
    "display(HTML('Synoniemen voor <b>' + search_word + '</b>: ' + \", \".join(syns)))\n",
    "\n",
    "# Search for all synonyms in corpus\n",
    "## Create queries: search by lemma\n",
    "syns_queries = [corpus_query(lemma=syn) for syn in syns]\n",
    "## Search for all synonyms in corpus\n",
    "cq = create_corpus(corpus).pattern(syns_queries)\n",
    "result_dict = cq.results()\n",
    "display_df(result_dict, labels=list(syns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T15:46:20.519833Z",
     "start_time": "2019-01-16T15:46:20.516208Z"
    }
   },
   "source": [
    "(case study 3 deleted)\n",
    "## Case study (sequential) 4: Find occurences of attributive adjectives not ending with -e, even though they are preceeded by a definite article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T16:30:47.240012Z",
     "start_time": "2019-02-15T16:30:38.740615Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.search.LexiconQuery import *\n",
    "from chaininglib.utils.dfops import df_filter\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "corpus_to_search=\"opensonar\"\n",
    "lexicon_to_search=\"molex\"\n",
    "\n",
    "# CORPUS: get [article + attributive adjective + nouns] combinations in which the adjective does not end with -e\n",
    "print('Get occurences of attributive adjectives not ending with -e')\n",
    "cq = create_corpus(corpus_to_search).pattern(r'[lemma=\"de|het\"][word=\"^g(.+)[^e]$\" & pos=\"ADJ\"][pos=\"NOUN\"]')\n",
    "df_corpus = cq.results()\n",
    "\n",
    "# LEXICON: get adjectives the lemma of which does not end with -e\n",
    "lq = create_lexicon(lexicon_to_search).lemma('^g(.+)[^e]$').pos('ADJ')\n",
    "df_lexicon = lq.results()\n",
    "\n",
    "# LEXICON: get adjectives having a final -e in definite attributive use\n",
    "print('Filtering lexicon results')\n",
    "final_e_condition = df_filter(df_lexicon[\"wordform\"], 'e$')\n",
    "df_lexicon_form_e = df_lexicon[ final_e_condition ]\n",
    "\n",
    "# RESULT: get the records out of our first list in which the -e-less-adjectives match the lemma form of our last list\n",
    "print('List of attributive adjectives not ending with -e even though they should have a final -e:')\n",
    "e_forms = list(df_lexicon_form_e.lemma)\n",
    "no_final_e_condition = df_filter(df_corpus[\"word 1\"], e_forms, method=\"isin\")\n",
    "result_df = df_corpus[ no_final_e_condition ]\n",
    "display_df( result_df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study (sequential) 5: (morphosyntactic lexicon and possibly unannotated corpus) Look up inflected forms and spelling variants for a given lemma in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T12:28:08.348083Z",
     "start_time": "2019-02-13T12:28:07.854534Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.ui.dfui import display_df\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.search.LexiconQuery import *\n",
    "\n",
    "lexicon_to_search=\"molex\"\n",
    "corpus_to_search=\"chn\"\n",
    "\n",
    "##############################################\n",
    "# TODO  zelfde met meerdere lemmata en gegroepeerd \n",
    "##############################################\n",
    "\n",
    "lemma_to_look_for=\"denken\"\n",
    "\n",
    "# LEXICON: Search for the inflected forms of a lemma in a morphosyntactic lexicon\n",
    "lq = create_lexicon(lexicon_to_search).lemma(lemma_to_look_for)\n",
    "df_lexicon = lq.results()\n",
    "display_df(df_lexicon)\n",
    "\n",
    "# Put all inflected forms into a list\n",
    "inflected_wordforms = list(df_lexicon.wordform)\n",
    "\n",
    "# CORPUS: Look up the inflected forms in a (possibly unannotated) corpus\n",
    "# beware: If the corpus is not annotated, all we can do is searching for the inflected words\n",
    "#         But if the corpus is lemmatized, we have to make sure we're retrieving correct data by specifying the lemma as well\n",
    "annotated_corpus = True\n",
    "query = r'[lemma=\"'+lemma_to_look_for+r'\" & word=\"'+r\"|\".join(inflected_wordforms)+r'\"]' if annotated_corpus else r'[word=\"'+r\"|\".join(inflected_wordforms)+r'\"]'\n",
    "cq = create_corpus(corpus_to_search).pattern(query)\n",
    "df_corpus = cq.results() \n",
    "display_df(df_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 6: Build frequency table of some corpus, based on lemma list of a given lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T13:45:11.210Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.utils.dfops import get_rank_diff\n",
    "from chaininglib.process.combined import get_frequency_list\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "base_lexicon=\"molex\"\n",
    "corpus_to_search1=\"opensonar\"\n",
    "corpus_to_search2=\"chn\"\n",
    "\n",
    "# build frequency tables of two corpora\n",
    "\n",
    "df_frequency_list1 = get_frequency_list(base_lexicon, \"NOUN\", corpus_to_search1)\n",
    "# sort and display\n",
    "df_top25_descending = df_frequency_list1.sort_values(ascending=False,by=['raw_freq']).head(25)\n",
    "df_top25_ascending = df_frequency_list1.sort_values(ascending=True, by=['rank']).head(25)\n",
    "display_df( df_top25_descending )\n",
    "display_df( df_top25['raw_freq'], labels='chart df1', mode='chart' )\n",
    "\n",
    "df_frequency_list2 = get_frequency_list(base_lexicon, \"NOUN\", corpus_to_search2)\n",
    "# sort and display\n",
    "df_top25_descending = df_frequency_list2.sort_values(ascending=False,by=['raw_freq']).head(25)\n",
    "df_top25_ascending = df_frequency_list2.sort_values(ascending=True, by=['rank']).head(25)\n",
    "display_df( df_top25_descending )\n",
    "display_df( df_top25['raw_freq'], labels='chart df2', mode='chart' )\n",
    "\n",
    "\n",
    "# TODO: lemmata tonen die in 1 of 2 ontbreken\n",
    "\n",
    "# compute the rank diff of lemmata in frequency tables\n",
    "\n",
    "# sort and display\n",
    "df_rankdiffs = get_rank_diff(df_frequency_list1, df_frequency_list2)\n",
    "\n",
    "display_df(df_rankdiffs.sort_values(by=['rank_diff']).head(25))\n",
    "\n",
    "df_top25_descending = df_rankdiffs.sort_values(ascending=False, by=['rank_diff']).head(25)\n",
    "display_df( df_top25_descending['rank_diff'], labels='chart large diff', mode='chart' )\n",
    "\n",
    "df_top25_ascending = df_rankdiffs.sort_values(ascending=True, by=['rank_diff']).head(25)\n",
    "display_df( df_top25_ascending['rank_diff'], labels='chart small diff', mode='chart' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 7: search in a corpus for wordforms of a lemma, which are not included in this lemma's paramadigm in a lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-13T13:47:33.206Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.process.combined import get_missing_wordforms\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "\n",
    "base_lexicon=\"molex\"\n",
    "corpus_to_search=\"opensonar\"\n",
    "\n",
    "df = get_missing_wordforms(base_lexicon, \"VERB\", corpus_to_search)\n",
    "\n",
    "df.to_csv( \"missing_wordforms.csv\", index=False)\n",
    "#df = load_dataframe(\"missing_wordforms.csv\")\n",
    "\n",
    "display_df(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 8: Train a tagger with data from an annotated corpus, and do something cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T12:47:35.423033Z",
     "start_time": "2019-02-13T12:47:34.555195Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.ui.dfui import display_df\n",
    "from chaininglib.process.corpus import get_tagger\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.search.LexiconQuery import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "base_lexicon=\"molex\"\n",
    "corpus_to_search1=\"opensonar\"\n",
    "corpus_to_search2=\"chn\"\n",
    "\n",
    "# we have a given word, let's say: \"loop\"\n",
    "some_word = \"loop\"\n",
    "\n",
    "# get the paradigm of the lemma our word is a part of\n",
    "l = create_lexicon(base_lexicon).lemma(some_word)\n",
    "df_paradigm = l.results()\n",
    "display_df(df_paradigm)\n",
    "\n",
    "# gather some pattern including our word, out of an annotated corpus\n",
    "# here: DET + ADJ + 'loop'\n",
    "c1 = create_corpus(corpus_to_search1).word(some_word).detailed_context(True)\n",
    "c2 = create_corpus(corpus_to_search2).word(some_word).detailed_context(True)\n",
    "df_corpus1 = c1.results() \n",
    "df_corpus2 = c2.results() \n",
    "display_df(df_corpus1)\n",
    "display_df(df_corpus2)\n",
    "\n",
    "\n",
    "df_all = pd.concat([df_corpus1, df_corpus2], keys=[corpus_to_search1, corpus_to_search2])\n",
    "display_df(df_all)\n",
    "\n",
    "# get a tagger trained with our corpus data\n",
    "tagger = get_tagger(df_all)\n",
    "\n",
    "# Use the trained tagger to tag unknown sentences\n",
    "# The input must be like: tagger.tag(['today','is','a','beautiful','day'])\n",
    "\n",
    "sentence = 'Mijn buurman kijkt door de loop van zijn geweer'\n",
    "tagged_sentence = tagger.tag( sentence.split() )\n",
    "\n",
    "print(tagged_sentence)\n",
    "\n",
    "\n",
    "# Know we can lemmatize each occurence of our lemma in the new sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 9: Search in corpus and filter on metadata\n",
    "First, we request all available metadata fields of the corpus. Then, we issue a search query, and request all metadata fields for the result. Finally, we filter on metadata values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-15T16:33:59.144779Z",
     "start_time": "2019-02-15T16:33:58.538033Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.metadata import get_available_metadata\n",
    "from chaininglib.utils.dfops import df_filter\n",
    "from chaininglib.ui.dfui import display_df\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "\n",
    "\n",
    "corpus_name=\"zeebrieven\"\n",
    "query=r'[lemma=\"boek\"]'\n",
    "# Request all metadata fields from corpus\n",
    "fields = get_available_metadata(corpus_name)\n",
    "# Perform query and ask all metadata\n",
    "c = create_corpus(corpus_name).pattern(query).extra_fields_doc(fields[\"document\"]) \n",
    "df_corpus = c.results()\n",
    "\n",
    "# Filter on year: > 1700\n",
    "df_filter_year = df_corpus[df_corpus[\"witnessYear_from\"].astype('int32') > 1700] \n",
    "display_df(df_filter_year, labels=\"After 1700\")\n",
    "\n",
    "# Filter on sender birth place Amsterdam\n",
    "condition = df_filter(df_corpus[\"afz_geb_plaats\"], regex_or_set=\"Amsterdam\")\n",
    "df_filter_place = df_corpus[ condition ]\n",
    "display_df(df_filter_place, labels=\"Sender born in Amsterdam\")\n",
    "\n",
    "\n",
    "# Group by birth place\n",
    "df = property_freq(df_corpus,\"afz_loc_plaats\")\n",
    "display_df(df, labels=\"Most frequent sender locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 10: Visualizing h-dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T13:23:43.873072Z",
     "start_time": "2019-02-13T13:20:27.890619Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.search.metadata import get_available_metadata\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "corpus_to_search=\"chn\"\n",
    "\n",
    "fields = get_available_metadata(corpus_to_search)\n",
    "\n",
    "\n",
    "df_corpus1 = create_corpus(corpus_to_search).pattern(r'[lemma=\"h[aeo].*\" & word=\"[aeo].*\"]').extra_fields_doc(fields[\"document\"]).results()\n",
    "df_corpus1 = create_corpus(corpus_to_search).pattern(r'[lemma=\"h[aeo].*\" & word=\"h[aeo].*\"]').extra_fields_doc(fields[\"document\"]).results()\n",
    "\n",
    "display_df( df_corpus1)\n",
    "display_df( df_corpus2)\n",
    "\n",
    "display_df( df_corpus1.groupby([\"Region\", \"Date\"]), labels=\"h-dropping\", mode='chart')\n",
    "display_df( df_corpus2.groupby([\"Region\", \"Date\"]), labels=\"normal\", mode='chart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs_env",
   "language": "python",
   "name": "cs_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
