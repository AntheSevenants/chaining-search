{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaining search\n",
    "\n",
    "\n",
    "\n",
    "## Sphinx documentatie: https://pythonhosted.org/an_example_pypi_project/sphinx.html\n",
    "## in voorbeelden handige python functies opnemen\n",
    "## zoals ; .sort_values(ascending=False,by=['raw_freq']));  list enz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library functions: Search\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T14:39:41.904717Z",
     "start_time": "2019-01-22T14:39:41.899150Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Args:\n",
    "    df: Pandas DataFrame to filter on\n",
    "    df: column on which we filter\n",
    "    method: \"regex\" of \"isin\"\n",
    "    regex_or_set: Regular expression (if method==\"regex\") or set (if method==\"isin\")\n",
    "'''\n",
    "\n",
    "def filter_df(df, column, method, regex_or_set):\n",
    "    if method==\"regex\":\n",
    "        filter_condition = df[column].str.contains(regex_or_set)\n",
    "    elif method==\"isin\":\n",
    "        filter_condition = df[column].isin(regex_or_set)\n",
    "    else:\n",
    "        raise ValueError(\"method should be one of regex or isin\")\n",
    "    return df[filter_condition]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T14:35:31.937772Z",
     "start_time": "2019-01-22T14:35:31.893616Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import urllib\n",
    "#import wx   # for interaction popups          TODO -> omzetten naar JS of zo\n",
    "import itertools # for frequency list function\n",
    "import numpy     # idem\n",
    "from IPython.display import FileLink, FileLinks\n",
    "AVAILABLE_CORPORA = ['chn', 'opensonar', 'zeebrieven', 'gysseling', 'nederlab']\n",
    "RECORDS_PER_PAGE = 1000\n",
    "\n",
    "# Get rid of ellipsis in display (otherwise relevant data might not be shown)\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "\n",
    "# Search methods\n",
    "\n",
    "def search_corpus_allwords(corpus, pos):\n",
    "    query = r'[word=\".*\"]'\n",
    "    if pos is not None:\n",
    "        query = r'[word=\".*\" & pos=\"'+pos+r'\"]'\n",
    "    return search_corpus(query, corpus)\n",
    "\n",
    "def search_corpus_alllemmata(corpus, pos):\n",
    "    query = r'[lemma=\".*\"]'\n",
    "    if pos is not None:\n",
    "        query = r'[lemma=\".*\" & pos=\"'+pos+r'\"]'\n",
    "    return search_corpus(query, corpus) \n",
    "\n",
    "def search_corpus(query, corpus, start_position=1, hits_only=True):\n",
    "    # show wait indicator\n",
    "    #app = wx.App()\n",
    "    #msg_to_user = wx.BusyInfo('Searching '+corpus+' corpus')\n",
    "    if corpus not in AVAILABLE_CORPORA:\n",
    "        raise ValueError(\"Unknown corpus: \" + corpus)\n",
    "    try:\n",
    "        # Do request to federated content search corpora, so we get same output format for every corpus\n",
    "        url = \"http://portal.clarin.inl.nl/fcscorpora/clariah-fcs-endpoints/sru?operation=searchRetrieve&queryType=fcs&maximumRecords=1000&x-fcs-context=\" + corpus + \"&query=\" + urllib.parse.quote(query)\n",
    "        print(url)\n",
    "        response = requests.get(url)\n",
    "        response_text = response.text    \n",
    "        df, next_page = _parse_xml(response_text, hits_only)\n",
    "        # If there are next pages, call search_corpus recursively\n",
    "        print(next_page)\n",
    "        if next_page > 0:\n",
    "            df_more = search_corpus(query, corpus, next_page, hits_only)\n",
    "            df = df.append(df_more, ignore_index=True)\n",
    "        # show message out of xml, if some error has occured (prevents empty output)\n",
    "        _show_error_if_any(response_text)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"An error occured when searching corpus \" + corpus + \": \"+ str(e))\n",
    "    #finally:\n",
    "    #    # remove wait indicator, and return dataframe\n",
    "    #    del msg_to_user        \n",
    "\n",
    "def search_corpus_multiple(queries, corpus):\n",
    "    result_dict = {}\n",
    "    for query in queries:\n",
    "        result_dict[query] = search_corpus(query,corpus)\n",
    "    return result_dict\n",
    "   \n",
    "\n",
    "def search_lexicon_alllemmata(lexicon, pos):\n",
    "    query = lexicon_query_alllemmata(lexicon, pos)\n",
    "    return search_lexicon(query, lexicon)\n",
    "\n",
    "def search_lexicon(query, lexicon):\n",
    "     # show wait indicator, so the user knows what's happening\n",
    "    #app = wx.App()\n",
    "    #msg_to_user = wx.BusyInfo('Searching '+lexicon+' lexicon')\n",
    "    # default endpoint, except when diamant is invoked\n",
    "    endpoint = \"http://172.16.4.56:8890/sparql\"\n",
    "    if (lexicon==\"diamant\"):\n",
    "        endpoint = \"http://svprre02:8080/fuseki/tdb/sparql\"\n",
    "    \n",
    "    try:\n",
    "        # Accept header is needed for virtuoso, it isn't otherwise!\n",
    "        response = requests.post(endpoint, data={\"query\":query}, headers = {\"Accept\":\"application/sparql-results+json\"})\n",
    "        \n",
    "        response_json = json.loads(response.text)\n",
    "        records_json = response_json[\"results\"][\"bindings\"]\n",
    "        records_string = json.dumps(records_json)    \n",
    "        df = pd.read_json(records_string, orient=\"records\")\n",
    "    \n",
    "        # make sure cells containing NULL are added too, otherwise we'll end up with ill-formed data\n",
    "        # TODO: maybe this can be replaced by:\n",
    "        # df = df.fillna('')\n",
    "        df = df.applymap(lambda x: '' if pd.isnull(x) else x[\"value\"])         \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"An error occured when searching lexicon \" + lexicon + \": \"+ str(e))\n",
    "    #finally:\n",
    "    #    # remove wait indicator, \n",
    "    #    del msg_to_user\n",
    "        \n",
    "\n",
    "# Processing methods\n",
    "\n",
    "def column_difference(df_column1, df_column2):\n",
    "    set_df1 = set(df_column1)\n",
    "    set_df2 = set(df_column2)\n",
    "    diff_left = set_df1.difference(set_df2)\n",
    "    diff_right = set_df2.difference(set_df1)\n",
    "    intersec = set_df1.intersection(set_df2)\n",
    "    return diff_left, diff_right, intersec\n",
    "\n",
    "def diamant_get_synonyms(df):\n",
    "    # Depending on the result type, we return the lemma or the definition text\n",
    "    lemmas = set(df[df[\"inputMode\"]==\"defText\"][\"n_ontolex_writtenRep\"])\n",
    "    defTexts = set(df[df[\"inputMode\"]==\"lemma\"][\"n_syndef_definitionText\"])\n",
    "    return lemmas|defTexts\n",
    "\n",
    "def _parse_xml(text, hits_only=True):\n",
    "    \n",
    "    # if hits_only=True, we'll only fetch info about the hits\n",
    "    # if hits_only=False, we'll fetch info about all the words\n",
    "    \n",
    "    # TODO: should we secure against untrusted XML?\n",
    "    root = ET.fromstring(text)\n",
    "    records = []\n",
    "    n_words_in_hit = 0\n",
    "    computed_nwih = False\n",
    "    layers_processed = 0\n",
    "    \n",
    "    for entry in root.iter(\"{http://clarin.eu/fcs/resource}ResourceFragment\"):    \n",
    "        \n",
    "        for dataView in entry.findall(\"{http://clarin.eu/fcs/resource}DataView\"):            \n",
    "            \n",
    "            # ----- [part 1] ----- \n",
    "            # in 'hits only' mode, we'll gather the hits, otherwise we'll gather all the words of the sentences\n",
    "            \n",
    "            # We only take into account hits, ignore metadata and segmenting dataViews\n",
    "            if (hits_only is True and dataView.get(\"type\")==\"application/x-clarin-fcs-hits+xml\"):\n",
    "                layers_processed = layers_processed + 1\n",
    "                result = dataView.find(\"{http://clarin.eu/fcs/dataview/hits}Result\")\n",
    "                left_context = result.text if result.text is not None else ''\n",
    "                hits = list(result)\n",
    "                if len(hits)==0:\n",
    "                    print([w for w in result.itertext()])\n",
    "                    print(\"no hit in kwic, skip\")\n",
    "                    continue\n",
    "                last_hit = hits[-1]\n",
    "                right_context = last_hit.tail if last_hit.tail is not None else ''\n",
    "                hit_words = [hit.text for hit in hits]\n",
    "                \n",
    "                if not computed_nwih:\n",
    "                    n_words_in_hit = len(hits)\n",
    "                    computed_nwih=True\n",
    "            \n",
    "            # Get each word\n",
    "            if ( hits_only is False and dataView.get(\"type\")==\"application/x-clarin-fcs-adv+xml\"):\n",
    "                layers_processed = layers_processed + 1             \n",
    "                for layer in dataView.findall(\".//{http://clarin.eu/fcs/dataview/advanced}Layer\"):                    \n",
    "                    if (layer.get(\"id\")==\"http://www.ivdnt.org/annotation-layers/word\"):\n",
    "                        hit_words = []\n",
    "                        path = \".//{http://clarin.eu/fcs/dataview/advanced}Span\"\n",
    "                        for one_span in layer.findall(path):\n",
    "                            span_text = one_span.text            \n",
    "                            hit_words.append(span_text)\n",
    "                if not computed_nwih:\n",
    "                    n_words_in_hit = len(hit_words)\n",
    "                    computed_nwih=True\n",
    "                \n",
    "            \n",
    "            # ----- [part 2] ----- \n",
    "            # gather info about each hit (=hits only mode) or about each word (=NOT hits only mode)\n",
    "                \n",
    "            # Get lemma of each hit\n",
    "            if (dataView.get(\"type\")==\"application/x-clarin-fcs-adv+xml\"):\n",
    "                layers_processed = layers_processed + 1             \n",
    "                for layer in dataView.findall(\".//{http://clarin.eu/fcs/dataview/advanced}Layer\"):                    \n",
    "                    if (layer.get(\"id\")==\"http://www.ivdnt.org/annotation-layers/lemma\"):\n",
    "                        hit_lemmata = []\n",
    "                        path = \".//{http://clarin.eu/fcs/dataview/advanced}Span\"\n",
    "                        if (hits_only is True):\n",
    "                            path = path+\"[@highlight='h1']\" \n",
    "                        for one_span in layer.findall(path):\n",
    "                            span_text = one_span.text            \n",
    "                            hit_lemmata.append(span_text)\n",
    "                            \n",
    "            # Get pos of each hit\n",
    "            if (dataView.get(\"type\")==\"application/x-clarin-fcs-adv+xml\"):\n",
    "                layers_processed = layers_processed + 1             \n",
    "                for layer in dataView.findall(\".//{http://clarin.eu/fcs/dataview/advanced}Layer\"):                    \n",
    "                    if (layer.get(\"id\")==\"http://www.ivdnt.org/annotation-layers/universal_dependency\"):\n",
    "                        hit_pos = []\n",
    "                        path = \".//{http://clarin.eu/fcs/dataview/advanced}Span\"\n",
    "                        if (hits_only is True):\n",
    "                            path = path+\"[@highlight='h1']\" \n",
    "                        for one_span in layer.findall(path):\n",
    "                            span_text = one_span.text            \n",
    "                            hit_pos.append(span_text)\n",
    "                            \n",
    "            if layers_processed == 3:\n",
    "                if hits_only:\n",
    "                    kwic = [left_context] + hit_lemmata + hit_pos + hit_words + [right_context]\n",
    "                else:\n",
    "                    kwic = hit_lemmata + hit_pos + hit_words\n",
    "                records.append(kwic)\n",
    "                layers_processed = 0     \n",
    "                    \n",
    "    if hits_only:\n",
    "        columns = [\"left context\"] + [\"lemma \" + str(n) for n in range(n_words_in_hit)] + [\"pos \" + str(n) for n in range(n_words_in_hit)] + [\"word \" + str(n) for n in range(n_words_in_hit)] + [\"right context\"]\n",
    "    else:\n",
    "        columns = [\"lemma \" + str(n) for n in range(n_words_in_hit)] + [\"pos \" + str(n) for n in range(n_words_in_hit)] + [\"word \" + str(n) for n in range(n_words_in_hit)]\n",
    "    \n",
    "    next_pos = 0\n",
    "    next_record_position = root.find(\"{http://docs.oasis-open.org/ns/search-ws/sruResponse}nextRecordPosition\")\n",
    "    if (next_record_position is not None):\n",
    "        next_pos = int(next_record_position.text)\n",
    "        \n",
    "    return pd.DataFrame(records, columns = columns), next_pos\n",
    "\n",
    "def _show_error_if_any(text):\n",
    "    # get error message out of xml and print it on screen\n",
    "    root = ET.fromstring(text)\n",
    "    msgs = []\n",
    "    for diagnostic in root.iter(\"{http://docs.oasis-open.org/ns/search-ws/diagnostic}diagnostic\"):\n",
    "        for msg in diagnostic.findall(\"{http://docs.oasis-open.org/ns/search-ws/diagnostic}message\"):\n",
    "            msg_text = msg.text if msg.text is not None else ''\n",
    "            msgs.append(msg_text)\n",
    "    if len(msgs) > 0:\n",
    "        print(\"; \".join(msgs))\n",
    "\n",
    "# View methods\n",
    "\n",
    "# results: dict of df's\n",
    "# labels: list of label corresponding to the df's in results\n",
    "def view_multiple_results(results, labels):\n",
    "    assert len(labels)==len(results)\n",
    "    for n,query in enumerate(results):\n",
    "        df = results[query]\n",
    "        if not df.empty:\n",
    "            display(HTML('Resultaten voor <b>' + labels[n] + \"</b>:\"))\n",
    "            display(df)\n",
    "            \n",
    "            \n",
    "            \n",
    "def get_frequency_list(lexicon, pos, corpus):\n",
    "    \n",
    "    # LEXICON: get a lemmata list to work with\n",
    "    df_lexicon = search_lexicon_alllemmata(lexicon, pos)\n",
    "    lexicon_lemmata_set = sorted( set([w.lower() for w in df_lexicon[\"writtenForm\"]]) )\n",
    "    lexicon_lemmata_arr= numpy.array(lexicon_lemmata_set)\n",
    "\n",
    "    # instantiate a dataframe for storing lemmata and frequencies\n",
    "    df_frequency_list = pd.DataFrame(index=lexicon_lemmata_arr, columns=['raw_freq'])\n",
    "    df_frequency_list.index.name = 'lemmata'\n",
    "\n",
    "    # CORPUS: loop through lemmata list, query the corpus with that lemma, and count the results\n",
    "\n",
    "    # It's a good idea to work with more than one lemma at once!\n",
    "    nr_of_lemmata_to_query_atonce = 100\n",
    "    \n",
    "    # loop over lemmata list \n",
    "    for i in range(0, len(lexicon_lemmata_set), nr_of_lemmata_to_query_atonce):\n",
    "        # slice to small sets of lemmata to query at once\n",
    "        small_lemmata_set = set( lexicon_lemmata_arr[i : i+nr_of_lemmata_to_query_atonce] )    \n",
    "\n",
    "        # join set of lemmata to send them in a query all at once\n",
    "        # beware: single quotes need escaping\n",
    "        lemmata_list = \"|\".join(small_lemmata_set).replace(\"'\", \"\\\\\\\\'\")\n",
    "        df_corpus = search_corpus(r'[lemma=\"' + lemmata_list + r'\"]', corpus)\n",
    "\n",
    "        # store frequencies\n",
    "        if (len(df_corpus)>0):\n",
    "            for one_lemma in small_lemmata_set: \n",
    "                raw_freq = len(df_corpus[df_corpus['lemma 0'] == one_lemma])\n",
    "                df_frequency_list.at[one_lemma, 'raw_freq'] = raw_freq \n",
    "                \n",
    "    # final step: compute rank\n",
    "    # this is needed to be able to compare different frequency lists \n",
    "    # with each other (which we could achieve by computing a rank diff)\n",
    "    df_frequency_list['rank'] = df_frequency_list['raw_freq'].rank(ascending = False).astype(int)\n",
    "    \n",
    "    return df_frequency_list;\n",
    "\n",
    "\n",
    "def get_missing_wordforms(lexicon, pos, corpus):\n",
    "    \n",
    "    # LEXICON: get a lemmata list to work with\n",
    "    df_lexicon = search_lexicon_alllemmata(lexicon, pos)\n",
    "    lexicon_lemmata_set = sorted( set([w.lower() for w in df_lexicon[\"writtenForm\"]]) )\n",
    "    lexicon_lemmata_arr= numpy.array(lexicon_lemmata_set)\n",
    "    \n",
    "    # instantiate a dataframe for storing lemmata and wordforms\n",
    "    df_enriched_lexicon = pd.DataFrame(index=lexicon_lemmata_arr, columns=['lemma', 'pos', 'known_wordforms', 'unknown_wordforms'])\n",
    "    df_enriched_lexicon.index.name = 'lemmata'\n",
    "    \n",
    "    # CORPUS: loop through lemmata list, query the corpus with that lemma, \n",
    "    # and compute difference between both\n",
    "\n",
    "    # It's a good idea to work with more than one lemma at once!\n",
    "    nr_of_lemmata_to_query_atonce = 100\n",
    "    \n",
    "    # loop over lemmata list \n",
    "    for i in range(0, len(lexicon_lemmata_set), nr_of_lemmata_to_query_atonce):\n",
    "        # slice to small sets of lemmata to query at once\n",
    "        small_lemmata_set = set( lexicon_lemmata_arr[i : i+nr_of_lemmata_to_query_atonce] )    \n",
    "        \n",
    "        # join set of lemmata to send them in a query all at once\n",
    "        # beware: single quotes need escaping\n",
    "        lemmata_list = \"|\".join(small_lemmata_set).replace(\"'\", \"\\\\\\\\'\")\n",
    "        df_corpus = search_corpus(r'[lemma=\"' + lemmata_list + r'\"]', corpus)\n",
    "        \n",
    "        # process results\n",
    "        if (len(df_corpus)>0):\n",
    "            for one_lemma in small_lemmata_set: \n",
    "                \n",
    "                # look up the known wordforms in the lexicon\n",
    "                query = lexicon_query(one_lemma, pos, lexicon)\n",
    "                df_known_wordforms = search_lexicon(query, lexicon)\n",
    "                \n",
    "                if (len(df_known_wordforms) != 0):\n",
    "                    known_wordforms = set( df_known_wordforms['wordform'].str.lower() )\n",
    "                    # find the wordforms in the corpus\n",
    "                    corpus_wordforms = set( (df_corpus[df_corpus['lemma 0'] == one_lemma])['word 0'].str.lower() )\n",
    "                    # determine which corpus wordforms are not in lexicon wordforms\n",
    "                    unknown_wordforms = corpus_wordforms.difference(known_wordforms)\n",
    "\n",
    "                    if (len(unknown_wordforms) !=0):\n",
    "                        # store the results\n",
    "                        df_enriched_lexicon.at[one_lemma, 'lemma'] = one_lemma\n",
    "                        df_enriched_lexicon.at[one_lemma, 'pos'] = pos\n",
    "                        df_enriched_lexicon.at[one_lemma, 'known_wordforms'] = known_wordforms\n",
    "                        df_enriched_lexicon.at[one_lemma, 'unknown_wordforms'] = unknown_wordforms\n",
    "                \n",
    "    # return non-empty results, t.i. cases in which we found some wordforms\n",
    "    return df_enriched_lexicon[ df_enriched_lexicon['unknown_wordforms'].notnull() ]\n",
    "        \n",
    "    \n",
    "def get_rank_diff(df1, df2):\n",
    "    \n",
    "    # find lemmata shared by both dataframes: computing ranks diffs is only possible\n",
    "    # when dealing with lemmata which are in both frames\n",
    "    lemmata_list1 = set(df1.index.tolist())\n",
    "    lemmata_list2 = set(df2.index.tolist())\n",
    "    common_lemmata_list = list( lemmata_list1.intersection(lemmata_list2) )\n",
    "    \n",
    "    # build dataframes limited to the common lemmata\n",
    "    limited_df1 = df1.loc[ common_lemmata_list , : ]\n",
    "    limited_df2 = df2.loc[ common_lemmata_list , : ]\n",
    "    \n",
    "    # recompute ranks in both dataframes, because in each frame the original ranks were\n",
    "    # computed with a lemmata list which might be larger than the lemmata list common\n",
    "    # to both dataframes\n",
    "    \n",
    "    limited_df1['rank'] = limited_df1['raw_freq'].rank(ascending = False).astype(int)\n",
    "    limited_df2['rank'] = limited_df2['raw_freq'].rank(ascending = False).astype(int)\n",
    "    \n",
    "    # instantiate a dataframe for storing lemmata and rank diffs\n",
    "    df_rankdiffs = pd.DataFrame(index=common_lemmata_list, columns=['rank_1', 'rank_2', 'rank_diff'])\n",
    "    df_rankdiffs.index.name = 'lemmata'\n",
    "    \n",
    "    df_rankdiffs['rank_1'] = limited_df1['rank']\n",
    "    df_rankdiffs['rank_2'] = limited_df2['rank']\n",
    "    df_rankdiffs['rank_diff'] = pd.DataFrame.abs( df_rankdiffs['rank_1'] - df_rankdiffs['rank_2'] )\n",
    "    \n",
    "    return df_rankdiffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library functions: UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T15:54:30.161099Z",
     "start_time": "2019-01-22T15:54:30.147834Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from pathlib import Path\n",
    "from IPython.display import Javascript\n",
    "from IPython.core.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEFAULT_QUERY = r'[lemma=\"boek\" & pos=\"verb\"]' #r'[lemma=\"boeken\" pos=\"verb\"]'\n",
    "DEFAULT_CORPUS = \"chn\"\n",
    "\n",
    "\n",
    "\n",
    "def create_corpus_ui():\n",
    "    # Create UI elements\n",
    "    corpusQueryField = widgets.Text(description=\"<b>CQL query:</b>\", value=DEFAULT_QUERY)\n",
    "    corpusField = widgets.Dropdown(\n",
    "        options=AVAILABLE_CORPORA,\n",
    "        value=DEFAULT_CORPUS,\n",
    "        description='<b>Corpus:</b>',\n",
    "    )\n",
    "    '''corpusSearchButton = widgets.Button(\n",
    "        description='Search',\n",
    "        button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Search',\n",
    "    )\n",
    "    # Handle events\n",
    "    corpusSearchButton.on_click(corpus_search)'''\n",
    "    \n",
    "    # Stack UI elements in vertical box and display\n",
    "    corpusUiBox = widgets.VBox([corpusQueryField,corpusField])\n",
    "    display(corpusUiBox)\n",
    "    \n",
    "    # Return fields, so their contents are accessible from the global namespace of the Notebook\n",
    "    return corpusQueryField, corpusField\n",
    "\n",
    "def create_lexicon_ui():\n",
    "    DEFAULT_SEARCHWORD = 'boek'\n",
    "    DEFAULT_LEXICON = \"diamant\"\n",
    "\n",
    "    # Create UI elements\n",
    "    searchWordField = widgets.Text(description=\"<b>Word:</b>\", value=DEFAULT_SEARCHWORD)\n",
    "    lexiconField = widgets.Dropdown(\n",
    "        options=['anw', 'celex', 'diamant', 'duelme', 'molex'],\n",
    "        value=DEFAULT_LEXICON,\n",
    "        description='<b>Lexicon:</b>',\n",
    "    )\n",
    "    '''lexSearchButton = widgets.Button(\n",
    "        description='Search',\n",
    "        button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Search',\n",
    "    )\n",
    "    # Handle events\n",
    "    lexSearchButton.on_click(lexicon_search)'''\n",
    "    # Stack UI elements in vertical box and display\n",
    "    lexUiBox = widgets.VBox([searchWordField,lexiconField])\n",
    "    display(lexUiBox)\n",
    "    return searchWordField, lexiconField\n",
    "\n",
    "\n",
    "def create_save_dataframe_ui(df):\n",
    "    # build ui for saving results\n",
    "    DEFAULT_FILENAME = 'mijn_resultaten.csv'\n",
    "    saveResultsCaption = widgets.Label(value='Sla uw resultaten op:')\n",
    "    fileNameField = widgets.Text(value=DEFAULT_FILENAME)\n",
    "    savebutton = widgets.Button(\n",
    "        description='Bestand opslaan',\n",
    "        disabled=False,\n",
    "        button_style='warning', \n",
    "        tooltip=DEFAULT_FILENAME,  # trick to pass filename to button widget\n",
    "        icon=''\n",
    "    )\n",
    "    # inject dataframe into button object\n",
    "    savebutton.df = df\n",
    "    # when the user types a new filename, it will be passed to the button tooltip property straight away\n",
    "    fileNameLink = widgets.jslink((fileNameField, 'value'), (savebutton, 'tooltip'))\n",
    "    # click event with callback\n",
    "    savebutton.on_click( _save_dataframe )    \n",
    "    saveResultsBox = widgets.HBox([saveResultsCaption, fileNameField, savebutton])\n",
    "    display(saveResultsBox)    \n",
    "    \n",
    "def _save_dataframe(button):\n",
    "    fileName = button.tooltip\n",
    "    # The result files can be saved locally or on the server:\n",
    "    # If result files are to be offered as downloads, set to True; otherwise set to False    \n",
    "    fileDownloadable = False\n",
    "    # specify paths here, if needed:\n",
    "    filePath_onServer = ''  # could be /path/to\n",
    "    filePath_default = ''\n",
    "    # compute full path given chosen mode\n",
    "    fullFileName = (filePath_onServer if fileDownloadable else filePath_default ) + fileName\n",
    "        \n",
    "    try:\n",
    "        button.df.to_csv( fullFileName, index=False)\n",
    "        # confirm it all went well\n",
    "        print(fileName + \" saved\")    \n",
    "        button.button_style = 'success'\n",
    "        button.icon = 'check'\n",
    "        # trick: https://stackoverflow.com/questions/31893930/download-csv-from-an-ipython-notebook\n",
    "        if (fileDownloadable):\n",
    "            downloadableFiles = FileLinks(filePath_onServer)\n",
    "            display(downloadableFiles)\n",
    "    except Exception as e:\n",
    "        button.button_style = 'danger'\n",
    "        raise ValueError(\"An error occured when saving \" + fileName + \": \"+ str(e))    \n",
    "\n",
    "    \n",
    "    \n",
    "def load_dataframe(filepath):\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(filepath + \" loaded successfully\")            \n",
    "    except Exception as e:\n",
    "        raise ValueError(\"An error occured when loading \" + filepath + \": \"+ str(e))\n",
    "    finally:\n",
    "        return df\n",
    "\n",
    "'''\n",
    "Args:\n",
    "    df: DataFrame to be displayed\n",
    "    columns: columns to display, or None to display all columns\n",
    "    title: Title displayed\n",
    "    mode: Way of displaying, one of 'table' (default) or 'chart'\n",
    "'''\n",
    "def display_df(df, columns=None, title=None, mode='table'):\n",
    "    \n",
    "    if columns is not None:\n",
    "        df_display=df[columns]\n",
    "    else:\n",
    "        df_display = df\n",
    "    \n",
    "    # chart mode\n",
    "    if mode == 'chart':\n",
    "        plt.figure()\n",
    "        df_display.plot.barh().set_title(title)\n",
    "    \n",
    "    # table mode (default)\n",
    "    else:    \n",
    "        if title is not None:\n",
    "            display(HTML(\"<b>%s</b>\" % title))        \n",
    "\n",
    "        display(df_display)\n",
    "    \n",
    "    # eventually, give UI to save data\n",
    "    create_save_dataframe_ui(df_display)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library functions: Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T14:35:34.495485Z",
     "start_time": "2019-01-22T14:35:34.474233Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def containsRegex(word):\n",
    "    return ( word.find('^')>-1 or\n",
    "            word.find('$')>-1 or \n",
    "            re.match(\"\\(.+?\\)\", word) or\n",
    "            re.match(\"\\[.+?\\]\", word) or\n",
    "            re.match(\"[\\+*]\", word) )\n",
    "                     \n",
    "def lexicon_query(word, pos, lexicon):\n",
    "    if (lexicon==\"anw\"):\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart = \"\"\"FILTER ( regex(?lemma, \\\"\"\"\"+word+\"\"\"\\\") || regex(?definition, \\\"\"\"\"+word+\"\"\"\\\") ) . \"\"\"\n",
    "        if (exactsearch == True):\n",
    "              subpart =  \"\"\"\n",
    "                { { ?lemId rdfs:label ?lemma .  \n",
    "                values ?lemma { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } }                 \n",
    "                UNION\n",
    "                { ?definitionId lemon:value ?definition .\n",
    "                values ?definition { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } } .\n",
    "                \"\"\"               \n",
    "        query = \"\"\"PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "                  PREFIX anw: <http://rdf.ivdnt.org/lexica/anw>\n",
    "                  PREFIX anwsch: <http://rdf.ivdnt.org/schema/anw/>\n",
    "                  PREFIX lemon: <http://lemon-model.net/lemon#>\n",
    "                  \n",
    "                  SELECT ?lemId ?lemma ?writtenForm ?definition concat('', ?definitionComplement) as ?definitionComplement\n",
    "                  FROM <http://rdf.ivdnt.org/lexica/anw>\n",
    "                  WHERE {\n",
    "                      ?lemId rdfs:label ?lemma .\n",
    "                      ?lemId ontolex:sense ?senseId .\n",
    "                      ?senseId lemon:definition ?definitionId .\n",
    "                      ?definitionId lemon:value ?definition .\n",
    "                      OPTIONAL { ?definitionId anwsch:definitionComplement ?definitionComplement .}\n",
    "                      OPTIONAL { ?lemId ontolex:canonicalForm ?lemCFId . \n",
    "                          ?lemCFId ontolex:writtenRepresentation ?writtenForm . }\n",
    "                      \"\"\"+subpart+\"\"\"\n",
    "                      }\"\"\"\n",
    "    elif (lexicon==\"diamant\"):\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart1 = \"\"\"?n_form ontolex:writtenRep ?n_ontolex_writtenRep . \n",
    "            FILTER regex(?n_ontolex_writtenRep, \\\"\"\"\"+word+\"\"\"\\\") . \"\"\"\n",
    "        subpart2 = \"\"\"?n_syndef diamant:definitionText ?n_syndef_definitionText .  \n",
    "            FILTER regex(?n_ontolex_writtenRep, \\\"\"\"\"+word+\"\"\"\\\") . \"\"\"\n",
    "        if (exactsearch == True):\n",
    "            subpart1 =  \"\"\"\n",
    "                { ?n_form ontolex:writtenRep ?n_ontolex_writtenRep . \n",
    "                values ?n_ontolex_writtenRep { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } \n",
    "                \"\"\"                \n",
    "            subpart2 = \"\"\"\n",
    "                { ?n_syndef diamant:definitionText ?n_syndef_definitionText . \n",
    "                values ?n_syndef_definitionText { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } \n",
    "                \"\"\"\n",
    "        query = \"\"\"\n",
    "        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "        prefix prov: <http://www.w3.org/ns/prov#>\n",
    "        prefix diamant: <http://rdf.ivdnt.org/schema/diamant#>\n",
    "        prefix lexinfo: <http://www.lexinfo.net/ontology/2.0/lexinfo#>\n",
    "        prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "        prefix lemon: <http://lemon-model.net/lemon#>\n",
    "        prefix ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "        prefix ud: <http://universaldependencies.org/u/pos/>\n",
    "        prefix skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "        prefix dcterms: <http://purl.org/dc/terms/>\n",
    "        prefix dc: <http://purl.org/dc/terms/>\n",
    "\n",
    "        select ?n_entry ?n_form ?n_ontolex_writtenRep ?n_syndef ?n_sensedef ?n_sensedef_definitionText ?n_syndef_definitionText ?n_sense ?inputMode ?wy_f_show ?wy_t_show\n",
    "        where\n",
    "        {\n",
    "        graph ?g\n",
    "        {\n",
    "        {\n",
    "            \"\"\" + subpart1 + \"\"\"\n",
    "            { ?n_entry a ontolex:LexicalEntry} .\n",
    "            { ?n_form a ontolex:Form} .\n",
    "            { ?n_sense a ontolex:LexicalSense} .\n",
    "            { ?n_syndef a diamant:SynonymDefinition} .\n",
    "            { ?n_sensedef a lemon:SenseDefinition} .\n",
    "            { ?n_syndef diamant:definitionText ?n_syndef_definitionText } .\n",
    "            { ?n_sensedef diamant:definitionText ?n_sensedef_definitionText } .\n",
    "            { ?n_entry ontolex:canonicalForm ?n_form } .\n",
    "            { ?n_entry ontolex:sense ?n_sense } .\n",
    "            { ?n_sense lemon:definition ?n_syndef } .\n",
    "            { ?n_sense lemon:definition ?n_sensedef } .\n",
    "              ?n_sense diamant:attestation ?n_attest_show .\n",
    "              ?n_sense diamant:attestation ?n_attest_filter .\n",
    "              ?n_attest_show diamant:text ?n_q_show .\n",
    "              ?n_attest_filter diamant:text ?n_q_filter .\n",
    "              ?n_attest_show a diamant:Attestation .\n",
    "              ?n_attest_filter a diamant:Attestation .\n",
    "              ?n_q_filter a diamant:Quotation .\n",
    "              ?n_q_show a diamant:Quotation .\n",
    "              ?n_q_filter diamant:witnessYearFrom ?wy_f_filter .\n",
    "              ?n_q_filter diamant:witnessYearTo ?wy_t_filter .\n",
    "              ?n_q_show diamant:witnessYearFrom ?wy_f_show .\n",
    "              ?n_q_show diamant:witnessYearTo ?wy_t_show .\n",
    "              FILTER (xsd:integer(?wy_f_show) >= 1200)\n",
    "              FILTER (xsd:integer(?wy_t_show) >= 1200)\n",
    "              FILTER (xsd:integer(?wy_f_show) <= 2018)\n",
    "              FILTER (xsd:integer(?wy_t_show) <= 2018)\n",
    "            { bind(\"lemma\" as ?inputMode) } .\n",
    "            } UNION\n",
    "          {\n",
    "            \"\"\" + subpart2 + \"\"\"\n",
    "            { ?n_sense a ontolex:LexicalSense} .\n",
    "            { ?n_syndef a diamant:SynonymDefinition} .\n",
    "            { ?n_sensedef a lemon:SenseDefinition} .\n",
    "            { ?n_form a ontolex:Form} .\n",
    "            { ?n_form ontolex:writtenRep ?n_ontolex_writtenRep } .  { ?n_entry a ontolex:LexicalEntry} .\n",
    "            { ?n_entry ontolex:sense ?n_sense } .\n",
    "            { ?n_sense lemon:definition ?n_syndef } .\n",
    "            { ?n_sense lemon:definition ?n_sensedef } .\n",
    "            { ?n_sensedef diamant:definitionText ?n_sensedef_definitionText } .\n",
    "            { ?n_entry ontolex:canonicalForm ?n_form } .\n",
    "            ?n_sense diamant:attestation ?n_attest_show .\n",
    "            ?n_sense diamant:attestation ?n_attest_filter .\n",
    "            ?n_attest_filter diamant:text ?n_q_filter .\n",
    "            ?n_attest_show diamant:text ?n_q_show .\n",
    "            ?n_q_filter diamant:witnessYearFrom ?wy_f_filter .\n",
    "            ?n_q_filter diamant:witnessYearTo ?wy_t_filter .\n",
    "            ?n_q_show diamant:witnessYearFrom ?wy_f_show .\n",
    "            ?n_q_show diamant:witnessYearTo ?wy_t_show .\n",
    "            ?n_attest_show a diamant:Attestation .\n",
    "            ?n_attest_filter a diamant:Attestation .\n",
    "            ?n_q_filter a diamant:Quotation .\n",
    "            ?n_q_show a diamant:Quotation .\n",
    "            FILTER (xsd:integer(?wy_f_show) >= 1200)\n",
    "            FILTER (xsd:integer(?wy_t_show) >= 1200)\n",
    "            FILTER (xsd:integer(?wy_f_show) <= 2018)\n",
    "            FILTER (xsd:integer(?wy_t_show) <= 2018)\n",
    "          { bind(\"defText\" as ?inputMode) } .\n",
    "            }\n",
    "        }\n",
    "        }\"\"\"\n",
    "    elif (lexicon==\"molex\"):\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart1 = \"\"\"\"\"\"\n",
    "        subpart2 = \"\"\"\"\"\"\n",
    "        subpartPos = \"\"\"\"\"\"\n",
    "        if (word != ''):\n",
    "            if (exactsearch == True):\n",
    "                subpart1 =  \"\"\"\n",
    "                    { ?lemCFId ontolex:writtenRep ?lemma . \n",
    "                    values ?lemma { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } \n",
    "                    UNION\n",
    "                    { ?wordformId ontolex:writtenRep ?wordform . \n",
    "                    values ?wordform { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } .\n",
    "                    \"\"\"        \n",
    "            else:\n",
    "                subpart2 = \"\"\"FILTER ( regex(?lemma, \\\"\"\"\"+word+\"\"\"\\\") || regex(?wordform, \\\"\"\"\"+word+\"\"\"\\\") ) . \"\"\"\n",
    "        if (pos is not None and pos != ''):\n",
    "            subpartPos = \"\"\"FILTER ( regex(?lemPos, \\\"\"\"\"+pos+\"\"\"$\\\") ) .\"\"\"\n",
    "        query = \"\"\"\n",
    "            PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "            PREFIX UD: <http://universaldependencies.org/u/>\n",
    "            PREFIX diamant: <http://rdf.ivdnt.org/schema/diamant#>\n",
    "            \n",
    "            SELECT ?lemEntryId ?lemma ?lemPos ?wordformId ?wordform ?hyphenation ?wordformPos ?Gender ?Number\n",
    "            FROM <http://rdf.ivdnt.org/lexica/molex>\n",
    "            WHERE\n",
    "            {\n",
    "            ?lemEntryId ontolex:canonicalForm ?lemCFId .\n",
    "            ?lemCFId ontolex:writtenRep ?lemma .\n",
    "            \"\"\"+subpart1+\"\"\"\n",
    "            OPTIONAL {?lemEntryId UD:Gender ?Gender .}\n",
    "            OPTIONAL {?lemEntryId UD:VerbForm ?verbform .}\n",
    "            ?lemEntryId UD:pos ?lemPos .\n",
    "            \"\"\"+subpartPos+\"\"\"\n",
    "            ?lemEntryId ontolex:lexicalForm ?wordformId .\n",
    "            ?wordformId UD:pos ?wordformPos .\n",
    "            OPTIONAL {?wordformId UD:Number ?Number .}\n",
    "            OPTIONAL {?wordformId ontolex:writtenRep ?wordform .}\n",
    "            OPTIONAL {?wordformId diamant:hyphenation ?hyphenation .}\n",
    "            \"\"\"+subpart2+\"\"\"\n",
    "            }\n",
    "        \"\"\"\n",
    "    elif (lexicon==\"duelme\"):\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart = \"\"\"FILTER ( regex(?lemma, \\\"\"\"\"+word+\"\"\"\\\") || regex(?wordform, \\\"\"\"\"+word+\"\"\"\\\") ) .\"\"\"\n",
    "        if (exactsearch == True):\n",
    "            subpart =  \"\"\"\n",
    "                { ?y lmf:hasLemma ?dl .  \n",
    "                values ?dl { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } }                 \n",
    "                \"\"\"        \n",
    "        query = \"\"\"\n",
    "            PREFIX duelme: <http://rdf.ivdnt.org/lexica/duelme>\n",
    "            PREFIX intskos: <http://ivdnt.org/schema/lexica#>\n",
    "            PREFIX lmf: <http://www.lexinfo.net/lmf>\n",
    "            PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "            PREFIX UD: <http://rdf.ivdnt.org/vocabs/UniversalDependencies2#>\n",
    "            \n",
    "            SELECT ?exampleSentence ?lemma ?gender ?number\n",
    "            WHERE  {\n",
    "                  ?d intskos:ExampleSentence ?exampleSentence .\n",
    "                  ?d lmf:ListOfComponents [lmf:Component ?y] .\n",
    "                  ?y lmf:hasLemma ?lemma . \n",
    "                  OPTIONAL {?y UD:Gender ?gender}\n",
    "                  OPTIONAL {?y UD:Number ?number}\n",
    "            \"\"\"+subpart+\"\"\"\n",
    "            }\n",
    "        \"\"\"\n",
    "    elif (lexicon==\"celex\"):\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart = \"\"\"FILTER ( regex(?lemma, \\\"\"\"\"+word+\"\"\"\\\") ) . \"\"\"\n",
    "        if (exactsearch == True):\n",
    "            subpart =  \"\"\"\n",
    "                { ?lemmaId ontolex:canonicalForm [ontolex:writtenRep ?lemma] .  \n",
    "                values ?lemma { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } }                 \n",
    "                \"\"\"        \n",
    "        query = \"\"\"\n",
    "            PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "            PREFIX celex: <http://rdf.ivdnt.org/lexica/celex>\n",
    "            PREFIX UD: <http://rdf.ivdnt.org/vocabs/UniversalDependencies2#>\n",
    "            PREFIX decomp: <http://www.w3.org/ns/lemon/decomp#>\n",
    "            PREFIX gold: <http://purl.org/linguistics/gold#>\n",
    "            \n",
    "            SELECT DISTINCT ?lemmaId ?lemma ?wordformId ?wordform ?number ?gender concat('', ?subLemmata) AS ?subLemmata\n",
    "            WHERE  {\n",
    "                ?lemmaId ontolex:canonicalForm [ontolex:writtenRep ?lemma] .\n",
    "                \"\"\"+subpart+\"\"\"\n",
    "                BIND( ?lemmaId AS ?lemmaIdIRI ).\n",
    "                ?lemmaId ontolex:lexicalForm ?wordformId .\n",
    "                ?wordformId ontolex:writtenRep ?wordform .\n",
    "                OPTIONAL {?wordformId UD:Number ?number} .\n",
    "                OPTIONAL {\n",
    "                    ?lemmaId UD:Gender ?g . \n",
    "                        bind( \n",
    "                            if(?g = UD:Fem_Gender, \n",
    "                            UD:Com_Gender, \n",
    "                                if(?g = UD:Masc_Gender,\n",
    "                                    UD:Com_Gender,\n",
    "                                    UD:Neut_Gender\n",
    "                                )\n",
    "                            )\n",
    "                            AS ?gender\n",
    "                        )\n",
    "                }\n",
    "                OPTIONAL {\n",
    "                    SELECT ?lemmaIdIRI (group_concat(DISTINCT concat(?partNr,\":\",?subLemma);separator=\" + \") as ?subLemmata)\n",
    "                    WHERE {\n",
    "                        SELECT ?lemmaIdIRI ?celexComp ?aWordformId ?subLemma ?partNr\n",
    "                        WHERE {\n",
    "                                {\n",
    "                                ?lemmaIdIRI ontolex:lexicalForm ?aWordformId . \n",
    "                                ?lemmaIdIRI decomp:constituent ?celexComp .\n",
    "                                OPTIONAL { ?celexComp gold:stem [ontolex:writtenRep ?subLemma] . }\n",
    "                                OPTIONAL { ?celexComp decomp:correspondsTo [ ontolex:canonicalForm [ontolex:writtenRep ?subLemma]] . }\n",
    "                                }\n",
    "                                {\n",
    "                                    {\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_1> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_2> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_3> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_4> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_5> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_6> ?celexComp .}                                        \n",
    "                                    }\n",
    "                                ?lemmaIdIRI ?rdfsynt ?celexComp .\n",
    "                                BIND(IF(STRSTARTS(str(?rdfsynt), \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"), replace(STRAFTER(str(?rdfsynt), \"#\"), \"_\", \"\"), \"999\") AS ?partNr) .\n",
    "                                MINUS {\n",
    "                                    ?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#0> ?celexComp .\n",
    "                                    }\n",
    "                                }\n",
    "                            FILTER (?partNr != \"999\") .\n",
    "                            }\n",
    "                            ORDER BY ?partNr\n",
    "                            }\n",
    "                        GROUP BY ?aWordformId ?lemmaIdIRI\n",
    "                    }\n",
    "            }\n",
    "        \"\"\"\n",
    "        \n",
    "    return query\n",
    "\n",
    "def corpus_query_lemma(word):\n",
    "    return r'[lemma=\"'+ word + r'\"]'\n",
    "\n",
    "def corpus_query_wordform(word):\n",
    "    return r'[word=\"'+ word + r'\"]'\n",
    "\n",
    "def lexicon_query_alllemmata(lexicon, pos):\n",
    "    if (lexicon==\"anw\"):\n",
    "        query = \"\"\"PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "                  PREFIX anw: <http://rdf.ivdnt.org/lexica/anw>                  \n",
    "                  SELECT DISTINCT ?writtenForm\n",
    "                  FROM <http://rdf.ivdnt.org/lexica/anw>\n",
    "                  WHERE {\n",
    "                      ?lemId rdfs:label ?lemma .\n",
    "                      ?lemId ontolex:canonicalForm ?lemCFId . \n",
    "                      ?lemCFId ontolex:writtenRepresentation ?writtenForm .\n",
    "                      }\n",
    "                      ORDER BY ?writtenForm\"\"\"\n",
    "    elif (lexicon==\"celex\"):\n",
    "        query = \"\"\"\n",
    "            PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "            \n",
    "            SELECT DISTINCT ?lemma AS ?writtenForm\n",
    "            WHERE  {\n",
    "                ?lemmaId ontolex:canonicalForm [ontolex:writtenRep ?lemma] .                \n",
    "                }\n",
    "            ORDER BY ?lemma\"\"\"\n",
    "    elif (lexicon==\"diamant\"):\n",
    "        query = \"\"\"\n",
    "        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "        prefix prov: <http://www.w3.org/ns/prov#>\n",
    "        prefix diamant: <http://rdf.ivdnt.org/schema/diamant#>\n",
    "        prefix lexinfo: <http://www.lexinfo.net/ontology/2.0/lexinfo#>\n",
    "        prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "        prefix lemon: <http://lemon-model.net/lemon#>\n",
    "        prefix ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "        prefix ud: <http://universaldependencies.org/u/pos/>\n",
    "        prefix skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "        prefix dcterms: <http://purl.org/dc/terms/>\n",
    "        prefix dc: <http://purl.org/dc/terms/>\n",
    "\n",
    "        select DISTINCT ?n_ontolex_writtenRep AS ?writtenForm\n",
    "        where\n",
    "        {\n",
    "        graph ?g\n",
    "        {\n",
    "        {\n",
    "            { ?n_form ontolex:writtenRep ?n_ontolex_writtenRep} .\n",
    "            { ?n_form a ontolex:Form} .\n",
    "        }\n",
    "        }\n",
    "        }\n",
    "        ORDER BY ?n_ontolex_writtenRep\n",
    "        LIMIT 10000\n",
    "        \"\"\"\n",
    "    elif (lexicon==\"duelme\"):\n",
    "        query = \"\"\"\n",
    "            PREFIX lmf: <http://www.lexinfo.net/lmf>            \n",
    "            SELECT DISTINCT ?lemma AS ?writtenForm\n",
    "            WHERE  {\n",
    "                  ?y lmf:hasLemma ?lemma . \n",
    "            }\n",
    "            ORDER BY ?lemma\"\"\"\n",
    "    elif (lexicon==\"molex\"):\n",
    "        pos_condition = \"\"\"\"\"\"\n",
    "        if pos is not None:\n",
    "            pos_condition = \"\"\"\n",
    "            {?lemEntryId UD:pos ?lemPos .\n",
    "            FILTER regex(?lemPos, '\"\"\"+pos+\"\"\"') } .\n",
    "            \"\"\"\n",
    "        query = \"\"\"\n",
    "                PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "                PREFIX UD: <http://universaldependencies.org/u/>\n",
    "                SELECT DISTINCT ?lemma AS ?writtenForm\n",
    "                FROM <http://rdf.ivdnt.org/lexica/molex>\n",
    "                WHERE\n",
    "                {\n",
    "                ?lemEntryId ontolex:canonicalForm ?lemCFId .\n",
    "                ?lemCFId ontolex:writtenRep ?lemma .  \n",
    "                \"\"\"+pos_condition+\"\"\"\n",
    "                }\n",
    "                 ORDER BY ?lemma\"\"\"\n",
    "    else:\n",
    "        raise ValueError(\"Lexicon \" + lexicon + \" not supported for querying all words.\")\n",
    "        \n",
    "    #print(query)\n",
    "    return query\n",
    "\n",
    "def columns_from_lex_query(lex_query):\n",
    "    # Get part after select, eg: \"?x ?y ?concat('',z) as ?a\"\n",
    "    select_string = re.search(r'select\\s+(?:distinct)*\\s*(.*)\\s*(?:where|from)', lex_query)\n",
    "    # Delete concat() part and following AS, because it can contain a space we do not want to split on\n",
    "    string_wh_concat = re.replace(r'concat\\(.*\\) AS', '', select_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the cell below to show the UI, and fill in your search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T14:35:35.930964Z",
     "start_time": "2019-01-22T14:35:35.909872Z"
    }
   },
   "outputs": [],
   "source": [
    "#from chaininglib import ui\n",
    "\n",
    "# Create corpus UI, creates references to field contents\n",
    "corpusQueryField, corpusField = create_corpus_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Click the cell below and press Run to perform the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T15:54:51.993481Z",
     "start_time": "2019-01-22T15:54:51.497985Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from chaininglib import search\n",
    "query= corpusQueryField.value\n",
    "corpus = corpusField.value\n",
    "df_corpus = search_corpus(query, corpus)\n",
    "#df_corpus = load_dataframe('mijn_resultaten.csv')\n",
    "display_df(df_corpus, title=\"Results:\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the cell below to show the UI, and fill in your search query in the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T14:35:39.078060Z",
     "start_time": "2019-01-22T14:35:39.061787Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from chaininglib import ui\n",
    "searchWordField, lexiconField = create_lexicon_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Click the cell below and press Run to perform the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T14:35:42.147205Z",
     "start_time": "2019-01-22T14:35:41.791699Z"
    }
   },
   "outputs": [],
   "source": [
    "#from chaininglib import queries, search\n",
    "\n",
    "search_word = searchWordField.value\n",
    "lexicon = lexiconField.value\n",
    "# USER: can replace this by own custom query\n",
    "query = lexicon_query(word=search_word, pos= '', lexicon=lexicon)\n",
    "\n",
    "df_lexicon = search_lexicon(query, lexicon)\n",
    "display(df_lexicon)\n",
    "#df_columns_list = list(df_lexicon.columns.values)\n",
    "#df_lexicon_in_columns = df_lexicon[df_columns_list]\n",
    "#display(df_lexicon_in_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 1 (parallel): Frequency of *puur*+verb and *zuiver*+verb compared\n",
    "* Below cell searches for *puur*+verb and for *zuiver*+verb in the CHN corpus\n",
    "* Compare frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T14:39:47.933723Z",
     "start_time": "2019-01-22T14:39:47.043682Z"
    }
   },
   "outputs": [],
   "source": [
    "#from chaininglib import search\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Word 1: puur\n",
    "word1= \"puur\"\n",
    "df_corpus1 = search_corpus('[word=\"' + word1 + r'\"][pos=\"verb\"]',corpus=\"chn\")\n",
    "display(HTML('<b>' + word1 + '</b>'))\n",
    "display(df_corpus1)\n",
    "\n",
    "# Word 2: zuiver\n",
    "word2 = \"zuiver\"\n",
    "df_corpus2 = search_corpus(r'[word=\"' + word2 + r'\"][pos=\"verb\"]',\"chn\")\n",
    "display(HTML('<b>' + word2 + '</b>'))\n",
    "display(df_corpus2)\n",
    "\n",
    "# Compute difference\n",
    "diff_left, diff_right, intersec = column_difference(df_corpus1[\"word 1\"], df_corpus2[\"word 1\"])\n",
    "# Elements of 1 that are not in 2\n",
    "display(HTML('Werkwoorden voor <b>' + word1 + '</b> niet in <b>' + word2 + '</b>: ' + \", \".join(diff_left)))\n",
    "# Elements of 2 that are not in 1\n",
    "display(HTML('Werkwoorden voor <b>' + word1 + '</b> niet in <b>' + word2 + '</b>: ' + \", \".join(diff_right)))\n",
    "# Elements both in 1 and 2\n",
    "display(HTML('Werkwoorden zowel voor <b>' + word1 + '</b> als voor <b>' + word2 + '</b>: ' + \", \".join(intersec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 2 (sequential): Retrieve synonyms from DiaMaNT, look up in Gysseling\n",
    "* Below cell searches for term \"boek\" in DiaMaNT, and looks up all variants in Gysseling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T14:35:56.234896Z",
     "start_time": "2019-01-22T14:35:45.746206Z"
    }
   },
   "outputs": [],
   "source": [
    "search_word = \"boek\"\n",
    "lexicon = \"diamant\"\n",
    "corpus= \"gysseling\"\n",
    "\n",
    "# First, lookup synonyms in DiaMaNT\n",
    "query = lexicon_query(word=search_word, pos= '', lexicon=lexicon)\n",
    "df_lexicon = search_lexicon(query, lexicon)\n",
    "syns = diamant_get_synonyms(df_lexicon) \n",
    "syns.add(search_word) # Also add search word itself\n",
    "display(HTML('Synoniemen voor <b>' + search_word + '</b>: ' + \", \".join(syns)))\n",
    "\n",
    "# Search for all synonyms in corpus\n",
    "## Create queries: search by lemma\n",
    "syns_queries = [corpus_query_lemma(syn) for syn in syns]\n",
    "## Search for all synonyms in corpus\n",
    "result_dict = search_corpus_multiple(syns_queries, corpus)\n",
    "view_multiple_results(result_dict, labels=list(syns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T16:24:19.655999Z",
     "start_time": "2019-01-11T16:24:19.645252Z"
    }
   },
   "source": [
    "## Case study (parallel) 3: Find corpus words not in lexicon; list most frequent ones.\n",
    "* Only parallel if you can ask the lexicon a list of all words.\n",
    "* Currently only working: ask DiaMaNT list of words (limited at 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T14:36:56.525588Z",
     "start_time": "2019-01-22T14:35:56.240217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Query lexicon to give list of all words\n",
    "lexicon=\"anw\"\n",
    "df_lexicon = search_lexicon_alllemmata(lexicon)\n",
    "## TODO: Why do double words appear?\n",
    "lexicon_set = sorted( set([w.lower() for w in df_lexicon[\"writtenForm\"]]) )\n",
    "display(lexicon_set)\n",
    "\n",
    "df_corpus = search_corpus_allwords(\"gysseling\", None)\n",
    "display(df_corpus)\n",
    "len(df_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T15:46:20.519833Z",
     "start_time": "2019-01-16T15:46:20.516208Z"
    }
   },
   "source": [
    "## Case study (sequential) 4: Find occurences of attributive adjectives not ending with -e, even though they are preceeded by a definite article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T14:53:04.668561Z",
     "start_time": "2019-01-22T14:52:54.047161Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_to_search=\"opensonar\"\n",
    "lexicon_to_search=\"molex\"\n",
    "\n",
    "# CORPUS: get [article + attributive adjective + nouns] combinations in which the adjective does not end with -e\n",
    "print('Stap 1:')\n",
    "df_corpus = search_corpus(r'[lemma=\"de|het\"][word=\"^g(.+)[^e]$\" & pos=\"ADJ\"][pos=\"NOUN\"]', corpus=corpus_to_search)\n",
    "display(df_corpus)\n",
    "\n",
    "# LEXICON: get adjectives the lemma of which does not end with -e\n",
    "query=lexicon_query('^g(.+)[^e]$', 'ADJ', lexicon_to_search)\n",
    "df_lexicon = search_lexicon(query, lexicon_to_search)\n",
    "display(df_lexicon)\n",
    "\n",
    "# LEXICON: get adjectives having a final -e in definite attributive use\n",
    "print('Filtering lexicon results')\n",
    "df_lexicon_form_e = filter_df(df_lexicon,column=\"wordform\",method=\"regex\", regex_or_set = 'e$')\n",
    "#final_e_condition=df_lexicon.wordform.str.contains('e$')\n",
    "#df = df_lexicon[final_e_condition]\n",
    "display(df_lexicon_form_e)\n",
    "\n",
    "# RESULT: get the records out of our first list in which the -e-less-adjectives match the lemma form of our last list\n",
    "print('Wanted list:')\n",
    "e_forms = list(df_lexicon_form_e.lemma)\n",
    "#no_final_e_condition = df_corpus['word 1'].isin(eless_forms)\n",
    "#result_df = df_corpus[no_final_e_condition]\n",
    "result_df = filter_df(df_corpus, column = \"word 1\", method=\"isin\", regex_or_set=e_forms)\n",
    "display( result_df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study (sequential) 5: (morphosyntactic lexicon and possibly unannotated corpus) Look up inflected forms and spelling variants for a given lemma in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T14:33:52.653090Z",
     "start_time": "2019-01-22T14:33:40.188Z"
    }
   },
   "outputs": [],
   "source": [
    "lexicon_to_search=\"molex\"\n",
    "corpus_to_search=\"chn\"\n",
    "\n",
    "##############################################\n",
    "# TODO  zelfde met meerdere lemmata en gegroepeerd \n",
    "##############################################\n",
    "\n",
    "lemma_to_look_for=\"denken\"\n",
    "\n",
    "# LEXICON: Search for the inflected forms of a lemma in a morphosyntactic lexicon\n",
    "query=lexicon_query(lemma_to_look_for, None, lexicon_to_search)\n",
    "df_lexicon = search_lexicon(query, lexicon_to_search)\n",
    "display(df_lexicon)\n",
    "\n",
    "# Put all inflected forms into a list\n",
    "inflected_wordforms = list(df_lexicon.wordform)\n",
    "\n",
    "# CORPUS: Look up the inflected forms in a (possibly unannotated) corpus\n",
    "# beware: If the corpus is not annotated, all we can do is searching for the inflected words\n",
    "#         But if the corpus is lemmatized, we have to make sure we're retrieving correct data by specifying the lemma as well\n",
    "annotated_corpus = True\n",
    "query = r'[lemma=\"'+lemma_to_look_for+r'\" & word=\"'+r\"|\".join(inflected_wordforms)+r'\"]' if annotated_corpus else r'[word=\"'+r\"|\".join(inflected_wordforms)+r'\"]'\n",
    "df_corpus = search_corpus(query, corpus=corpus_to_search)\n",
    "display(df_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study : Build frequency table of some corpus, based on lemma list of a given lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T14:33:52.653972Z",
     "start_time": "2019-01-22T14:33:40.190Z"
    }
   },
   "outputs": [],
   "source": [
    "base_lexicon=\"anw\"\n",
    "corpus_to_search1=\"opensonar\"\n",
    "corpus_to_search2=\"chn\"\n",
    "\n",
    "# build frequency tables of two corpora\n",
    "\n",
    "df_frequency_list1 = get_frequency_list(base_lexicon, \"NOUN\", corpus_to_search1)\n",
    "display( df_frequency_list1.sort_values(ascending=False,by=['raw_freq']).head(25) )\n",
    "display_df(df_frequency_list1.sort_values(ascending=True, by=['rank']).head(25), columns='raw_freq', title='chart df1', mode='chart' )\n",
    "\n",
    "df_frequency_list2 = get_frequency_list(base_lexicon, \"NOUN\", corpus_to_search2)\n",
    "display(df_frequency_list2.sort_values(ascending=False,by=['raw_freq']).head(25))\n",
    "display_df(df_frequency_list2.sort_values(ascending=True, by=['rank']).head(25), columns='raw_freq', title='chart df2', mode='chart' )\n",
    "\n",
    "\n",
    "# TODO: lemmata tonen die in 1 of 2 ontbreken\n",
    "\n",
    "# compute the rank diff of lemmata in frequency tables\n",
    "\n",
    "df_rankdiffs = get_rank_diff(df_frequency_list1, df_frequency_list2)\n",
    "\n",
    "display(df_rankdiffs.sort_values(by=['rank_diff']).head(25))\n",
    "display_df( df_rankdiffs.sort_values(ascending=False, by=['rank_diff']).head(25), columns='rank_diff', title='chart large diff', mode='chart' )\n",
    "display_df( df_rankdiffs.sort_values(ascending=True, by=['rank_diff']).head(25), columns='rank_diff', title='chart small diff', mode='chart' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: search in a corpus for wordforms of a lemma, which are not included of this lemma's paramadigm in a lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_lexicon=\"molex\"\n",
    "corpus_to_search=\"opensonar\"\n",
    "\n",
    "df = get_missing_wordforms(base_lexicon, \"VERB\", corpus_to_search)\n",
    "\n",
    "df.to_csv( \"missing_wordforms.csv\", index=False)\n",
    "#df = load_dataframe(\"missing_wordforms.csv\")\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: Train a tagger with data from an annotated corpus, an do something cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "base_lexicon=\"molex\"\n",
    "corpus_to_search=\"opensonar\"\n",
    "\n",
    "# we have a given word, let's say: \"loop\"\n",
    "some_word = \"loop\"\n",
    "\n",
    "# get the paradigm of the lemma our word is a part of\n",
    "query = lexicon_query(some_word, pos=None, lexicon=base_lexicon)\n",
    "df_paradigm = search_lexicon(query, base_lexicon)\n",
    "display(df_paradigm)\n",
    "\n",
    "# gather some pattern including our word, out of an annotated corpus\n",
    "# here: DET + ADJ + 'loop'\n",
    "corpus_query = corpus_query_wordform(some_word)\n",
    "df_corpus = search_corpus(corpus_query, corpus=corpus_to_search, hits_only=False)\n",
    "\n",
    "display(df_corpus)\n",
    "\n",
    "# Train a tagger with the corpus annotations\n",
    "# The input must be like: tagger.train([ [('today','NN'),('is','VBZ'),('good','JJ'),('day','NN')], [...] ])\n",
    "\n",
    "collocations = []\n",
    "nr_of_words_per_sentence = int( df_corpus.shape[1] / 3 )  # divided by the number of information layers (lemma, pos, wordform)\n",
    "print(\"nr_of_words_per_sentence = \" + str(nr_of_words_per_sentence))\n",
    "\n",
    "for index, row in df_corpus.iterrows():\n",
    "    one_collocation =  []\n",
    "    wrong = False\n",
    "    for i in range(0, nr_of_words_per_sentence, 1): \n",
    "        tuple = ( row['word '+str(i)], row['pos '+str(i)] )\n",
    "        one_collocation.append( tuple )\n",
    "        if (row['word '+str(i)] is None or row['pos '+str(i)] is None):\n",
    "            wrong = True\n",
    "    if wrong is False:\n",
    "        collocations.append(one_collocation)\n",
    "    \n",
    "print(collocations)\n",
    "\n",
    "print(type(collocations))\n",
    "\n",
    "\n",
    "print('training now...')\n",
    "tagger = PerceptronTagger(load=False)\n",
    "tagger.train(collocations)\n",
    "\n",
    "# Use the trained tagger to tag unknown collocations\n",
    "# The input must be like: tagger.tag(['today','is','a','beautiful','day'])\n",
    "\n",
    "print('tagging now...')\n",
    "sentence = 'Mijn buurman kijkt door de loop van zijn geweer'\n",
    "tagged_sentence = tagger.tag( sentence.split() )\n",
    "\n",
    "print(tagged_sentence)\n",
    "\n",
    "\n",
    "# Know we can lemmatize each occurence of our lemma in the new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs_env",
   "language": "python",
   "name": "cs_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
