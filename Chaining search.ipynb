{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaining search\n",
    "\n",
    "\n",
    "\n",
    "## Sphinx documentatie: https://pythonhosted.org/an_example_pypi_project/sphinx.html\n",
    "## in voorbeelden handige python functies opnemen\n",
    "## zoals ; .sort_values(ascending=False,by=['raw_freq']));  list enz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library functions: Search\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T14:23:23.808462Z",
     "start_time": "2019-02-08T14:23:23.521657Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "def filter_condition(df, column, method, regex_or_set):\n",
    "    '''\n",
    "    Helpfunction to build some condition to filter a Pandas DataFrame, according to a set of parameters\n",
    "    Args:\n",
    "        df: Pandas DataFrame to filter on\n",
    "        df: column on which we filter\n",
    "        method: \"contains_regex\" of \"isin_set\"\n",
    "        regex_or_set: Regular expression (if method==\"contains_regex\") or set (if method==\"isin_set\")\n",
    "    Returns:\n",
    "        a condition\n",
    "    '''\n",
    "    \n",
    "    if method==\"contains_regex\":\n",
    "        filter_condition = df[column].str.contains(regex_or_set)\n",
    "    elif method==\"isin_set\":\n",
    "        filter_condition = df[column].isin(regex_or_set)\n",
    "    else:\n",
    "        raise ValueError(\"method should be one of regex or isin\")\n",
    "    return filter_condition\n",
    "    \n",
    "    \n",
    "\n",
    "def concat_df(df_arr, keys_arr=None):\n",
    "    '''\n",
    "    This function concatenates two dataframes \n",
    "    Args:\n",
    "        df_arr: array of Pandas DataFrames\n",
    "        keys_arr: array of keys to assign to the records of each DataFrame, so we can still distinguish the original DataFrames\n",
    "    Returns:\n",
    "        a single Pandas DataFrame \n",
    "        \n",
    "    >>> new_df = concat_df( [dataframe1, dataframe2, dataframe3], ['chn corpus', 'nederlab', 'opensonar'] )\n",
    "    >>> display_df(new_df)\n",
    "    '''\n",
    "    # ref: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
    "    \n",
    "    if keys_arr is not None:\n",
    "        concat_df = pd.concat( df_arr, keys=keys_arr )\n",
    "    else:\n",
    "        concat_df = pd.concat( df_arr )\n",
    "    \n",
    "    return concat_df\n",
    "\n",
    "\n",
    "\n",
    "def join_df(df_arr, join_type=None):\n",
    "    \n",
    "    '''\n",
    "    This function joins two dataframes (=concat along axis 1) \n",
    "    Args:\n",
    "        df_arr: array of Pandas DataFrames\n",
    "        join_type: {inner, outer (default)}\n",
    "    Returns:\n",
    "        a single Pandas DataFrame \n",
    "        \n",
    "    >>> new_df = join_df( [dataframe1, dataframe2] )\n",
    "    >>> display_df(new_df)\n",
    "    '''\n",
    "    \n",
    "    # ref: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
    "    \n",
    "    if join_type is None:\n",
    "        concat_df = pd.concat( df_arr, axis=1 )\n",
    "    else:\n",
    "        concat_df = pd.concat( df_arr, axis=1, join=join_type )\n",
    "    \n",
    "    return concat_df\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def get_tagger(df_corpus):\n",
    "    '''\n",
    "    This function instantiates a tagger trained with some corpus annotations \n",
    "    Args:\n",
    "        df_corpus: Pandas DataFrame with annotated corpus data\n",
    "    Returns:\n",
    "        a PerceptronTagger instance \n",
    "    \n",
    "    >>> tagger = get_tagger(df_corpus)  # df_corpus containes a Pandas DataFrame with lots of corpus data\n",
    "    >>> sentence = 'Here is some beautiful sentence'\n",
    "    >>> tagged_sentence = tagger.tag( sentence.split() )\n",
    "    >>> print(tagged_sentence) \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # The corpus DataFrame consists of a number of sentences (rows) with a fixed number of tokens.\n",
    "    # Each token has a fixed number of layers holding info like: lemma, wordform or part-of-speech. \n",
    "    # As a result, the number of columns of each row = [number of tokens] x [number of layers]\n",
    "    \n",
    "    # To be able to feed the tagger correctly, we need to compute the number of layers,\n",
    "    # so we can infer the number of tokens the sentences hold. This is because\n",
    "    # the tagger expects us to feed it with arrays with length = [number of tokens], as elements of\n",
    "    # one single array holding all sentences arrays (see below).\n",
    "    \n",
    "    # So, determine how many layers (lemma, pos, wordform) we have \n",
    "    column_names = list(df_corpus.columns.values)\n",
    "    for n, val in enumerate(column_names):\n",
    "        # remove the numbers at the end of the layers names (lemma 1, lemma 2, ..., pos 1, pos 2, ...)\n",
    "        # so we end up with clean layers name only\n",
    "        column_names[n] = val.split(' ')[0] \n",
    "    number_of_layers = len(set(column_names))\n",
    "\n",
    "    # Now we can determine the standard length of our corpus sentences: that can be computed \n",
    "    # by dividing the number of columns of the corpus DataFrame by the number of layers\n",
    "    # we just computed.\n",
    "    sentences = []\n",
    "    nr_of_words_per_sentence = int( df_corpus.shape[1] / number_of_layers )  \n",
    "\n",
    "    # Build training data for the tagger in the right format\n",
    "    # The input must be like: [ [('today','NN'),('is','VBZ'),('good','JJ'),('day','NN')], [...] ]\n",
    "    for index, row in df_corpus.iterrows():\n",
    "        one_sentence =  []\n",
    "        wrong = False\n",
    "        for i in range(0, nr_of_words_per_sentence, 1): \n",
    "            tuple = ( row['word '+str(i)], row['pos '+str(i)] )\n",
    "            one_sentence.append( tuple )\n",
    "            if (row['word '+str(i)] is None or row['pos '+str(i)] is None):\n",
    "                wrong = True\n",
    "        if wrong is False:\n",
    "            sentences.append(one_sentence)\n",
    "\n",
    "    # Instantiate and train the tagger now\n",
    "    tagger = PerceptronTagger(load=False)\n",
    "    tagger.train(sentences)\n",
    "    \n",
    "    return tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:42:14.160612Z",
     "start_time": "2019-02-08T15:42:13.887816Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import urllib\n",
    "#import wx   # for interaction popups          TODO -> omzetten naar JS of zo\n",
    "import itertools # for frequency list function and from_iterable\n",
    "import numpy     # idem\n",
    "from IPython.display import FileLink, FileLinks\n",
    "AVAILABLE_CORPORA = {'chn':'http://svprmc05.inl.nl/blacklab-server/chn',\n",
    "                     'opensonar':'http://172.16.10.93:8080/blacklab-server/opensonar',\n",
    "                     'zeebrieven':'http://svprmc20.ivdnt.org/blacklab-server/zeebrieven',\n",
    "                     'gysseling':'http://svprmc20.ivdnt.org/blacklab-server/gysseling',\n",
    "                     'nederlab':''}\n",
    "RECORDS_PER_PAGE = 1000\n",
    "\n",
    "# Fields parsed by default from corpus xml by _parse_xml\n",
    "# Extra fields can be given to _parse_xml by users\n",
    "DEFAULT_FIELDS_TOKEN = [\"word\", \"lemma\", \"universal_dependency\"]\n",
    "DEFAULT_FIELDS_DOC = []\n",
    "\n",
    "# Get rid of ellipsis in display (otherwise relevant data might not be shown)\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "\n",
    "\n",
    "\n",
    "# Search methods\n",
    "\n",
    "def search_corpus_allwords(corpus, pos=None):\n",
    "    '''\n",
    "    This function gets all words of a corpus. If needed, the output can be restricted to words with a given part-of-speech\n",
    "    Args:\n",
    "        corpus: corpus name\n",
    "        pos: part-of-speech (optional)\n",
    "    Returns:\n",
    "        a Pandas DataFrame containing corpus data\n",
    "        \n",
    "    >>> df_corpus = search_corpus_allwords(\"gysseling\")\n",
    "    >>> display_df(df_corpus)\n",
    "    '''\n",
    "    \n",
    "    query = r'[word=\".*\"]'\n",
    "    if pos is not None:\n",
    "        query = r'[word=\".*\" & pos=\"'+pos+r'\"]'\n",
    "    return search_corpus(query, corpus)\n",
    "\n",
    "def search_corpus_alllemmata(corpus, pos):\n",
    "    '''\n",
    "    This function gets all lemmata of a corpus. If needed, the output can be restricted to lemmata with a given part-of-speech\n",
    "    Args:\n",
    "        corpus: corpus name\n",
    "        pos: part-of-speech (optional)\n",
    "    Returns:\n",
    "        a Pandas DataFrame containing corpus data\n",
    "        \n",
    "    >>> df_corpus = search_corpus_alllemmata(\"chn\")\n",
    "    >>> display_df(df_corpus)\n",
    "    '''\n",
    "    \n",
    "    query = r'[lemma=\".*\"]'\n",
    "    if pos is not None:\n",
    "        query = r'[lemma=\".*\" & pos=\"'+pos+r'\"]'\n",
    "    return search_corpus(query, corpus) \n",
    "\n",
    "def search_corpus(query, corpus, start_position=1, detailed_context=False, extra_fields_doc=[], extra_fields_token=[]):\n",
    "    '''\n",
    "    This function searches a corpus given a query and a corpus name\n",
    "    Args:\n",
    "        query: a corpus query, eg. previously generated by corpus_query_lemma() or such\n",
    "        corpus: a corpus name\n",
    "        start_position: (optional) corpus response page (usually used by the function automatically calling itself recursively)\n",
    "        detailed_context: (optional) {True, False (default)} \n",
    "        extra_fields_doc: \n",
    "        extra_fields_token: \n",
    "    Returns:\n",
    "        a Pandas DataFrame containing corpus data\n",
    "        \n",
    "    >>> df_corpus = search_corpus(r'[pos=\"ADJ\"][word=\"huis\"]', \"chn\")\n",
    "    >>> display_df(df_corpus)\n",
    "    '''\n",
    "    \n",
    "    # show wait indicator\n",
    "    show_wait_indicator('Searching '+corpus+ ' at page '+str(start_position))    \n",
    "    \n",
    "    if corpus not in AVAILABLE_CORPORA:\n",
    "        raise ValueError(\"Unknown corpus: \" + corpus)\n",
    "    try:\n",
    "        # Do request to federated content search corpora, so we get same output format for every corpus\n",
    "        url = \"http://portal.clarin.inl.nl/fcscorpora/clariah-fcs-endpoints/sru?operation=searchRetrieve&queryType=fcs&maximumRecords=1000&x-fcs-context=\" + corpus + \"&query=\" + urllib.parse.quote(query)\n",
    "        #print(url)\n",
    "        response = requests.get(url)\n",
    "        response_text = response.text    \n",
    "        df, next_page = _parse_xml(response_text, detailed_context, extra_fields_doc, extra_fields_token)\n",
    "        # If there are next pages, call search_corpus recursively\n",
    "        #print(next_page)\n",
    "        if next_page > 0:\n",
    "            remove_wait_indicator()\n",
    "            df_more = search_corpus(query, corpus, next_page, detailed_context, extra_fields_doc, extra_fields_token)\n",
    "            df = df.append(df_more, ignore_index=True)\n",
    "            \n",
    "        remove_wait_indicator()\n",
    "        \n",
    "        # show message out of xml, if some error has occured (prevents empty output)\n",
    "        _show_error_if_any(response_text)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        remove_wait_indicator()\n",
    "        raise ValueError(\"An error occured when searching corpus \" + corpus + \": \"+ str(e))\n",
    "     \n",
    "\n",
    "    \n",
    "    \n",
    "def search_corpus_multiple(queries, corpus):\n",
    "    '''\n",
    "    This function sends multiples queries at once to the search_corpus function\n",
    "    Args:\n",
    "        queries: array of corpus queries, eg. previously generated by corpus_query_lemma() or such\n",
    "        corpus: a corpus name \n",
    "    Returns:\n",
    "        a dictionary of Pandas DataFrames, associating each query (key) to the resulting corpus data (value)\n",
    "    '''\n",
    "    result_dict = {}\n",
    "    for query in queries:\n",
    "        result_dict[query] = search_corpus(query,corpus)\n",
    "    return result_dict\n",
    "   \n",
    "    \n",
    "\n",
    "def search_lexicon_alllemmata(lexicon, pos=None):\n",
    "    '''\n",
    "    This function gets all lemmata of a lexicon. If needed, the output can be restricted to lemmata with a given part-of-speech\n",
    "    Args:\n",
    "        lexicon: a lexicon name\n",
    "        pos: part-of-speech (optional)\n",
    "    Returns:\n",
    "        a Pandas DataFrame containing lexicon data \n",
    "        \n",
    "    >>> df_corpus = search_corpus_alllemmata(\"chn\")\n",
    "    >>> display_df(df_corpus)\n",
    "    '''\n",
    "    query = lexicon_query_alllemmata(lexicon, pos)\n",
    "    return search_lexicon(query, lexicon)\n",
    "\n",
    "\n",
    "\n",
    "def search_lexicon(query, lexicon):\n",
    "    '''\n",
    "    This function searches a lexicon given a query and a lexicon name\n",
    "    Args:\n",
    "        query: a lexicon query, typically previously generated by lexicon_query() or such \n",
    "        lexicon: a lexicon name\n",
    "    Returns:\n",
    "        a Pandas DataFrame with lexicon data \n",
    "        \n",
    "    '''\n",
    "     # show wait indicator, so the user knows what's happening\n",
    "    show_wait_indicator('Searching '+lexicon)\n",
    "    \n",
    "    # default endpoint, except when diamant is invoked\n",
    "    endpoint = \"http://172.16.4.56:8890/sparql\"\n",
    "    if (lexicon==\"diamant\"):\n",
    "        endpoint = \"http://svprre02:8080/fuseki/tdb/sparql\"\n",
    "    \n",
    "    try:\n",
    "        # Accept header is needed for virtuoso, it isn't otherwise!\n",
    "        response = requests.post(endpoint, data={\"query\":query}, headers = {\"Accept\":\"application/sparql-results+json\"})\n",
    "        \n",
    "        response_json = json.loads(response.text)\n",
    "        records_json = response_json[\"results\"][\"bindings\"]\n",
    "        records_string = json.dumps(records_json)    \n",
    "        df = pd.read_json(records_string, orient=\"records\")\n",
    "    \n",
    "        # make sure cells containing NULL are added too, otherwise we'll end up with ill-formed data\n",
    "        # CAUSES MALFUNCTION: df = df.fillna('')\n",
    "        df = df.applymap(lambda x: '' if pd.isnull(x) else x[\"value\"])         \n",
    "        \n",
    "        # remove wait indicator, \n",
    "        remove_wait_indicator()\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        remove_wait_indicator()\n",
    "        raise ValueError(\"An error occured when searching lexicon \" + lexicon + \": \"+ str(e))          \n",
    "        \n",
    "\n",
    "# Processing methods\n",
    "\n",
    "def column_difference(df_column1, df_column2):\n",
    "    '''\n",
    "    This function computes differences and similarities between two Pandas DataFrames\n",
    "    Args:\n",
    "        df_column1: a Pandas DataFrame, filtered by one column\n",
    "        df_column2: a Pandas DataFrame, filtered by one column\n",
    "    Returns:\n",
    "        diff_left: array of words only in df_column1\n",
    "        diff_right: array of words only in df_column2\n",
    "        intersec: array of words both in df_column1 and df_column2\n",
    "        \n",
    "    >>> diff_left, diff_right, intersec = column_difference(df_corpus1[\"word 1\"], df_corpus2[\"word 1\"])\n",
    "    >>> display( 'These words are only in DataFrame #1 : ' + \", \".join(diff_left) )\n",
    "    >>> display( 'These words are only in DataFrame #2 : ' + \", \".join(diff_right) )\n",
    "    >>> display( 'These words are common to both DataFrame : ' + \", \".join(intersec) )\n",
    "    '''\n",
    "    \n",
    "    set_df1 = set(df_column1)\n",
    "    set_df2 = set(df_column2)\n",
    "    diff_left = set_df1.difference(set_df2)\n",
    "    diff_right = set_df2.difference(set_df1)\n",
    "    intersec = set_df1.intersection(set_df2)\n",
    "    return diff_left, diff_right, intersec\n",
    "\n",
    "def diamant_get_synonyms(df):\n",
    "    '''\n",
    "    This function gets lemmata or definitions out of a Pandas DataFrame with Diamant data. \n",
    "    The output set content depends on the result type.\n",
    "    \n",
    "    Args:\n",
    "        df: a Pandas DataFrame containing Diamant data\n",
    "    Returns:\n",
    "        a set of lemmata OR a set of synonym definitions\n",
    "        \n",
    "    >>> query = lexicon_query(word=search_word, pos= '', lexicon=lexicon)\n",
    "    >>> df_lexicon = search_lexicon(query, lexicon)\n",
    "    >>> syns = diamant_get_synonyms(df_lexicon) \n",
    "    >>> display( 'Synoniemen voor ' + search_word + ': ' + \", \".join(syns)))\n",
    "    '''\n",
    "    \n",
    "    # Depending on the result type, we return the lemma or the definition text\n",
    "    lemmas = set(df[df[\"inputMode\"]==\"defText\"][\"n_ontolex_writtenRep\"])\n",
    "    defTexts = set(df[df[\"inputMode\"]==\"lemma\"][\"n_syndef_definitionText\"])\n",
    "    return lemmas|defTexts\n",
    "\n",
    "\n",
    "def _parse_xml(text, detailed_context=False, extra_fields_doc=[], extra_fields_token=[]):\n",
    "    '''\n",
    "    This function converts the XML output of a lexicon or corpus search into a Pandas DataFrame for further processing\n",
    "    \n",
    "    Args:\n",
    "        text: the XML response of a lexicon/corpus search, as a string\n",
    "        detailed_context: (optional) True to parse the layers of all tokens, False to limit detailed parsing to hits\n",
    "        extra_fields_doc: \n",
    "        extra_fields_token: \n",
    "    Returns:\n",
    "        df: a Pandas DataFrame representing the parse results\n",
    "        next_pos: the next result page to be parsed (since the results might be spread among several XML response pages), \n",
    "        or 0 if there is no page left to be parsed\n",
    "    '''\n",
    "    \n",
    "    # TODO: should we secure against untrusted XML?\n",
    "    root = ET.fromstring(text)\n",
    "    records = []\n",
    "    n_tokens = 0\n",
    "    computed_nt = False\n",
    "    cols= []\n",
    "    \n",
    "    fields_token = DEFAULT_FIELDS_TOKEN + extra_fields_token\n",
    "    fields_doc = DEFAULT_FIELDS_DOC + extra_fields_doc\n",
    "    for entry in root.iter(\"{http://clarin.eu/fcs/resource}ResourceFragment\"):\n",
    "        doc_metadata = {}\n",
    "        for dataView in entry.findall(\"{http://clarin.eu/fcs/resource}DataView\"):\n",
    "            # Parse document metadata\n",
    "            if(dataView.get(\"type\")==\"application/x-clariah-fcs-simple-metadata+xml\"):\n",
    "                for keyval in dataView.findall(\"keyval\"):\n",
    "                    key = keyval.get(\"key\")\n",
    "                    if key in fields_doc:\n",
    "                        value = keyval.get(\"value\")\n",
    "                        doc_metadata[key] = value\n",
    "            \n",
    "            # ----- [part 1] ----- \n",
    "            # in 'hits only' mode, we'll gather the hits, otherwise we'll gather all the words of the sentences\n",
    "            \n",
    "            # We only take hits into account, ignore metadata and segmenting dataViews\n",
    "            if (detailed_context is False and dataView.get(\"type\")==\"application/x-clarin-fcs-hits+xml\"):\n",
    "                result = dataView.find(\"{http://clarin.eu/fcs/dataview/hits}Result\")\n",
    "                left_context = result.text if result.text is not None else ''\n",
    "                hits = list(result)\n",
    "                if len(hits)==0:\n",
    "                    print([w for w in result.itertext()])\n",
    "                    print(\"no hit in kwic, skip\")\n",
    "                    continue\n",
    "                last_hit = hits[-1]\n",
    "                right_context = last_hit.tail if last_hit.tail is not None else ''\n",
    "                #hit_words = [hit.text for hit in hits]\n",
    "            \n",
    "            # ----- [part 2] ----- \n",
    "            # gather info about each hit (=hits only mode) or about each word (=NOT hits only mode)\n",
    "            \n",
    "            # Get lemma of each hit\n",
    "            cols= []\n",
    "            if (dataView.get(\"type\")==\"application/x-clarin-fcs-adv+xml\"):\n",
    "                hit_layer = defaultdict(list) \n",
    "                for layer in dataView.findall(\".//{http://clarin.eu/fcs/dataview/advanced}Layer\"):\n",
    "                    layer_id = layer.get(\"id\").split(\"/\")[-1]\n",
    "                    # Only capture this layer, if it is in the list of designated fields (default+extra by user)\n",
    "                    if layer_id in fields_token:\n",
    "                        path = \".//{http://clarin.eu/fcs/dataview/advanced}Span\"\n",
    "                        if (detailed_context is False):\n",
    "                            path = path+\"[@highlight='h1']\" \n",
    "                        for one_span in layer.findall(path):\n",
    "                            span_text = one_span.text            \n",
    "                            hit_layer[layer_id].append(span_text)\n",
    "                        # Compute number of columns and create columns only once\n",
    "                        if not computed_nt:\n",
    "                            n_tokens = len(hit_layer[layer_id])\n",
    "                            computed_nt=True\n",
    "                data, cols = _combine_layers(hit_layer, n_tokens, doc_metadata_req=fields_doc, doc_metadata_recv=doc_metadata)\n",
    "                if detailed_context is False:\n",
    "                    kwic = [left_context] + data + [right_context]\n",
    "                else:\n",
    "                    kwic = data\n",
    "                records.append(kwic)  \n",
    "    if detailed_context is False:\n",
    "        columns = [\"left context\"] + cols + [\"right context\"]\n",
    "    else:\n",
    "        columns = cols\n",
    "    \n",
    "    next_pos = 0\n",
    "    next_record_position = root.find(\"{http://docs.oasis-open.org/ns/search-ws/sruResponse}nextRecordPosition\")\n",
    "    if (next_record_position is not None):\n",
    "        next_pos = int(next_record_position.text)\n",
    "    return pd.DataFrame(records, columns = columns), next_pos\n",
    "\n",
    "def _combine_layers(hit_layer, n_tokens, doc_metadata_req, doc_metadata_recv):\n",
    "    '''\n",
    "    Combine the layers, in alphabetical order of the layer names, to a flat list, with separate column per layer per word in hit, and document metadata added as last columns\n",
    "    \n",
    "    Args:\n",
    "        hit_layer: dictionary with list of items per layer\n",
    "        n_tokens: number of tokens for which token-level annotations exist.\n",
    "                    Is equal to total number of tokens in sentence if _parse_xml is called with detailed_context=True.\n",
    "                    Is equal to number of tokens in hit if _parse_xml is called with detailed_context=False.\n",
    "        doc_metadata_req: list of document metadata fields which have been requested\n",
    "        doc_metadata_recv: dictionary with document metadata that is actually present in hits:\n",
    "                        can contain less fields than doc_fields_requested\n",
    "    Returns:\n",
    "        data: flat list with combined token layers, sorted alphabetically, and document metadata\n",
    "    '''\n",
    "    # Sort layer keys to ensure same order of data in every row and column titles\n",
    "    layers_keys = sorted(hit_layer.keys())\n",
    "    # Original structure is list of tokens per layer id\n",
    "    # Arrange items first on token, then on layer_id\n",
    "    layers_token_flat = [hit_layer[layer_id][n] for n in range(n_tokens) for layer_id in layers_keys]\n",
    "    # Flatten list of document metadata fields\n",
    "    # Use all requested fields, some of which may not be available in this hit\n",
    "    doc_flat = [doc_metadata_recv[field] if field in doc_metadata_recv else \"\" for field in doc_metadata_req]\n",
    "    # Combine token and document data\n",
    "    data = layers_token_flat + doc_flat\n",
    "    \n",
    "    ### Columns\n",
    "    # Create list of columns, in same order\n",
    "    tokens_columns = [layer_id+ \" \"+str(n) for n in range(n_tokens) for layer_id in layers_keys]\n",
    "    # Add all requested document metadata fields as columns\n",
    "    columns = tokens_columns + doc_metadata_req\n",
    "    return data, columns\n",
    "\n",
    "def _show_error_if_any(text):\n",
    "    '''\n",
    "    This function reads error messages in the XML output of a lexicon or corpus search \n",
    "    and it finds any, it is printed on screen\n",
    "    \n",
    "    Args:\n",
    "        text: the XML response of a lexicon/corpus search, as a string\n",
    "    Returns:\n",
    "        N/A\n",
    "    '''\n",
    "    root = ET.fromstring(text)\n",
    "    msgs = []\n",
    "    for diagnostic in root.iter(\"{http://docs.oasis-open.org/ns/search-ws/diagnostic}diagnostic\"):\n",
    "        for msg in diagnostic.findall(\"{http://docs.oasis-open.org/ns/search-ws/diagnostic}message\"):\n",
    "            msg_text = msg.text if msg.text is not None else ''\n",
    "            msgs.append(msg_text)\n",
    "    if len(msgs) > 0:\n",
    "        print(\"; \".join(msgs))\n",
    "\n",
    "# View methods\n",
    "\n",
    "\n",
    "def view_multiple_results(results, labels):\n",
    "    '''\n",
    "    This function shows the content of multiple Pandas DataFrames out of a dictionary associating\n",
    "    labels (eg. corpus or lexicon names) to dataframes (values). It is typically called\n",
    "    after search_corpus_multiple(), since this function returns such a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        results: a dictionary of Pandas DataFrames\n",
    "        labels: list of labels corresponding to the Pandas DataFrames in results\n",
    "    Returns:\n",
    "        N/A\n",
    "        \n",
    "    >>> result_dict = search_corpus_multiple(queries, corpus)\n",
    "    >>> view_multiple_results(result_dict, labels=list(syns))\n",
    "    '''\n",
    "    assert len(labels)==len(results)\n",
    "    for n,query in enumerate(results):\n",
    "        df = results[query]\n",
    "        if not df.empty:\n",
    "            display(HTML('Resultaten voor <b>' + labels[n] + \"</b>:\"))\n",
    "            display(df)\n",
    "            \n",
    "            \n",
    "            \n",
    "def get_frequency_list(lexicon, pos, corpus):\n",
    "    '''\n",
    "    This function builds a lemmata frequency list of a corpus, \n",
    "    given a lexicon (for obvious reasons limited to some part-of-speech).\n",
    "    \n",
    "    Args:\n",
    "        lexicon: a lexicon name\n",
    "        pos: a part-of-speech to limit the search to\n",
    "        corpus: the corpus to be searched\n",
    "    Returns:\n",
    "        a Pandas DataFrame with raw frequencies ('raw_freq' column) and rankings ('rank' column)\n",
    "        \n",
    "    >>> df_frequency_list = get_frequency_list(some_lexicon, \"NOUN\", corpus_to_search)\n",
    "    >>> display(df_frequency_list)\n",
    "    '''\n",
    "    \n",
    "    print('Beware: building a frequency list can take a long time')\n",
    "    \n",
    "    # LEXICON: get a lemmata list to work with\n",
    "    df_lexicon = search_lexicon_alllemmata(lexicon, pos)\n",
    "    lexicon_lemmata_set = sorted( set([w.lower() for w in df_lexicon[\"writtenForm\"]]) )\n",
    "    lexicon_lemmata_arr= numpy.array(lexicon_lemmata_set)\n",
    "\n",
    "    # instantiate a dataframe for storing lemmata and frequencies\n",
    "    df_frequency_list = pd.DataFrame(index=lexicon_lemmata_arr, columns=['raw_freq'])\n",
    "    df_frequency_list.index.name = 'lemmata'\n",
    "\n",
    "    # CORPUS: loop through lemmata list, query the corpus with that lemma, and count the results\n",
    "\n",
    "    # It's a good idea to work with more than one lemma at once!\n",
    "    nr_of_lemmata_to_query_atonce = 100\n",
    "    \n",
    "    # loop over lemmata list \n",
    "    for i in range(0, len(lexicon_lemmata_set), nr_of_lemmata_to_query_atonce):\n",
    "        # slice to small sets of lemmata to query at once\n",
    "        small_lemmata_set = set( lexicon_lemmata_arr[i : i+nr_of_lemmata_to_query_atonce] )    \n",
    "\n",
    "        # join set of lemmata to send them in a query all at once\n",
    "        # beware: single quotes need escaping\n",
    "        lemmata_list = \"|\".join(small_lemmata_set).replace(\"'\", \"\\\\\\\\'\")\n",
    "        df_corpus = search_corpus(r'[lemma=\"' + lemmata_list + r'\"]', corpus)\n",
    "\n",
    "        # store frequencies\n",
    "        if (len(df_corpus)>0):\n",
    "            for one_lemma in small_lemmata_set: \n",
    "                raw_freq = len(df_corpus[df_corpus['lemma 0'] == one_lemma])\n",
    "                df_frequency_list.at[one_lemma, 'raw_freq'] = raw_freq \n",
    "                \n",
    "    # final step: compute rank\n",
    "    # this is needed to be able to compare different frequency lists \n",
    "    # with each other (which we could achieve by computing a rank diff)\n",
    "    df_frequency_list['rank'] = df_frequency_list['raw_freq'].rank(ascending = False).astype(int)\n",
    "    \n",
    "    return df_frequency_list;\n",
    "\n",
    "\n",
    "def get_missing_wordforms(lexicon, pos, corpus):    \n",
    "    '''\n",
    "    This function gathers all paradigms of a lexicon with a given part-of-speech\n",
    "    and searches an annotated corpus for words missing in those paradigms\n",
    "    \n",
    "    Args:\n",
    "        lexicon: a lexicon name\n",
    "        pos: a part-of-speech to limit the search to\n",
    "        corpus: the corpus to be searched\n",
    "    Returns:\n",
    "        a Pandas DataFrame associating lemmata to their paradigms ('known_wordforms' column) and\n",
    "        missing wordforms found in the corpus ('unknown_wordforms' column).\n",
    "        \n",
    "    >>> df = get_missing_wordforms(\"molex\", \"VERB\", \"opensonar\")\n",
    "    >>> df.to_csv( \"missing_wordforms.csv\", index=False)\n",
    "    '''\n",
    "    \n",
    "    print('Beware: finding missing wordforms in a lexicon can take a long time');\n",
    "    \n",
    "    # LEXICON: get a lemmata list to work with\n",
    "    df_lexicon = search_lexicon_alllemmata(lexicon, pos)\n",
    "    lexicon_lemmata_set = sorted( set([w.lower() for w in df_lexicon[\"writtenForm\"]]) )\n",
    "    lexicon_lemmata_arr= numpy.array(lexicon_lemmata_set)\n",
    "    \n",
    "    # instantiate a dataframe for storing lemmata and wordforms\n",
    "    df_enriched_lexicon = pd.DataFrame(index=lexicon_lemmata_arr, columns=['lemma', 'pos', 'known_wordforms', 'unknown_wordforms'])\n",
    "    df_enriched_lexicon.index.name = 'lemmata'\n",
    "    \n",
    "    # CORPUS: loop through lemmata list, query the corpus with that lemma, \n",
    "    # and compute difference between both\n",
    "\n",
    "    # It's a good idea to work with more than one lemma at once!\n",
    "    nr_of_lemmata_to_query_atonce = 100\n",
    "    \n",
    "    # loop over lemmata list \n",
    "    for i in range(0, len(lexicon_lemmata_set), nr_of_lemmata_to_query_atonce):\n",
    "        # slice to small sets of lemmata to query at once\n",
    "        small_lemmata_set = set( lexicon_lemmata_arr[i : i+nr_of_lemmata_to_query_atonce] )    \n",
    "        \n",
    "        # join set of lemmata to send them in a query all at once\n",
    "        # beware: single quotes need escaping\n",
    "        lemmata_list = \"|\".join(small_lemmata_set).replace(\"'\", \"\\\\\\\\'\")\n",
    "        df_corpus = search_corpus(r'[lemma=\"' + lemmata_list + r'\"]', corpus)\n",
    "        \n",
    "        # process results\n",
    "        if (len(df_corpus)>0):\n",
    "            for one_lemma in small_lemmata_set: \n",
    "                \n",
    "                # look up the known wordforms in the lexicon\n",
    "                query = lexicon_query(one_lemma, pos, lexicon)\n",
    "                df_known_wordforms = search_lexicon(query, lexicon)\n",
    "                \n",
    "                if (len(df_known_wordforms) != 0):\n",
    "                    known_wordforms = set( df_known_wordforms['wordform'].str.lower() )\n",
    "                    # find the wordforms in the corpus\n",
    "                    corpus_wordforms = set( (df_corpus[df_corpus['lemma 0'] == one_lemma])['word 0'].str.lower() )\n",
    "                    # determine which corpus wordforms are not in lexicon wordforms\n",
    "                    unknown_wordforms = corpus_wordforms.difference(known_wordforms)\n",
    "\n",
    "                    if (len(unknown_wordforms) !=0):\n",
    "                        # store the results\n",
    "                        df_enriched_lexicon.at[one_lemma, 'lemma'] = one_lemma\n",
    "                        df_enriched_lexicon.at[one_lemma, 'pos'] = pos\n",
    "                        df_enriched_lexicon.at[one_lemma, 'known_wordforms'] = known_wordforms\n",
    "                        df_enriched_lexicon.at[one_lemma, 'unknown_wordforms'] = unknown_wordforms\n",
    "                \n",
    "    # return non-empty results, t.i. cases in which we found some wordforms\n",
    "    return df_enriched_lexicon[ df_enriched_lexicon['unknown_wordforms'].notnull() ]\n",
    "        \n",
    "    \n",
    "def get_rank_diff(df1, df2):\n",
    "    '''\n",
    "    This function compares the rankings of words common to two dataframes, and compute a rank_diff, in such\n",
    "    a way that one can see which words are very frequent in one set and rare in the other.\n",
    "    \n",
    "    Args:\n",
    "        df1: a Pandas DataFrame\n",
    "        df2: a Pandas DataFrame\n",
    "    Returns:\n",
    "        a Pandas DataFrame with lemmata (index), ranks of both input dataframes ('rank_1' and 'rank_2' columns) \n",
    "        and the rank_diff ('rank_diff' column).\n",
    "        \n",
    "    >>> df_frequency_list1 = get_frequency_list(base_lexicon, \"NOUN\", corpus_to_search1)\n",
    "    >>> df_frequency_list2 = get_frequency_list(base_lexicon, \"NOUN\", corpus_to_search2)\n",
    "    >>> df_rankdiffs = get_rank_diff(df_frequency_list1, df_frequency_list2)\n",
    "    '''\n",
    "    \n",
    "    # Find lemmata shared by both dataframes: computing ranks diffs is only possible\n",
    "    # when dealing with lemmata which are in both frames\n",
    "    lemmata_list1 = set(df1.index.tolist())\n",
    "    lemmata_list2 = set(df2.index.tolist())\n",
    "    common_lemmata_list = list( lemmata_list1.intersection(lemmata_list2) )\n",
    "    \n",
    "    # Build dataframes limited to the common lemmata\n",
    "    limited_df1 = df1.loc[ common_lemmata_list , : ]\n",
    "    limited_df2 = df2.loc[ common_lemmata_list , : ]\n",
    "    \n",
    "    # Recompute ranks in both dataframes, because in each frame the original ranks were\n",
    "    # computed with a lemmata list which might be larger than the lemmata list common\n",
    "    # to both dataframes\n",
    "    \n",
    "    limited_df1['rank'] = limited_df1['raw_freq'].rank(ascending = False).astype(int)\n",
    "    limited_df2['rank'] = limited_df2['raw_freq'].rank(ascending = False).astype(int)\n",
    "    \n",
    "    # Instantiate a dataframe for storing lemmata and rank diffs\n",
    "    df_rankdiffs = pd.DataFrame(index=common_lemmata_list, columns=['rank_1', 'rank_2', 'rank_diff'])\n",
    "    df_rankdiffs.index.name = 'lemmata'\n",
    "    \n",
    "    df_rankdiffs['rank_1'] = limited_df1['rank']\n",
    "    df_rankdiffs['rank_2'] = limited_df2['rank']\n",
    "    df_rankdiffs['rank_diff'] = pd.DataFrame.abs( df_rankdiffs['rank_1'] - df_rankdiffs['rank_2'] )\n",
    "    \n",
    "    return df_rankdiffs\n",
    "\n",
    "\n",
    "# TODO: Method misses token fields which are extracted from POS tag by FCS (eg. inflection)\n",
    "def _parse_blacklab_metadata(text):\n",
    "    '''\n",
    "    This method parses metadata fields from a Blacklab metadata response\n",
    "    Args:\n",
    "        text: the XML response of a lexicon/corpus search, as a string\n",
    "    Returns:\n",
    "        A dictionary of with lists of document and token metadata\n",
    "    '''\n",
    "    \n",
    "    # TODO: should we secure against untrusted XML?\n",
    "    root = ET.fromstring(text)\n",
    "    doc_fields = [md.get(\"name\") for md in root.iter(\"metadataField\")]\n",
    "    token_fields = [prop.get(\"name\") for prop in root.iter(\"property\")]\n",
    "    return {\"document\": doc_fields, \"token\": token_fields}\n",
    "    \n",
    "\n",
    "def _corpus_metadata_blacklab(corpus_name):\n",
    "    '''\n",
    "    Return all possible metadata fields for a BlackLab-based corpus, by sending a request to the corpus\n",
    "    \n",
    "    Args:\n",
    "        corpus_name: Name of the corpus\n",
    "    Returns:\n",
    "        A dictionary of with lists of document and token metadata\n",
    "    '''\n",
    "    corpus_url = AVAILABLE_CORPORA[corpus_name]\n",
    "    response = requests.get(corpus_url)\n",
    "    response_text = response.text  \n",
    "    return _parse_blacklab_metadata(response_text)\n",
    "\n",
    "def get_available_metadata(resource_type, resource_name):\n",
    "    '''\n",
    "    Return all possible metadata fields for a lexicon or corpus\n",
    "    \n",
    "    Args:\n",
    "        resource_type: One of 'lexicon' or 'corpus'\n",
    "        resource_name: Name of the lexicon or corpus\n",
    "    Returns:\n",
    "        A list of metadata fields\n",
    "    '''\n",
    "    if resource_type==\"lexicon\":\n",
    "        # Create sample query for this lexicon\n",
    "        q = lexicon_query(word=\"\", pos=\"\", lexicon=resource_name)\n",
    "        return _etadata_from_lexicon_query(q)\n",
    "    elif resource_type==\"corpus\":\n",
    "        if resource_name in AVAILABLE_CORPORA and resource_name != \"nederlab\":\n",
    "            return _corpus_metadata_blacklab(resource_name)\n",
    "        elif resource_name==\"nederlab\":\n",
    "            print(\"Corpus metadata not yet available for Nederlab\")\n",
    "            return []\n",
    "        else:\n",
    "            ValueError(\"Unknown corpus: \" + resource_name + \". Should be one of \" + AVAILABLE_CORPORA.keys())\n",
    "    else:\n",
    "        raise ValueError(\"resource_type should be 'corpus' or 'lexicon'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library functions: UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:42:17.743540Z",
     "start_time": "2019-02-08T15:42:17.518243Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "#import tkinter as tk\n",
    "#from tkinter import filedialog\n",
    "from pathlib import Path\n",
    "from IPython.display import Javascript\n",
    "from IPython.core.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEFAULT_QUERY = r'[lemma=\"boek\" & pos=\"verb\"]' #r'[lemma=\"boeken\" pos=\"verb\"]'\n",
    "DEFAULT_CORPUS = \"chn\"\n",
    "\n",
    "\n",
    "def show_wait_indicator(message=None):\n",
    "    \n",
    "    print('...' + (message if message else 'Busy now') + '...', end=\"\\r\") \n",
    "    sys.stdout.write(\"\\033[F\")\n",
    "\n",
    "def remove_wait_indicator():    \n",
    "    print('                                                                    ', end=\"\\r\")\n",
    "    sys.stdout.write(\"\\033[F\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def create_corpus_ui():\n",
    "    '''\n",
    "    This function builds a GUI for corpus search\n",
    "    \n",
    "    Args:\n",
    "        N/A\n",
    "    Returns:\n",
    "        N/A\n",
    "    '''\n",
    "    \n",
    "    # Create UI elements\n",
    "    corpusQueryField = widgets.Text(description=\"<b>CQL query:</b>\", value=DEFAULT_QUERY)\n",
    "    corpusField = widgets.Dropdown(\n",
    "        options=AVAILABLE_CORPORA.keys(),\n",
    "        value=DEFAULT_CORPUS,\n",
    "        description='<b>Corpus:</b>',\n",
    "    )\n",
    "    '''corpusSearchButton = widgets.Button(\n",
    "        description='Search',\n",
    "        button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Search',\n",
    "    )\n",
    "    # Handle events\n",
    "    corpusSearchButton.on_click(corpus_search)'''\n",
    "    \n",
    "    # Stack UI elements in vertical box and display\n",
    "    corpusUiBox = widgets.VBox([corpusQueryField,corpusField])\n",
    "    display(corpusUiBox)\n",
    "    \n",
    "    # Return fields, so their contents are accessible from the global namespace of the Notebook\n",
    "    return corpusQueryField, corpusField\n",
    "\n",
    "def create_lexicon_ui():\n",
    "    '''\n",
    "    This function builds a GUI for lexicon search.\n",
    "    \n",
    "    Args:\n",
    "        N/A\n",
    "    Returns:\n",
    "        N/A\n",
    "    '''\n",
    "    \n",
    "    DEFAULT_SEARCHWORD = 'boek'\n",
    "    DEFAULT_LEXICON = \"diamant\"\n",
    "\n",
    "    # Create UI elements\n",
    "    searchWordField = widgets.Text(description=\"<b>Word:</b>\", value=DEFAULT_SEARCHWORD)\n",
    "    lexiconField = widgets.Dropdown(\n",
    "        options=['anw', 'celex', 'diamant', 'duelme', 'molex'],\n",
    "        value=DEFAULT_LEXICON,\n",
    "        description='<b>Lexicon:</b>',\n",
    "    )\n",
    "    '''lexSearchButton = widgets.Button(\n",
    "        description='Search',\n",
    "        button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Search',\n",
    "    )\n",
    "    # Handle events\n",
    "    lexSearchButton.on_click(lexicon_search)'''\n",
    "    # Stack UI elements in vertical box and display\n",
    "    lexUiBox = widgets.VBox([searchWordField,lexiconField])\n",
    "    display(lexUiBox)\n",
    "    return searchWordField, lexiconField\n",
    "\n",
    "\n",
    "def create_save_dataframe_ui(df):\n",
    "    '''\n",
    "    This function builds a GUI for saving the results of some lexicon or corpus query to a .csv file.\n",
    "    One can use load_dataframe(filepath) to reload the results later on.\n",
    "    \n",
    "    Args:\n",
    "        df: a Pandas DataFrame\n",
    "    Returns:\n",
    "        N/A\n",
    "    '''\n",
    "    \n",
    "    # build ui for saving results\n",
    "    DEFAULT_FILENAME = 'mijn_resultaten.csv'\n",
    "    saveResultsCaption = widgets.Label(value='Sla uw resultaten op:')\n",
    "    fileNameField = widgets.Text(value=DEFAULT_FILENAME)\n",
    "    savebutton = widgets.Button(\n",
    "        description='Bestand opslaan',\n",
    "        disabled=False,\n",
    "        button_style='warning', \n",
    "        tooltip=DEFAULT_FILENAME,  # trick to pass filename to button widget\n",
    "        icon=''\n",
    "    )\n",
    "    # inject dataframe into button object\n",
    "    savebutton.df = df\n",
    "    # when the user types a new filename, it will be passed to the button tooltip property straight away\n",
    "    fileNameLink = widgets.jslink((fileNameField, 'value'), (savebutton, 'tooltip'))\n",
    "    # click event with callback\n",
    "    savebutton.on_click( _save_dataframe )    \n",
    "    saveResultsBox = widgets.HBox([saveResultsCaption, fileNameField, savebutton])\n",
    "    display(saveResultsBox)\n",
    "    \n",
    "def _save_dataframe(button):\n",
    "    fileName = button.tooltip\n",
    "    # The result files can be saved locally or on the server:\n",
    "    # If result files are to be offered as downloads, set to True; otherwise set to False    \n",
    "    fileDownloadable = False\n",
    "    # specify paths here, if needed:\n",
    "    filePath_onServer = ''  # could be /path/to\n",
    "    filePath_default = ''\n",
    "    # compute full path given chosen mode\n",
    "    fullFileName = (filePath_onServer if fileDownloadable else filePath_default ) + fileName\n",
    "        \n",
    "    try:\n",
    "        button.df.to_csv( fullFileName, index=False)\n",
    "        # confirm it all went well\n",
    "        print(fileName + \" saved\")    \n",
    "        button.button_style = 'success'\n",
    "        button.icon = 'check'\n",
    "        # trick: https://stackoverflow.com/questions/31893930/download-csv-from-an-ipython-notebook\n",
    "        if (fileDownloadable):\n",
    "            downloadableFiles = FileLinks(filePath_onServer)\n",
    "            display(downloadableFiles)\n",
    "    except Exception as e:\n",
    "        button.button_style = 'danger'\n",
    "        raise ValueError(\"An error occured when saving \" + fileName + \": \"+ str(e))    \n",
    "\n",
    "    \n",
    "    \n",
    "def load_dataframe(filepath):\n",
    "    '''\n",
    "    This functions (re)loads some previously saved Pandas DataFrame\n",
    "    \n",
    "    Args:\n",
    "        filepath: path to the saved Pandas DataFrame (.csv)\n",
    "    Returns: \n",
    "        a Pandas DataFrame representing the content of the file\n",
    "    \n",
    "    >>> df_corpus = load_dataframe('mijn_resultaten.csv')\n",
    "    >>> display_df(df_corpus, title=\"Results:\")\n",
    "    '''\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(filepath + \" loaded successfully\")            \n",
    "    except Exception as e:\n",
    "        raise ValueError(\"An error occured when loading \" + filepath + \": \"+ str(e))\n",
    "    finally:\n",
    "        return df\n",
    "\n",
    "\n",
    "def display_df(df, columns=None, title=None, mode='table'):\n",
    "    '''\n",
    "    This function displays a Pandas DataFrame as a table of as a chart.\n",
    "    \n",
    "    If the 'chart' mode is chosen, the function draws a horizontal chart representing a dataframe.\n",
    "    One axis is the index of the dataframe, and the other axis is the given column, which holds the \n",
    "    values to plot in the chart.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to be displayed\n",
    "        columns: columns to display, or None to display all columns\n",
    "        title: Title displayed\n",
    "        mode: Way of displaying, one of 'table' (default) or 'chart'\n",
    "    Returns:\n",
    "        N/A\n",
    "    '''\n",
    "    if columns is not None:\n",
    "        df_display = df[columns]\n",
    "    else:\n",
    "        df_display = df\n",
    "    \n",
    "    # chart mode\n",
    "    if mode == 'chart':\n",
    "        plt.figure()\n",
    "        df_display.plot.barh().set_title(title)\n",
    "    \n",
    "    # table mode (default)\n",
    "    else:    \n",
    "        if title is not None:\n",
    "            display(HTML(\"<b>%s</b>\" % title))        \n",
    "\n",
    "        display(df_display)\n",
    "    \n",
    "    # eventually, give UI to save data\n",
    "    create_save_dataframe_ui(df_display)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library functions: Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T15:42:18.320576Z",
     "start_time": "2019-02-08T15:42:18.274160Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def containsRegex(word):\n",
    "    '''\n",
    "    This function checks whether some string contains a regular expression or not\n",
    "    \n",
    "    Args:\n",
    "        word: a string to check for regular expressions\n",
    "    Returns:\n",
    "        A boolean\n",
    "    '''\n",
    "    return ( word.find('^')>-1 or\n",
    "            word.find('$')>-1 or \n",
    "            re.match(\"\\(.+?\\)\", word) or\n",
    "            re.match(\"\\[.+?\\]\", word) or\n",
    "            re.match(\"[\\+*]\", word) )\n",
    "                     \n",
    "def lexicon_query(word, pos, lexicon):\n",
    "    '''\n",
    "    This function builds a query for getting the paradigm etc. of a given lemma out of a given lexicon.\n",
    "    The resulting query string is to be used as a parameter of search_lexicon() \n",
    "    \n",
    "    Args:\n",
    "        word: a lemma/wordform to build the query with\n",
    "        pos: a part-of-speech to build the query with\n",
    "        lexicon: a lexicon to build the query for\n",
    "    Returns:\n",
    "        a query string to be used as a parameter of search_lexicon() \n",
    "    '''\n",
    "    \n",
    "    if (lexicon==\"anw\"):\n",
    "        # part-of-speech filter not supported for this lexicon\n",
    "        if (pos is not None and pos != ''):\n",
    "            print('Filtering by part-of-speech is not (yet) supported in the \\''+lexicon+'\\' lexicon')\n",
    "        # exact or fuzzy search\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart = \"\"\"FILTER ( regex(?lemma, \\\"\"\"\"+word+\"\"\"\\\") || regex(?definition, \\\"\"\"\"+word+\"\"\"\\\") ) . \"\"\"\n",
    "        if (exactsearch == True):\n",
    "              subpart =  \"\"\"\n",
    "                { { ?lemId rdfs:label ?lemma .  \n",
    "                values ?lemma { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } }                 \n",
    "                UNION\n",
    "                { ?definitionId lemon:value ?definition .\n",
    "                values ?definition { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } } .\n",
    "                \"\"\"               \n",
    "        query = \"\"\"PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "                  PREFIX anw: <http://rdf.ivdnt.org/lexica/anw>\n",
    "                  PREFIX anwsch: <http://rdf.ivdnt.org/schema/anw/>\n",
    "                  PREFIX lemon: <http://lemon-model.net/lemon#>\n",
    "                  \n",
    "                  SELECT ?lemId ?lemma ?writtenForm ?definition concat('', ?definitionComplement) as ?definitionComplement\n",
    "                  FROM <http://rdf.ivdnt.org/lexica/anw>\n",
    "                  WHERE {\n",
    "                      ?lemId rdfs:label ?lemma .\n",
    "                      ?lemId ontolex:sense ?senseId .\n",
    "                      ?senseId lemon:definition ?definitionId .\n",
    "                      ?definitionId lemon:value ?definition .\n",
    "                      OPTIONAL { ?definitionId anwsch:definitionComplement ?definitionComplement .}\n",
    "                      OPTIONAL { ?lemId ontolex:canonicalForm ?lemCFId . \n",
    "                          ?lemCFId ontolex:writtenRepresentation ?writtenForm . }\n",
    "                      \"\"\"+subpart+\"\"\"\n",
    "                      }\"\"\"\n",
    "    elif (lexicon==\"diamant\"):\n",
    "        # part-of-speech filter not supported for this lexicon\n",
    "        if (pos is not None and pos != ''):\n",
    "            print('Filtering by part-of-speech is not (yet) supported in the \\''+lexicon+'\\' lexicon')\n",
    "        # exact or fuzzy search\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart1 = \"\"\"?n_form ontolex:writtenRep ?n_ontolex_writtenRep . \n",
    "            FILTER regex(?n_ontolex_writtenRep, \\\"\"\"\"+word+\"\"\"\\\") . \"\"\"\n",
    "        subpart2 = \"\"\"?n_syndef diamant:definitionText ?n_syndef_definitionText .  \n",
    "            FILTER regex(?n_ontolex_writtenRep, \\\"\"\"\"+word+\"\"\"\\\") . \"\"\"\n",
    "        if (exactsearch == True):\n",
    "            subpart1 =  \"\"\"\n",
    "                { ?n_form ontolex:writtenRep ?n_ontolex_writtenRep . \n",
    "                values ?n_ontolex_writtenRep { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } \n",
    "                \"\"\"                \n",
    "            subpart2 = \"\"\"\n",
    "                { ?n_syndef diamant:definitionText ?n_syndef_definitionText . \n",
    "                values ?n_syndef_definitionText { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } \n",
    "                \"\"\"\n",
    "        query = \"\"\"\n",
    "        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "        prefix prov: <http://www.w3.org/ns/prov#>\n",
    "        prefix diamant: <http://rdf.ivdnt.org/schema/diamant#>\n",
    "        prefix lexinfo: <http://www.lexinfo.net/ontology/2.0/lexinfo#>\n",
    "        prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "        prefix lemon: <http://lemon-model.net/lemon#>\n",
    "        prefix ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "        prefix ud: <http://universaldependencies.org/u/pos/>\n",
    "        prefix skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "        prefix dcterms: <http://purl.org/dc/terms/>\n",
    "        prefix dc: <http://purl.org/dc/terms/>\n",
    "\n",
    "        select ?n_entry ?n_form ?n_ontolex_writtenRep ?n_syndef ?n_sensedef ?n_sensedef_definitionText ?n_syndef_definitionText ?n_sense ?inputMode ?wy_f_show ?wy_t_show\n",
    "        where\n",
    "        {\n",
    "        graph ?g\n",
    "        {\n",
    "        {\n",
    "            \"\"\" + subpart1 + \"\"\"\n",
    "            { ?n_entry a ontolex:LexicalEntry} .\n",
    "            { ?n_form a ontolex:Form} .\n",
    "            { ?n_sense a ontolex:LexicalSense} .\n",
    "            { ?n_syndef a diamant:SynonymDefinition} .\n",
    "            { ?n_sensedef a lemon:SenseDefinition} .\n",
    "            { ?n_syndef diamant:definitionText ?n_syndef_definitionText } .\n",
    "            { ?n_sensedef diamant:definitionText ?n_sensedef_definitionText } .\n",
    "            { ?n_entry ontolex:canonicalForm ?n_form } .\n",
    "            { ?n_entry ontolex:sense ?n_sense } .\n",
    "            { ?n_sense lemon:definition ?n_syndef } .\n",
    "            { ?n_sense lemon:definition ?n_sensedef } .\n",
    "              ?n_sense diamant:attestation ?n_attest_show .\n",
    "              ?n_sense diamant:attestation ?n_attest_filter .\n",
    "              ?n_attest_show diamant:text ?n_q_show .\n",
    "              ?n_attest_filter diamant:text ?n_q_filter .\n",
    "              ?n_attest_show a diamant:Attestation .\n",
    "              ?n_attest_filter a diamant:Attestation .\n",
    "              ?n_q_filter a diamant:Quotation .\n",
    "              ?n_q_show a diamant:Quotation .\n",
    "              ?n_q_filter diamant:witnessYearFrom ?wy_f_filter .\n",
    "              ?n_q_filter diamant:witnessYearTo ?wy_t_filter .\n",
    "              ?n_q_show diamant:witnessYearFrom ?wy_f_show .\n",
    "              ?n_q_show diamant:witnessYearTo ?wy_t_show .\n",
    "              FILTER (xsd:integer(?wy_f_show) >= 1200)\n",
    "              FILTER (xsd:integer(?wy_t_show) >= 1200)\n",
    "              FILTER (xsd:integer(?wy_f_show) <= 2018)\n",
    "              FILTER (xsd:integer(?wy_t_show) <= 2018)\n",
    "            { bind(\"lemma\" as ?inputMode) } .\n",
    "            } UNION\n",
    "          {\n",
    "            \"\"\" + subpart2 + \"\"\"\n",
    "            { ?n_sense a ontolex:LexicalSense} .\n",
    "            { ?n_syndef a diamant:SynonymDefinition} .\n",
    "            { ?n_sensedef a lemon:SenseDefinition} .\n",
    "            { ?n_form a ontolex:Form} .\n",
    "            { ?n_form ontolex:writtenRep ?n_ontolex_writtenRep } .  { ?n_entry a ontolex:LexicalEntry} .\n",
    "            { ?n_entry ontolex:sense ?n_sense } .\n",
    "            { ?n_sense lemon:definition ?n_syndef } .\n",
    "            { ?n_sense lemon:definition ?n_sensedef } .\n",
    "            { ?n_sensedef diamant:definitionText ?n_sensedef_definitionText } .\n",
    "            { ?n_entry ontolex:canonicalForm ?n_form } .\n",
    "            ?n_sense diamant:attestation ?n_attest_show .\n",
    "            ?n_sense diamant:attestation ?n_attest_filter .\n",
    "            ?n_attest_filter diamant:text ?n_q_filter .\n",
    "            ?n_attest_show diamant:text ?n_q_show .\n",
    "            ?n_q_filter diamant:witnessYearFrom ?wy_f_filter .\n",
    "            ?n_q_filter diamant:witnessYearTo ?wy_t_filter .\n",
    "            ?n_q_show diamant:witnessYearFrom ?wy_f_show .\n",
    "            ?n_q_show diamant:witnessYearTo ?wy_t_show .\n",
    "            ?n_attest_show a diamant:Attestation .\n",
    "            ?n_attest_filter a diamant:Attestation .\n",
    "            ?n_q_filter a diamant:Quotation .\n",
    "            ?n_q_show a diamant:Quotation .\n",
    "            FILTER (xsd:integer(?wy_f_show) >= 1200)\n",
    "            FILTER (xsd:integer(?wy_t_show) >= 1200)\n",
    "            FILTER (xsd:integer(?wy_f_show) <= 2018)\n",
    "            FILTER (xsd:integer(?wy_t_show) <= 2018)\n",
    "          { bind(\"defText\" as ?inputMode) } .\n",
    "            }\n",
    "        }\n",
    "        }\"\"\"\n",
    "    elif (lexicon==\"molex\"):\n",
    "        # exact or fuzzy search\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart1 = \"\"\"\"\"\"\n",
    "        subpart2 = \"\"\"\"\"\"\n",
    "        subpartPos = \"\"\"\"\"\"\n",
    "        if (word != ''):\n",
    "            if (exactsearch == True):\n",
    "                subpart1 =  \"\"\"\n",
    "                    { ?lemCFId ontolex:writtenRep ?lemma . \n",
    "                    values ?lemma { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } \n",
    "                    UNION\n",
    "                    { ?wordformId ontolex:writtenRep ?wordform . \n",
    "                    values ?wordform { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } } .\n",
    "                    \"\"\"        \n",
    "            else:\n",
    "                subpart2 = \"\"\"FILTER ( regex(?lemma, \\\"\"\"\"+word+\"\"\"\\\") || regex(?wordform, \\\"\"\"\"+word+\"\"\"\\\") ) . \"\"\"\n",
    "        if (pos is not None and pos != ''):\n",
    "            subpartPos = \"\"\"FILTER ( regex(?lemPos, \\\"\"\"\"+pos+\"\"\"$\\\") ) .\"\"\"\n",
    "        query = \"\"\"\n",
    "            PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "            PREFIX UD: <http://universaldependencies.org/u/>\n",
    "            PREFIX diamant: <http://rdf.ivdnt.org/schema/diamant#>\n",
    "            \n",
    "            SELECT ?lemEntryId ?lemma ?lemPos ?wordformId ?wordform ?hyphenation ?wordformPos ?Gender ?Number\n",
    "            FROM <http://rdf.ivdnt.org/lexica/molex>\n",
    "            WHERE\n",
    "            {\n",
    "            ?lemEntryId ontolex:canonicalForm ?lemCFId .\n",
    "            ?lemCFId ontolex:writtenRep ?lemma .\n",
    "            \"\"\"+subpart1+\"\"\"\n",
    "            OPTIONAL {?lemEntryId UD:Gender ?Gender .}\n",
    "            OPTIONAL {?lemEntryId UD:VerbForm ?verbform .}\n",
    "            ?lemEntryId UD:pos ?lemPos .\n",
    "            \"\"\"+subpartPos+\"\"\"\n",
    "            ?lemEntryId ontolex:lexicalForm ?wordformId .\n",
    "            ?wordformId UD:pos ?wordformPos .\n",
    "            OPTIONAL {?wordformId UD:Number ?Number .}\n",
    "            OPTIONAL {?wordformId ontolex:writtenRep ?wordform .}\n",
    "            OPTIONAL {?wordformId diamant:hyphenation ?hyphenation .}\n",
    "            \"\"\"+subpart2+\"\"\"\n",
    "            }\n",
    "        \"\"\"\n",
    "    elif (lexicon==\"duelme\"):\n",
    "        # part-of-speech filter not supported for this lexicon\n",
    "        if (pos is not None and pos != ''):\n",
    "            print('Filtering by part-of-speech is not (yet) supported in the \\''+lexicon+'\\' lexicon')\n",
    "        # exact or fuzzy search\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart = \"\"\"FILTER ( regex(?lemma, \\\"\"\"\"+word+\"\"\"\\\") || regex(?wordform, \\\"\"\"\"+word+\"\"\"\\\") ) .\"\"\"\n",
    "        if (exactsearch == True):\n",
    "            subpart =  \"\"\"\n",
    "                { ?y lmf:hasLemma ?dl .  \n",
    "                values ?dl { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } }                 \n",
    "                \"\"\"        \n",
    "        query = \"\"\"\n",
    "            PREFIX duelme: <http://rdf.ivdnt.org/lexica/duelme>\n",
    "            PREFIX intskos: <http://ivdnt.org/schema/lexica#>\n",
    "            PREFIX lmf: <http://www.lexinfo.net/lmf>\n",
    "            PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "            PREFIX UD: <http://rdf.ivdnt.org/vocabs/UniversalDependencies2#>\n",
    "            \n",
    "            SELECT ?exampleSentence ?lemma ?gender ?number\n",
    "            WHERE  {\n",
    "                  ?d intskos:ExampleSentence ?exampleSentence .\n",
    "                  ?d lmf:ListOfComponents [lmf:Component ?y] .\n",
    "                  ?y lmf:hasLemma ?lemma . \n",
    "                  OPTIONAL {?y UD:Gender ?gender}\n",
    "                  OPTIONAL {?y UD:Number ?number}\n",
    "            \"\"\"+subpart+\"\"\"\n",
    "            }\n",
    "        \"\"\"\n",
    "    elif (lexicon==\"celex\"):\n",
    "        # part-of-speech filter not supported for this lexicon\n",
    "        if (pos is not None and pos != ''):\n",
    "            print('Filtering by part-of-speech is not (yet) supported in the \\''+lexicon+'\\' lexicon')\n",
    "        # exact or fuzzy search\n",
    "        exactsearch = (not containsRegex(word))\n",
    "        subpart = \"\"\"FILTER ( regex(?lemma, \\\"\"\"\"+word+\"\"\"\\\") ) . \"\"\"\n",
    "        if (exactsearch == True):\n",
    "            subpart =  \"\"\"\n",
    "                { ?lemmaId ontolex:canonicalForm [ontolex:writtenRep ?lemma] .  \n",
    "                values ?lemma { \\\"\"\"\"+word+\"\"\"\\\"@nl \\\"\"\"\"+word+\"\"\"\\\" } }                 \n",
    "                \"\"\"        \n",
    "        query = \"\"\"\n",
    "            PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "            PREFIX celex: <http://rdf.ivdnt.org/lexica/celex>\n",
    "            PREFIX UD: <http://rdf.ivdnt.org/vocabs/UniversalDependencies2#>\n",
    "            PREFIX decomp: <http://www.w3.org/ns/lemon/decomp#>\n",
    "            PREFIX gold: <http://purl.org/linguistics/gold#>\n",
    "            \n",
    "            SELECT DISTINCT ?lemmaId ?lemma ?wordformId ?wordform ?number ?gender concat('', ?subLemmata) AS ?subLemmata\n",
    "            WHERE  {\n",
    "                ?lemmaId ontolex:canonicalForm [ontolex:writtenRep ?lemma] .\n",
    "                \"\"\"+subpart+\"\"\"\n",
    "                BIND( ?lemmaId AS ?lemmaIdIRI ).\n",
    "                ?lemmaId ontolex:lexicalForm ?wordformId .\n",
    "                ?wordformId ontolex:writtenRep ?wordform .\n",
    "                OPTIONAL {?wordformId UD:Number ?number} .\n",
    "                OPTIONAL {\n",
    "                    ?lemmaId UD:Gender ?g . \n",
    "                        bind( \n",
    "                            if(?g = UD:Fem_Gender, \n",
    "                            UD:Com_Gender, \n",
    "                                if(?g = UD:Masc_Gender,\n",
    "                                    UD:Com_Gender,\n",
    "                                    UD:Neut_Gender\n",
    "                                )\n",
    "                            )\n",
    "                            AS ?gender\n",
    "                        )\n",
    "                }\n",
    "                OPTIONAL {\n",
    "                    SELECT ?lemmaIdIRI (group_concat(DISTINCT concat(?partNr,\":\",?subLemma);separator=\" + \") as ?subLemmata)\n",
    "                    WHERE {\n",
    "                        SELECT ?lemmaIdIRI ?celexComp ?aWordformId ?subLemma ?partNr\n",
    "                        WHERE {\n",
    "                                {\n",
    "                                ?lemmaIdIRI ontolex:lexicalForm ?aWordformId . \n",
    "                                ?lemmaIdIRI decomp:constituent ?celexComp .\n",
    "                                OPTIONAL { ?celexComp gold:stem [ontolex:writtenRep ?subLemma] . }\n",
    "                                OPTIONAL { ?celexComp decomp:correspondsTo [ ontolex:canonicalForm [ontolex:writtenRep ?subLemma]] . }\n",
    "                                }\n",
    "                                {\n",
    "                                    {\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_1> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_2> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_3> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_4> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_5> ?celexComp .}\n",
    "                                        UNION\n",
    "                                        {?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#_6> ?celexComp .}                                        \n",
    "                                    }\n",
    "                                ?lemmaIdIRI ?rdfsynt ?celexComp .\n",
    "                                BIND(IF(STRSTARTS(str(?rdfsynt), \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"), replace(STRAFTER(str(?rdfsynt), \"#\"), \"_\", \"\"), \"999\") AS ?partNr) .\n",
    "                                MINUS {\n",
    "                                    ?lemmaIdIRI <http://www.w3.org/1999/02/22-rdf-syntax-ns#0> ?celexComp .\n",
    "                                    }\n",
    "                                }\n",
    "                            FILTER (?partNr != \"999\") .\n",
    "                            }\n",
    "                            ORDER BY ?partNr\n",
    "                            }\n",
    "                        GROUP BY ?aWordformId ?lemmaIdIRI\n",
    "                    }\n",
    "            }\n",
    "        \"\"\"\n",
    "        \n",
    "    return query\n",
    "\n",
    "\n",
    "\n",
    "def corpus_query_lemma(lemma):\n",
    "    '''\n",
    "    This function builds a query for getting occurances of a given lemma within a given corpus\n",
    "    Args:\n",
    "        lemma: a lemma to look for \n",
    "    Returns:\n",
    "        a corpus query string\n",
    "        \n",
    "    >>> lemma_query = corpus_query_lemma(\"lopen\")\n",
    "    >>> df_corpus = search_corpus(lemma_query, \"chn\")\n",
    "    >>> display(df_corpus)\n",
    "    '''\n",
    "    return r'[lemma=\"'+ lemma + r'\"]'\n",
    "\n",
    "\n",
    "def corpus_query_wordform(word):\n",
    "    '''\n",
    "    This function builds a query for getting occurances of a given wordform within a given corpus\n",
    "    Args:\n",
    "        word: a wordform to look for \n",
    "    Returns:\n",
    "        a corpus query string\n",
    "        \n",
    "    >>> wordform_query = corpus_query_wordform(\"liep\")\n",
    "    >>> df_corpus = search_corpus(wordform_query, \"chn\")\n",
    "    >>> display(df_corpus)\n",
    "    '''\n",
    "    return r'[word=\"'+ word + r'\"]'\n",
    "\n",
    "def lexicon_query_alllemmata(lexicon, pos):\n",
    "    '''\n",
    "    This function builds a query for getting all lemmata of a lexicon, if needed restricted to a given part-of-speech.\n",
    "    The resulting query string is to be used as a parameter of search_lexicon().\n",
    "    \n",
    "    Args:\n",
    "        lexicon: a lexicon name \n",
    "        pos: (optional) a part-of-speech\n",
    "    Returns:\n",
    "        a lexicon query string\n",
    "    '''\n",
    "    \n",
    "    if (lexicon==\"anw\"):\n",
    "        # part-of-speech filter not supported for this lexicon\n",
    "        if (pos is not None and pos != ''):\n",
    "            print('Filtering by part-of-speech is not (yet) supported in the \\''+lexicon+'\\' lexicon')\n",
    "        query = \"\"\"PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "                  PREFIX anw: <http://rdf.ivdnt.org/lexica/anw>                  \n",
    "                  SELECT DISTINCT ?writtenForm\n",
    "                  FROM <http://rdf.ivdnt.org/lexica/anw>\n",
    "                  WHERE {\n",
    "                      ?lemId rdfs:label ?lemma .\n",
    "                      ?lemId ontolex:canonicalForm ?lemCFId . \n",
    "                      ?lemCFId ontolex:writtenRepresentation ?writtenForm .\n",
    "                      }\n",
    "                      ORDER BY ?writtenForm\"\"\"\n",
    "    elif (lexicon==\"celex\"):\n",
    "        # part-of-speech filter not supported for this lexicon\n",
    "        if (pos is not None and pos != ''):\n",
    "            print('Filtering by part-of-speech is not (yet) supported in the \\''+lexicon+'\\' lexicon')\n",
    "        query = \"\"\"\n",
    "            PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "            \n",
    "            SELECT DISTINCT ?lemma AS ?writtenForm\n",
    "            WHERE  {\n",
    "                ?lemmaId ontolex:canonicalForm [ontolex:writtenRep ?lemma] .                \n",
    "                }\n",
    "            ORDER BY ?lemma\"\"\"\n",
    "    elif (lexicon==\"diamant\"):\n",
    "        # part-of-speech filter not supported for this lexicon\n",
    "        if (pos is not None and pos != ''):\n",
    "            print('Filtering by part-of-speech is not (yet) supported in the \\''+lexicon+'\\' lexicon')\n",
    "        query = \"\"\"\n",
    "        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "        prefix prov: <http://www.w3.org/ns/prov#>\n",
    "        prefix diamant: <http://rdf.ivdnt.org/schema/diamant#>\n",
    "        prefix lexinfo: <http://www.lexinfo.net/ontology/2.0/lexinfo#>\n",
    "        prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "        prefix lemon: <http://lemon-model.net/lemon#>\n",
    "        prefix ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "        prefix ud: <http://universaldependencies.org/u/pos/>\n",
    "        prefix skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "        prefix dcterms: <http://purl.org/dc/terms/>\n",
    "        prefix dc: <http://purl.org/dc/terms/>\n",
    "\n",
    "        select DISTINCT ?n_ontolex_writtenRep AS ?writtenForm\n",
    "        where\n",
    "        {\n",
    "        graph ?g\n",
    "        {\n",
    "        {\n",
    "            { ?n_form ontolex:writtenRep ?n_ontolex_writtenRep} .\n",
    "            { ?n_form a ontolex:Form} .\n",
    "        }\n",
    "        }\n",
    "        }\n",
    "        ORDER BY ?n_ontolex_writtenRep\n",
    "        LIMIT 10000\n",
    "        \"\"\"\n",
    "    elif (lexicon==\"duelme\"):\n",
    "        # part-of-speech filter not supported for this lexicon\n",
    "        if (pos is not None and pos != ''):\n",
    "            print('Filtering by part-of-speech is not (yet) supported in the \\''+lexicon+'\\' lexicon')\n",
    "        query = \"\"\"\n",
    "            PREFIX lmf: <http://www.lexinfo.net/lmf>            \n",
    "            SELECT DISTINCT ?lemma AS ?writtenForm\n",
    "            WHERE  {\n",
    "                  ?y lmf:hasLemma ?lemma . \n",
    "            }\n",
    "            ORDER BY ?lemma\"\"\"\n",
    "    elif (lexicon==\"molex\"):\n",
    "        # part-of-speech filter\n",
    "        pos_condition = \"\"\"\"\"\"\n",
    "        if pos is not None and pos != '':\n",
    "            pos_condition = \"\"\"\n",
    "            {?lemEntryId UD:pos ?lemPos .\n",
    "            FILTER regex(?lemPos, '\"\"\"+pos+\"\"\"') } .\n",
    "            \"\"\"\n",
    "        query = \"\"\"\n",
    "                PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
    "                PREFIX UD: <http://universaldependencies.org/u/>\n",
    "                SELECT DISTINCT ?lemma AS ?writtenForm\n",
    "                FROM <http://rdf.ivdnt.org/lexica/molex>\n",
    "                WHERE\n",
    "                {\n",
    "                ?lemEntryId ontolex:canonicalForm ?lemCFId .\n",
    "                ?lemCFId ontolex:writtenRep ?lemma .  \n",
    "                \"\"\"+pos_condition+\"\"\"\n",
    "                }\n",
    "                 ORDER BY ?lemma\"\"\"\n",
    "    else:\n",
    "        raise ValueError(\"Lexicon \" + lexicon + \" not supported for querying all words.\")\n",
    "        \n",
    "    #print(query)\n",
    "    return query\n",
    "\n",
    "def _metadata_from_lexicon_query(lex_query):\n",
    "    '''\n",
    "    Extract metadata fields from a lexicon query string\n",
    "    \n",
    "    Args:\n",
    "        lex_query: A query string issued to a lexicon, can be constructed using lexicon_query()\n",
    "    Returns:\n",
    "        A list of metadata fields\n",
    "    '''\n",
    "    # Get part after select, eg: \"?x ?y ?concat('',z) as ?a\"\n",
    "    select_match = re.search(r'select\\s+(?:distinct)*\\s*(.*)\\s*(?:where|from)', lex_query, flags=re.IGNORECASE)\n",
    "    if select_match:\n",
    "        select_string = select_match.group(1)\n",
    "        #Delete concat() part and following AS, because it can contain a space we do not want to split on\n",
    "        string_wh_concat = re.sub(r'concat\\(.*\\) AS', '', select_string, flags=re.IGNORECASE)\n",
    "        split_string = string_wh_concat.split()\n",
    "        for i,elem in enumerate(split_string):\n",
    "            if elem.lower()==\"AS\":\n",
    "                # Remove AS and element before AS\n",
    "                split_string.pop(i)\n",
    "                split_string.pop(i-1)\n",
    "                # Assume only one AS, so we escape loop\n",
    "                break\n",
    "        columns = [c.lstrip(\"?\") for c in split_string]\n",
    "    else:\n",
    "        raise ValueError(\"No columns find in lexicon query.\")\n",
    "    return columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the cell below to show the UI, and fill in your search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T15:51:29.957256Z",
     "start_time": "2019-02-07T15:51:29.939067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05245223969447b78b1ba7a7bd1852d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='[lemma=\"boek\" & pos=\"verb\"]', description='<b>CQL query:</b>'), Dropdown(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from chaininglib import ui\n",
    "\n",
    "# Create corpus UI, creates references to field contents\n",
    "corpusQueryField, corpusField = create_corpus_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Click the cell below and press Run to perform the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T15:51:31.580633Z",
     "start_time": "2019-02-07T15:51:31.400999Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F...Searching zeebrieven at page 1001...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 "
     ]
    }
   ],
   "source": [
    "#from chaininglib import search\n",
    "query= corpusQueryField.value\n",
    "corpus = corpusField.value\n",
    "df_corpus = search_corpus(query, corpus)\n",
    "#df_corpus = load_dataframe('mijn_resultaten.csv')\n",
    "display_df(df_corpus, title=\"Results:\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the cell below to show the UI, and fill in your search query in the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:27:48.739501Z",
     "start_time": "2019-02-04T17:27:48.079Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from chaininglib import ui\n",
    "searchWordField, lexiconField = create_lexicon_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Click the cell below and press Run to perform the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:27:48.740295Z",
     "start_time": "2019-02-04T17:27:48.082Z"
    }
   },
   "outputs": [],
   "source": [
    "#from chaininglib import queries, search\n",
    "\n",
    "search_word = searchWordField.value\n",
    "lexicon = lexiconField.value\n",
    "# USER: can replace this by own custom query\n",
    "query = lexicon_query(word=search_word, pos= '', lexicon=lexicon)\n",
    "\n",
    "df_lexicon = search_lexicon(query, lexicon)\n",
    "display(df_lexicon)\n",
    "#df_columns_list = list(df_lexicon.columns.values)\n",
    "#df_lexicon_in_columns = df_lexicon[df_columns_list]\n",
    "#display(df_lexicon_in_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 1 (parallel): Frequency of *puur*+verb and *zuiver*+verb compared\n",
    "* Below cell searches for *puur*+verb and for *zuiver*+verb in the CHN corpus\n",
    "* Compare frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:27:48.741069Z",
     "start_time": "2019-02-04T17:27:48.086Z"
    }
   },
   "outputs": [],
   "source": [
    "#from chaininglib import search\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Word 1: puur\n",
    "word1= \"puur\"\n",
    "df_corpus1 = search_corpus('[word=\"' + word1 + r'\"][pos=\"verb\"]',corpus=\"chn\")\n",
    "display(HTML('<b>' + word1 + '</b>'))\n",
    "display(df_corpus1)\n",
    "\n",
    "# Word 2: zuiver\n",
    "word2 = \"zuiver\"\n",
    "df_corpus2 = search_corpus(r'[word=\"' + word2 + r'\"][pos=\"verb\"]',\"chn\")\n",
    "display(HTML('<b>' + word2 + '</b>'))\n",
    "display(df_corpus2)\n",
    "\n",
    "# Compute difference\n",
    "diff_left, diff_right, intersec = column_difference(df_corpus1[\"word 1\"], df_corpus2[\"word 1\"])\n",
    "# Elements of 1 that are not in 2\n",
    "display(HTML('Werkwoorden voor <b>' + word1 + '</b> niet in <b>' + word2 + '</b>: ' + \", \".join(diff_left)))\n",
    "# Elements of 2 that are not in 1\n",
    "display(HTML('Werkwoorden voor <b>' + word1 + '</b> niet in <b>' + word2 + '</b>: ' + \", \".join(diff_right)))\n",
    "# Elements both in 1 and 2\n",
    "display(HTML('Werkwoorden zowel voor <b>' + word1 + '</b> als voor <b>' + word2 + '</b>: ' + \", \".join(intersec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 2 (sequential): Retrieve synonyms from DiaMaNT, look up in Gysseling\n",
    "* Below cell searches for term \"boek\" in DiaMaNT, and looks up all variants in Gysseling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:27:48.741938Z",
     "start_time": "2019-02-04T17:27:48.090Z"
    }
   },
   "outputs": [],
   "source": [
    "search_word = \"boek\"\n",
    "lexicon = \"diamant\"\n",
    "corpus= \"gysseling\"\n",
    "\n",
    "# First, lookup synonyms in DiaMaNT\n",
    "query = lexicon_query(word=search_word, pos= '', lexicon=lexicon)\n",
    "df_lexicon = search_lexicon(query, lexicon)\n",
    "syns = diamant_get_synonyms(df_lexicon) \n",
    "syns.add(search_word) # Also add search word itself\n",
    "display(HTML('Synoniemen voor <b>' + search_word + '</b>: ' + \", \".join(syns)))\n",
    "\n",
    "# Search for all synonyms in corpus\n",
    "## Create queries: search by lemma\n",
    "syns_queries = [corpus_query_lemma(syn) for syn in syns]\n",
    "## Search for all synonyms in corpus\n",
    "result_dict = search_corpus_multiple(syns_queries, corpus)\n",
    "view_multiple_results(result_dict, labels=list(syns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T16:24:19.655999Z",
     "start_time": "2019-01-11T16:24:19.645252Z"
    }
   },
   "source": [
    "## Case study (parallel) 3: Find corpus words not in lexicon; list most frequent ones.\n",
    "* Only parallel if you can ask the lexicon a list of all words.\n",
    "* Currently only working: ask DiaMaNT list of words (limited at 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:27:48.742799Z",
     "start_time": "2019-02-04T17:27:48.094Z"
    }
   },
   "outputs": [],
   "source": [
    "# Query lexicon to give list of all words\n",
    "lexicon=\"anw\"\n",
    "df_lexicon = search_lexicon_alllemmata(lexicon)\n",
    "## TODO: Why do double words appear?\n",
    "display(df_lexicon)\n",
    "lexicon_set = sorted( set([w.lower() for w in df_lexicon[\"writtenForm\"]]) )\n",
    "display(lexicon_set)\n",
    "\n",
    "df_corpus = search_corpus_allwords(\"gysseling\", None)\n",
    "display(df_corpus)\n",
    "len(df_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T15:46:20.519833Z",
     "start_time": "2019-01-16T15:46:20.516208Z"
    }
   },
   "source": [
    "## Case study (sequential) 4: Find occurences of attributive adjectives not ending with -e, even though they are preceeded by a definite article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:27:48.743596Z",
     "start_time": "2019-02-04T17:27:48.098Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_to_search=\"opensonar\"\n",
    "lexicon_to_search=\"molex\"\n",
    "\n",
    "# CORPUS: get [article + attributive adjective + nouns] combinations in which the adjective does not end with -e\n",
    "print('Stap 1:')\n",
    "df_corpus = search_corpus(r'[lemma=\"de|het\"][word=\"^g(.+)[^e]$\" & pos=\"ADJ\"][pos=\"NOUN\"]', corpus=corpus_to_search)\n",
    "display(df_corpus)\n",
    "\n",
    "# LEXICON: get adjectives the lemma of which does not end with -e\n",
    "query=lexicon_query('^g(.+)[^e]$', 'ADJ', lexicon_to_search)\n",
    "df_lexicon = search_lexicon(query, lexicon_to_search)\n",
    "display(df_lexicon)\n",
    "\n",
    "# LEXICON: get adjectives having a final -e in definite attributive use\n",
    "print('Filtering lexicon results')\n",
    "df_lexicon_form_e = df_lexicon[ filter_condition(df_lexicon,column=\"wordform\",method=\"contains_regex\", regex_or_set = 'e$') ]\n",
    "#final_e_condition=df_lexicon.wordform.str.contains('e$')\n",
    "#df = df_lexicon[final_e_condition]\n",
    "display(df_lexicon_form_e)\n",
    "\n",
    "# RESULT: get the records out of our first list in which the -e-less-adjectives match the lemma form of our last list\n",
    "print('Wanted list:')\n",
    "e_forms = list(df_lexicon_form_e.lemma)\n",
    "#no_final_e_condition = df_corpus['word 1'].isin(eless_forms)\n",
    "#result_df = df_corpus[no_final_e_condition]\n",
    "result_df = df_corpus[ filter_condition(df_corpus, column = \"word 1\", method=\"isin_set\", regex_or_set=e_forms) ]\n",
    "display( result_df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study (sequential) 5: (morphosyntactic lexicon and possibly unannotated corpus) Look up inflected forms and spelling variants for a given lemma in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:27:48.744390Z",
     "start_time": "2019-02-04T17:27:48.101Z"
    }
   },
   "outputs": [],
   "source": [
    "lexicon_to_search=\"molex\"\n",
    "corpus_to_search=\"chn\"\n",
    "\n",
    "##############################################\n",
    "# TODO  zelfde met meerdere lemmata en gegroepeerd \n",
    "##############################################\n",
    "\n",
    "lemma_to_look_for=\"denken\"\n",
    "\n",
    "# LEXICON: Search for the inflected forms of a lemma in a morphosyntactic lexicon\n",
    "query=lexicon_query(lemma_to_look_for, None, lexicon_to_search)\n",
    "df_lexicon = search_lexicon(query, lexicon_to_search)\n",
    "display(df_lexicon)\n",
    "\n",
    "# Put all inflected forms into a list\n",
    "inflected_wordforms = list(df_lexicon.wordform)\n",
    "\n",
    "# CORPUS: Look up the inflected forms in a (possibly unannotated) corpus\n",
    "# beware: If the corpus is not annotated, all we can do is searching for the inflected words\n",
    "#         But if the corpus is lemmatized, we have to make sure we're retrieving correct data by specifying the lemma as well\n",
    "annotated_corpus = True\n",
    "query = r'[lemma=\"'+lemma_to_look_for+r'\" & word=\"'+r\"|\".join(inflected_wordforms)+r'\"]' if annotated_corpus else r'[word=\"'+r\"|\".join(inflected_wordforms)+r'\"]'\n",
    "df_corpus = search_corpus(query, corpus=corpus_to_search)\n",
    "display(df_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 6: Build frequency table of some corpus, based on lemma list of a given lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:27:48.745116Z",
     "start_time": "2019-02-04T17:27:48.107Z"
    }
   },
   "outputs": [],
   "source": [
    "base_lexicon=\"molex\"\n",
    "corpus_to_search1=\"opensonar\"\n",
    "corpus_to_search2=\"chn\"\n",
    "\n",
    "# build frequency tables of two corpora\n",
    "\n",
    "df_frequency_list1 = get_frequency_list(base_lexicon, \"NOUN\", corpus_to_search1)\n",
    "display( df_frequency_list1.sort_values(ascending=False,by=['raw_freq']).head(25) )\n",
    "display_df(df_frequency_list1.sort_values(ascending=True, by=['rank']).head(25), columns='raw_freq', title='chart df1', mode='chart' )\n",
    "\n",
    "df_frequency_list2 = get_frequency_list(base_lexicon, \"NOUN\", corpus_to_search2)\n",
    "display(df_frequency_list2.sort_values(ascending=False,by=['raw_freq']).head(25))\n",
    "display_df(df_frequency_list2.sort_values(ascending=True, by=['rank']).head(25), columns='raw_freq', title='chart df2', mode='chart' )\n",
    "\n",
    "\n",
    "# TODO: lemmata tonen die in 1 of 2 ontbreken\n",
    "\n",
    "# compute the rank diff of lemmata in frequency tables\n",
    "\n",
    "df_rankdiffs = get_rank_diff(df_frequency_list1, df_frequency_list2)\n",
    "\n",
    "display(df_rankdiffs.sort_values(by=['rank_diff']).head(25))\n",
    "display_df( df_rankdiffs.sort_values(ascending=False, by=['rank_diff']).head(25), columns='rank_diff', title='chart large diff', mode='chart' )\n",
    "display_df( df_rankdiffs.sort_values(ascending=True, by=['rank_diff']).head(25), columns='rank_diff', title='chart small diff', mode='chart' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 7: search in a corpus for wordforms of a lemma, which are not included in this lemma's paramadigm in a lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:27:48.745942Z",
     "start_time": "2019-02-04T17:27:48.110Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "base_lexicon=\"molex\"\n",
    "corpus_to_search=\"opensonar\"\n",
    "\n",
    "df = get_missing_wordforms(base_lexicon, \"VERB\", corpus_to_search)\n",
    "\n",
    "df.to_csv( \"missing_wordforms.csv\", index=False)\n",
    "#df = load_dataframe(\"missing_wordforms.csv\")\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 8: Train a tagger with data from an annotated corpus, and do something cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T17:27:48.746715Z",
     "start_time": "2019-02-04T17:27:48.113Z"
    }
   },
   "outputs": [],
   "source": [
    "base_lexicon=\"molex\"\n",
    "corpus_to_search1=\"opensonar\"\n",
    "corpus_to_search2=\"chn\"\n",
    "\n",
    "# we have a given word, let's say: \"loop\"\n",
    "some_word = \"loop\"\n",
    "\n",
    "# get the paradigm of the lemma our word is a part of\n",
    "query = lexicon_query(some_word, pos=None, lexicon=base_lexicon)\n",
    "df_paradigm = search_lexicon(query, base_lexicon)\n",
    "display(df_paradigm)\n",
    "\n",
    "# gather some pattern including our word, out of an annotated corpus\n",
    "# here: DET + ADJ + 'loop'\n",
    "corpus_query = corpus_query_wordform(some_word)\n",
    "df_corpus1 = search_corpus(corpus_query, corpus=corpus_to_search1, detailed_context=True)\n",
    "display(df_corpus1)\n",
    "df_corpus2 = search_corpus(corpus_query, corpus=corpus_to_search2, detailed_context=True)\n",
    "display(df_corpus2)\n",
    "\n",
    "\n",
    "df_all = concat_df([df_corpus1, df_corpus2], [corpus_to_search1, corpus_to_search2])\n",
    "display(df_all)\n",
    "\n",
    "# get a tagger trained with our corpus data\n",
    "tagger = get_tagger(df_all)\n",
    "\n",
    "# Use the trained tagger to tag unknown sentences\n",
    "# The input must be like: tagger.tag(['today','is','a','beautiful','day'])\n",
    "\n",
    "sentence = 'Mijn buurman kijkt door de loop van zijn geweer'\n",
    "tagged_sentence = tagger.tag( sentence.split() )\n",
    "\n",
    "print(tagged_sentence)\n",
    "\n",
    "\n",
    "# Know we can lemmatize each occurence of our lemma in the new sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 9: Search in corpus and filter on metadata\n",
    "First, we request all available metadata fields of the corpus. Then, we issue a search query, and request all metadata fields for the result. Finally, we filter on metadata values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T16:29:06.839642Z",
     "start_time": "2019-02-08T16:29:06.410879Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_name=\"zeebrieven\"\n",
    "query=r'[lemma=\"boek\"]'\n",
    "# Request all metadata fields from corpus\n",
    "fields = get_available_metadata(\"corpus\", corpus_name)\n",
    "# Perform query and ask all metadata\n",
    "df_corpus = search_corpus(query, corpus_name, extra_fields_doc=fields[\"document\"])\n",
    "\n",
    "# All results\n",
    "display_df(df_corpus, title=\"All results:\")\n",
    "\n",
    "# Filter on year: > 1700\n",
    "df_filter_year = df_corpus[df_corpus[\"witnessYear_from\"].astype('int32') > 1700] \n",
    "display_df(df_filter_year, title=\"After 1700:\")\n",
    "\n",
    "# Filter on sender birth place Leiden\n",
    "#condition = filter_condition(df_corpus, column=\"afz_geb_plaats\", method=\"contains_regex\", regex_or_set=\"Leiden\")\n",
    "#df_filter_place = df_corpus[ condition ]\n",
    "#display_df(df_filter_place, title=\"Sender born in Leiden:\")\n",
    "\n",
    "# Group by birth place\n",
    "display_df(df_corpus.groupby(\"adr_loc_plaats\").size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 10: Visualizing h-dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_to_search=\"chn\"\n",
    "\n",
    "fields = get_available_metadata(\"corpus\", corpus_to_search)\n",
    "#print(fields)\n",
    "\n",
    "df_corpus1 = search_corpus(r'[lemma=\"h[aeo].*\" & word=\"[aeo].*\"]', corpus_to_search, extra_fields_doc=fields[\"document\"])\n",
    "df_corpus2 = search_corpus(r'[lemma=\"h[aeo].*\" & word=\"h[aeo].*\"]', corpus_to_search, extra_fields_doc=fields[\"document\"])\n",
    "\n",
    "display_df( df_corpus1)\n",
    "display_df( df_corpus2)\n",
    "\n",
    "display_df( df_corpus1.groupby([\"Region\", \"Date\"]), title=\"h-dropping\", mode='chart')\n",
    "display_df( df_corpus2.groupby([\"Region\", \"Date\"]), title=\"normal\", mode='chart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs_env",
   "language": "python",
   "name": "cs_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
