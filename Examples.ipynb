{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples Chaining search\n",
    "This notebook contains a number of examples of chaining linguistic resources: corpora, lexica and treebanks. Try the examples, or copy the code and customize the examples in the [Sandbox](Sandbox.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of examples\n",
    "### Corpora\n",
    " * [Corpus search](#corpus-search)\n",
    " * [Frequency of *zeker*+verb and *vast*+verb compared](#freq-puur-zuiver)\n",
    " * [Train a POS tagger on an annotated corpus](#pos-tagger)\n",
    " * [Search in corpus and filter on metadata](#corpus-filter-metadata)\n",
    " * [Visualizing h-dropping](#visualizing-h-dropping)\n",
    " * [Generate lexicon from several corpora](#lexicon-several-corpora)\n",
    "\n",
    "### Lexica\n",
    " * [Lexicon search](#lexicon-search)\n",
    "\n",
    "### Corpus + lexicon\n",
    " * [Retrieve synonyms from DiaMaNT, look up in Gysseling](#synonyms-diamant-gysseling)\n",
    " * [Build a frequency list of the lemma of some corpus output](#freq-lemma-corpus)\n",
    " * [Find occurences of attributive adjectives not ending with -e, even though they are preceeded by a definite article](#adjective-e)\n",
    " * [Look up inflected forms and spelling variants for a given lemma in a corpus](#inflected-spelling-corpus)\n",
    " * [Corpus frequency list of lemmata from lexicon with given lemma](#corpus-frequency-lemma-pos)\n",
    " * [Build a frequency table of some corpus, based on lemmata of a given lexicon](#freqtable-lemmalist)\n",
    " * [Search corpus for wordforms of lemma not included in lexicon](#corpus-wordforms-not-lexicon)\n",
    " \n",
    "### Treebanks\n",
    " * [Treebank search](#treebank-search)\n",
    " * [Which objects of verb *geven* occur?](#treebank-objects-geven)\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T10:51:07.839335Z",
     "start_time": "2019-03-04T10:51:07.835114Z"
    }
   },
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus search <a class=\"anchor\" id=\"corpus-search\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the cell below to show the UI, and fill in your search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:16:27.367051Z",
     "start_time": "2019-03-08T15:16:27.074574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4484d2324f4e7d89227294f59fc55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='[lemma=\"boek\"]', description='<b>CQL query:</b>'), Dropdown(description='<b>Corpus:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from chaininglib.ui.search import create_corpus_ui\n",
    "from chaininglib.ui.dfui import display_df, get_uploader\n",
    "\n",
    "# Create corpus UI, creates references to field contents\n",
    "corpusQueryField, corpusField = create_corpus_ui()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Click the cell below and press Run to perform the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:16:54.232642Z",
     "start_time": "2019-03-08T15:16:42.875451Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                                    "
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Results</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left context</th>\n",
       "      <th>lemma 0</th>\n",
       "      <th>pos 0</th>\n",
       "      <th>word 0</th>\n",
       "      <th>right context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>heeft 4 gl 0 Aen</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeken</td>\n",
       "      <td>en pampier en pennen en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lijeue man stelt alles te</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeck</td>\n",
       "      <td>waet ghij uijt geft dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alzoo hij niet op de</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>bouck</td>\n",
       "      <td>en stondt en hij heeft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Schrijfpampier a 6 Sr t</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boek.</td>\n",
       "      <td>50 ditto ongsneeden ditto a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>voorne missive, dat UEDs mijn</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeken</td>\n",
       "      <td>bij berntrop laat verkoopen, als</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>verbeeld hebbe, dat wat geleerde</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeken</td>\n",
       "      <td>aan gaat, schouten altoos geprafereert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>beijde, accuratesse goede order der</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boecken,</td>\n",
       "      <td>&amp; Voorsigtigheijd, die prijse ik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rollen matten MR &amp; Eenige</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeken</td>\n",
       "      <td>Maar also die twee Scheepen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>te Senden, &amp; wat de</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeken</td>\n",
       "      <td>aan betreft Zo Gelieft maar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>laaten wagten tot ik mijn</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeken</td>\n",
       "      <td>heeft, want weet anders niet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&amp; houw nu alle de</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeken</td>\n",
       "      <td>&amp; bonte &amp; die sijn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gantsche regel die in myn</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>Boek</td>\n",
       "      <td>niet te vinden is, en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>al frij hoch bij hente</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>bock</td>\n",
       "      <td>soo het scijnt en men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>reeck. van hem wt v</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeken</td>\n",
       "      <td>Is heden versuijmt wij sijn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>maer ick behoefde wel een</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeck</td>\n",
       "      <td>pampier te hebben daerom sal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dan ick behoefde wel een</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeck</td>\n",
       "      <td>Pampier daerom sal ick opbreecken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>van concept en wilde syn</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeken</td>\n",
       "      <td>absolut ter secretarij versorgen Eyndelyk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>’t Eijschen, van onvertueren van</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeken</td>\n",
       "      <td>ligten van copien Extracten of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>post ryders of eenige andere</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeken,</td>\n",
       "      <td>ook eenige ingelyd groente of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ik u bij dese, 2</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>Boecken,</td>\n",
       "      <td>die bij mij gedrukt en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>seggen dat in ander mans</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>bocken</td>\n",
       "      <td>quadt studeren is voor waert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>niets van myne agter gelatene</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeken</td>\n",
       "      <td>vernomen nog hoe de suyker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Bedanke UEd voor de toegezondene</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>Boeken,</td>\n",
       "      <td>houde my daarontrent Verder gerécommandeert,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>26 september 1780 Nota Van</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>Boeken</td>\n",
       "      <td>&amp; voor d’Heer J.M.Van Vloten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Hr van Tiel die uE</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeken</td>\n",
       "      <td>houdt, heeft niet kunnen klaar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>om uE Copij van die</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeken</td>\n",
       "      <td>te senden, het welk pr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>de Datums als in Zyn</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boek</td>\n",
       "      <td>genoteert staat, de reeden is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>leezen van diverse, voornaemlijk Historische,</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>Boeken</td>\n",
       "      <td>zodaenig zult bevlijtigd hebben dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>heeft UE schryf om een</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boek</td>\n",
       "      <td>ik weet niet hoe veel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>die moet nu ook gedurig</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeken</td>\n",
       "      <td>hebbe, ik ben alle jare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ik het wissiltie wegens die</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>boeken</td>\n",
       "      <td>die onse Zoon Gerrit heeft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ik myn liede man wat</td>\n",
       "      <td>boek</td>\n",
       "      <td>NOU</td>\n",
       "      <td>bokken</td>\n",
       "      <td>wat kastengen wat nuyten wat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     left context lemma 0 pos 0    word 0  \\\n",
       "0                                heeft 4 gl 0 Aen    boek   NOU    boeken   \n",
       "1                       lijeue man stelt alles te    boek   NOU     boeck   \n",
       "2                            alzoo hij niet op de    boek   NOU     bouck   \n",
       "3                         Schrijfpampier a 6 Sr t    boek   NOU     boek.   \n",
       "4                   voorne missive, dat UEDs mijn    boek   NOU    boeken   \n",
       "5                verbeeld hebbe, dat wat geleerde    boek   NOU    boeken   \n",
       "6             beijde, accuratesse goede order der    boek   NOU  boecken,   \n",
       "7                       rollen matten MR & Eenige    boek   NOU    boeken   \n",
       "8                             te Senden, & wat de    boek   NOU    boeken   \n",
       "9                       laaten wagten tot ik mijn    boek   NOU    boeken   \n",
       "10                              & houw nu alle de    boek   NOU    boeken   \n",
       "11                      gantsche regel die in myn    boek   NOU      Boek   \n",
       "12                         al frij hoch bij hente    boek   NOU      bock   \n",
       "13                            reeck. van hem wt v    boek   NOU    boeken   \n",
       "14                      maer ick behoefde wel een    boek   NOU     boeck   \n",
       "15                       dan ick behoefde wel een    boek   NOU     boeck   \n",
       "16                       van concept en wilde syn    boek   NOU    boeken   \n",
       "17               ’t Eijschen, van onvertueren van    boek   NOU    boeken   \n",
       "18                   post ryders of eenige andere    boek   NOU   boeken,   \n",
       "19                               ik u bij dese, 2    boek   NOU  Boecken,   \n",
       "20                       seggen dat in ander mans    boek   NOU    bocken   \n",
       "21                  niets van myne agter gelatene    boek   NOU    boeken   \n",
       "22               Bedanke UEd voor de toegezondene    boek   NOU   Boeken,   \n",
       "23                     26 september 1780 Nota Van    boek   NOU    Boeken   \n",
       "24                             Hr van Tiel die uE    boek   NOU    boeken   \n",
       "25                            om uE Copij van die    boek   NOU    boeken   \n",
       "26                           de Datums als in Zyn    boek   NOU      boek   \n",
       "27  leezen van diverse, voornaemlijk Historische,    boek   NOU    Boeken   \n",
       "28                         heeft UE schryf om een    boek   NOU      boek   \n",
       "29                        die moet nu ook gedurig    boek   NOU    boeken   \n",
       "30                    ik het wissiltie wegens die    boek   NOU    boeken   \n",
       "31                           ik myn liede man wat    boek   NOU    bokken   \n",
       "\n",
       "                                   right context  \n",
       "0                        en pampier en pennen en  \n",
       "1                        waet ghij uijt geft dat  \n",
       "2                         en stondt en hij heeft  \n",
       "3                    50 ditto ongsneeden ditto a  \n",
       "4               bij berntrop laat verkoopen, als  \n",
       "5         aan gaat, schouten altoos geprafereert  \n",
       "6               & Voorsigtigheijd, die prijse ik  \n",
       "7                    Maar also die twee Scheepen  \n",
       "8                    aan betreft Zo Gelieft maar  \n",
       "9                   heeft, want weet anders niet  \n",
       "10                            & bonte & die sijn  \n",
       "11                         niet te vinden is, en  \n",
       "12                         soo het scijnt en men  \n",
       "13                   Is heden versuijmt wij sijn  \n",
       "14                  pampier te hebben daerom sal  \n",
       "15             Pampier daerom sal ick opbreecken  \n",
       "16     absolut ter secretarij versorgen Eyndelyk  \n",
       "17                ligten van copien Extracten of  \n",
       "18                 ook eenige ingelyd groente of  \n",
       "19                        die bij mij gedrukt en  \n",
       "20                  quadt studeren is voor waert  \n",
       "21                    vernomen nog hoe de suyker  \n",
       "22  houde my daarontrent Verder gerécommandeert,  \n",
       "23                  & voor d’Heer J.M.Van Vloten  \n",
       "24                houdt, heeft niet kunnen klaar  \n",
       "25                        te senden, het welk pr  \n",
       "26                 genoteert staat, de reeden is  \n",
       "27           zodaenig zult bevlijtigd hebben dat  \n",
       "28                         ik weet niet hoe veel  \n",
       "29                       hebbe, ik ben alle jare  \n",
       "30                    die onse Zoon Gerrit heeft  \n",
       "31                  wat kastengen wat nuyten wat  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b7162d320c49bca9f8eb467a3b8826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Sla uw resultaten op:'), Text(value='Results.csv'), Button(button_style='warning',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from chaininglib.search.CorpusQuery import *\n",
    "\n",
    "#from chaininglib import search\n",
    "query= corpusQueryField.value\n",
    "corpus_name = corpusField.value\n",
    "df_corpus = create_corpus(corpus_name).pattern(query).search().kwic()\n",
    "#df_corpus = load_dataframe('mijn_resultaten.csv')\n",
    "display_df(df_corpus, labels=\"Results\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of *zeker*+verb and *vast*+verb compared <a class=\"anchor\" id=\"freq-puur-zuiver\"></a>\n",
    "* Below cell searches for *zeker*+verb and for *vast*+verb in the Letters as Loot (zeebrieven) corpus\n",
    "* Compare frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:33:20.909030Z",
     "start_time": "2019-03-06T16:33:20.304048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                                    "
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>zeker</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left context</th>\n",
       "      <th>lemma 0</th>\n",
       "      <th>pos 0</th>\n",
       "      <th>word 0</th>\n",
       "      <th>lemma 1</th>\n",
       "      <th>pos 1</th>\n",
       "      <th>word 1</th>\n",
       "      <th>right context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>onder weeg is UE sal</td>\n",
       "      <td>zeker</td>\n",
       "      <td>ADV</td>\n",
       "      <td>seeker</td>\n",
       "      <td>kunnen</td>\n",
       "      <td>VRB</td>\n",
       "      <td>kunne</td>\n",
       "      <td>sien hoi haer ge stael</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maken soo salt maer alte</td>\n",
       "      <td>zeker</td>\n",
       "      <td>ADV</td>\n",
       "      <td>seckr</td>\n",
       "      <td>voortgaan</td>\n",
       "      <td>VRB</td>\n",
       "      <td>voort</td>\n",
       "      <td>gaen daerom meene ick het</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dit zoo zijnde, dat niet</td>\n",
       "      <td>zeker</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>zeker</td>\n",
       "      <td>weten</td>\n",
       "      <td>VRB</td>\n",
       "      <td>weet,</td>\n",
       "      <td>kom in t geval van</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>niet en kan men niet</td>\n",
       "      <td>zeker</td>\n",
       "      <td>ADV</td>\n",
       "      <td>seecker</td>\n",
       "      <td>schrijven</td>\n",
       "      <td>VRB</td>\n",
       "      <td>Schrijven</td>\n",
       "      <td>God gun en geeft ons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nes UWEGb genoegen, hie sal</td>\n",
       "      <td>zeker</td>\n",
       "      <td>ADV</td>\n",
       "      <td>seeker</td>\n",
       "      <td>melden</td>\n",
       "      <td>VRB</td>\n",
       "      <td>gemeld</td>\n",
       "      <td>hebben de doot van suster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>oope dat Ick daer van</td>\n",
       "      <td>zeker</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>seeker</td>\n",
       "      <td>zullen</td>\n",
       "      <td>VRB</td>\n",
       "      <td>sal</td>\n",
       "      <td>weese maer het sal noch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kan ik uw ook niets</td>\n",
       "      <td>zeker</td>\n",
       "      <td>ADV</td>\n",
       "      <td>seeker</td>\n",
       "      <td>melden</td>\n",
       "      <td>VRB</td>\n",
       "      <td>melden,</td>\n",
       "      <td>want die producten dat wij</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mijn vertrek kan ik niets</td>\n",
       "      <td>zeker</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>zeeker</td>\n",
       "      <td>melden</td>\n",
       "      <td>VRB</td>\n",
       "      <td>melde</td>\n",
       "      <td>denke in ’t Laast van</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doch ick en kan nietmendal</td>\n",
       "      <td>zeker</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>seekers</td>\n",
       "      <td>schrijven</td>\n",
       "      <td>VRB</td>\n",
       "      <td>schrijuen,</td>\n",
       "      <td>ende verhoope UE in corte</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  left context lemma 0 pos 0   word 0    lemma 1 pos 1  \\\n",
       "0         onder weeg is UE sal   zeker   ADV   seeker     kunnen   VRB   \n",
       "1     maken soo salt maer alte   zeker   ADV    seckr  voortgaan   VRB   \n",
       "2     dit zoo zijnde, dat niet   zeker   ADJ    zeker      weten   VRB   \n",
       "3         niet en kan men niet   zeker   ADV  seecker  schrijven   VRB   \n",
       "4  Nes UWEGb genoegen, hie sal   zeker   ADV   seeker     melden   VRB   \n",
       "5        oope dat Ick daer van   zeker   ADJ   seeker     zullen   VRB   \n",
       "6          kan ik uw ook niets   zeker   ADV   seeker     melden   VRB   \n",
       "7    mijn vertrek kan ik niets   zeker   ADJ   zeeker     melden   VRB   \n",
       "8   doch ick en kan nietmendal   zeker   ADJ  seekers  schrijven   VRB   \n",
       "\n",
       "       word 1               right context  \n",
       "0       kunne      sien hoi haer ge stael  \n",
       "1       voort   gaen daerom meene ick het  \n",
       "2       weet,          kom in t geval van  \n",
       "3   Schrijven        God gun en geeft ons  \n",
       "4      gemeld   hebben de doot van suster  \n",
       "5         sal     weese maer het sal noch  \n",
       "6     melden,  want die producten dat wij  \n",
       "7       melde       denke in ’t Laast van  \n",
       "8  schrijuen,   ende verhoope UE in corte  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9e5b66a9cd4aa68ab7fe201446ce0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Sla uw resultaten op:'), Text(value='zeker.csv'), Button(button_style='warning', d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                                    "
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>vast</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left context</th>\n",
       "      <th>lemma 0</th>\n",
       "      <th>pos 0</th>\n",
       "      <th>word 0</th>\n",
       "      <th>lemma 1</th>\n",
       "      <th>pos 1</th>\n",
       "      <th>word 1</th>\n",
       "      <th>right context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M.Dalij, die heeft mij voor</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>vast</td>\n",
       "      <td>beloven</td>\n",
       "      <td>VRB</td>\n",
       "      <td>beloofd,</td>\n",
       "      <td>de helfte van sijne Reekening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aff scheijden &amp; met die</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>vaste</td>\n",
       "      <td>hopen</td>\n",
       "      <td>VRB</td>\n",
       "      <td>hoope,</td>\n",
       "      <td>in onse loop baan wandelen,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tog kan ik het niet</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>vast</td>\n",
       "      <td>schrijven</td>\n",
       "      <td>VRB</td>\n",
       "      <td>schryven</td>\n",
       "      <td>want daris somtys gen stat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ghe sicht dat wij voor</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>vast</td>\n",
       "      <td>vertrouwen</td>\n",
       "      <td>VRB</td>\n",
       "      <td>vertrouden</td>\n",
       "      <td>dat altemael turcken waeren wij</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bessten koomen dan gij kend</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>vast</td>\n",
       "      <td>geloven</td>\n",
       "      <td>VRB</td>\n",
       "      <td>gelooven</td>\n",
       "      <td>dat gij mij niet half</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hoop Ick dat het nu</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>vast</td>\n",
       "      <td>zullen</td>\n",
       "      <td>VRB</td>\n",
       "      <td>sal</td>\n",
       "      <td>gaen daerom versuijmt nu geen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ik kan het niet voor</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>vast</td>\n",
       "      <td>schrijven</td>\n",
       "      <td>VRB</td>\n",
       "      <td>schryve</td>\n",
       "      <td>Dog hoop ik dat Got</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Vertrek Kan ik nog niet</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>fast</td>\n",
       "      <td>melden</td>\n",
       "      <td>VRB</td>\n",
       "      <td>Melden</td>\n",
       "      <td>want Het is Hier Zeer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in leyt want komt hy</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>vast</td>\n",
       "      <td>terechtkomen</td>\n",
       "      <td>VRB</td>\n",
       "      <td>te</td>\n",
       "      <td>regt want de heer en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>godt vartrout dij heft soo</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>vast</td>\n",
       "      <td>bouwen</td>\n",
       "      <td>VRB</td>\n",
       "      <td>gebout</td>\n",
       "      <td>hijer mede wens jck mij</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>beste van Niet hebbe het</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>vast</td>\n",
       "      <td>voornemen</td>\n",
       "      <td>VRB</td>\n",
       "      <td>voorgenoomen</td>\n",
       "      <td>Zoo godt wel om bij</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>en dan sal wy wel</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>vas</td>\n",
       "      <td>afdanken</td>\n",
       "      <td>VRB</td>\n",
       "      <td>afgedak</td>\n",
       "      <td>worden moeder doet de groetenis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>daar zyn E myn soo</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>vast</td>\n",
       "      <td>beloven</td>\n",
       "      <td>VRB</td>\n",
       "      <td>belooft</td>\n",
       "      <td>heeft geen schip te laten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>en dat agter ’t schip</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>vast</td>\n",
       "      <td>zijn</td>\n",
       "      <td>VRB</td>\n",
       "      <td>was</td>\n",
       "      <td>hebben wy verlooren, een groote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>zyn, dat ik in gedagte</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>vast</td>\n",
       "      <td>zijn</td>\n",
       "      <td>VRB</td>\n",
       "      <td>ben</td>\n",
       "      <td>geweest, hier geleegentheyd was dezelve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>want ’t zal zyn leeven</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>vast</td>\n",
       "      <td>verlengen</td>\n",
       "      <td>VRB</td>\n",
       "      <td>verlengen,</td>\n",
       "      <td>en buyten dien, zyn de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>is van consept ik hadt</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>vast</td>\n",
       "      <td>denken</td>\n",
       "      <td>VRB</td>\n",
       "      <td>gedagt</td>\n",
       "      <td>dat uwed= gerippatriejeerdt soude hebbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[…] waer op u l</td>\n",
       "      <td>vast</td>\n",
       "      <td>ADV</td>\n",
       "      <td>vast</td>\n",
       "      <td>mogen</td>\n",
       "      <td>VRB</td>\n",
       "      <td>moocht</td>\n",
       "      <td>vertrouwen wij sijn noch altesamen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   left context lemma 0 pos 0 word 0       lemma 1 pos 1  \\\n",
       "0   M.Dalij, die heeft mij voor    vast   ADV   vast       beloven   VRB   \n",
       "1       aff scheijden & met die    vast   ADJ  vaste         hopen   VRB   \n",
       "2           tog kan ik het niet    vast   ADV   vast     schrijven   VRB   \n",
       "3        ghe sicht dat wij voor    vast   ADV   vast    vertrouwen   VRB   \n",
       "4   bessten koomen dan gij kend    vast   ADV   vast       geloven   VRB   \n",
       "5           hoop Ick dat het nu    vast   ADV   vast        zullen   VRB   \n",
       "6          ik kan het niet voor    vast   ADV   vast     schrijven   VRB   \n",
       "7       Vertrek Kan ik nog niet    vast   ADV   fast        melden   VRB   \n",
       "8          in leyt want komt hy    vast   ADV   vast  terechtkomen   VRB   \n",
       "9    godt vartrout dij heft soo    vast   ADV   vast        bouwen   VRB   \n",
       "10     beste van Niet hebbe het    vast   ADV   vast     voornemen   VRB   \n",
       "11            en dan sal wy wel    vast   ADV    vas      afdanken   VRB   \n",
       "12           daar zyn E myn soo    vast   ADV   vast       beloven   VRB   \n",
       "13        en dat agter ’t schip    vast   ADV   vast          zijn   VRB   \n",
       "14       zyn, dat ik in gedagte    vast   ADV   vast          zijn   VRB   \n",
       "15       want ’t zal zyn leeven    vast   ADV   vast     verlengen   VRB   \n",
       "16       is van consept ik hadt    vast   ADV   vast        denken   VRB   \n",
       "17              […] waer op u l    vast   ADV   vast         mogen   VRB   \n",
       "\n",
       "          word 1                            right context  \n",
       "0       beloofd,            de helfte van sijne Reekening  \n",
       "1         hoope,              in onse loop baan wandelen,  \n",
       "2       schryven               want daris somtys gen stat  \n",
       "3     vertrouden          dat altemael turcken waeren wij  \n",
       "4       gelooven                    dat gij mij niet half  \n",
       "5            sal            gaen daerom versuijmt nu geen  \n",
       "6        schryve                      Dog hoop ik dat Got  \n",
       "7         Melden                    want Het is Hier Zeer  \n",
       "8             te                     regt want de heer en  \n",
       "9         gebout                  hijer mede wens jck mij  \n",
       "10  voorgenoomen                      Zoo godt wel om bij  \n",
       "11       afgedak          worden moeder doet de groetenis  \n",
       "12       belooft                heeft geen schip te laten  \n",
       "13           was          hebben wy verlooren, een groote  \n",
       "14           ben  geweest, hier geleegentheyd was dezelve  \n",
       "15    verlengen,                   en buyten dien, zyn de  \n",
       "16        gedagt  dat uwed= gerippatriejeerdt soude hebbe  \n",
       "17        moocht       vertrouwen wij sijn noch altesamen  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d938dc6860dc4efa93d38537c0011eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Sla uw resultaten op:'), Text(value='vast.csv'), Button(button_style='warning', de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Werkwoorden voor <b>zeker</b> niet in <b>vast</b>: Schrijven, gemeld, melde, melden,, voort, kunne, weet,, schrijuen,"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Werkwoorden voor <b>vast</b> niet in <b>zeker</b>: belooft, was, hoope,, gedagt, vertrouden, ben, gelooven, beloofd,, schryve, moocht, schryven, te, verlengen,, afgedak, gebout, Melden, voorgenoomen"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Werkwoorden zowel voor <b>zeker</b> als voor <b>vast</b>: sal"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from chaininglib import search\n",
    "from IPython.core.display import display, HTML\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.ui.dfui import display_df\n",
    "from chaininglib.utils.dfops import column_difference\n",
    "\n",
    "corpus_name = \"zeebrieven\"\n",
    "\n",
    "# Word 1: puur\n",
    "word1= \"zeker\"\n",
    "cq1 = create_corpus(corpus_name).pattern(r'[lemma=\"' + word1 + r'\"][pos=\"VRB.*\"]')\n",
    "df_corpus1 = cq1.search().kwic()\n",
    "display_df(df_corpus1, word1)\n",
    "\n",
    "# Word 2: zuiver\n",
    "word2 = \"vast\"\n",
    "cq2 = create_corpus(corpus_name).pattern(r'[lemma=\"' + word2 + r'\"][pos=\"VRB.*\"]')\n",
    "df_corpus2 = cq2.search().kwic()\n",
    "display_df(df_corpus2, word2)\n",
    "\n",
    "# Compute difference\n",
    "diff_left, diff_right, intersec = column_difference(df_corpus1[\"word 1\"], df_corpus2[\"word 1\"])\n",
    "# Elements of 1 that are not in 2\n",
    "display(HTML('Werkwoorden voor <b>' + word1 + '</b> niet in <b>' + word2 + '</b>: ' + \", \".join(diff_left)))\n",
    "# Elements of 2 that are not in 1\n",
    "display(HTML('Werkwoorden voor <b>' + word2 + '</b> niet in <b>' + word1 + '</b>: ' + \", \".join(diff_right)))\n",
    "# Elements both in 1 and 2\n",
    "display(HTML('Werkwoorden zowel voor <b>' + word1 + '</b> als voor <b>' + word2 + '</b>: ' + \", \".join(intersec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a POS tagger on an annotated corpus <a class=\"anchor\" id=\"pos-tagger\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:33:21.728366Z",
     "start_time": "2019-03-06T16:33:20.911111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querying zeebrieven...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'some_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9c151c966982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mone_corpus\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'zeebrieven'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gysseling'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'querying '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mone_corpus\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msome_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetailed_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mdf_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'some_word' is not defined"
     ]
    }
   ],
   "source": [
    "from chaininglib.ui.dfui import display_df\n",
    "from chaininglib.process.corpus import get_tagger\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.search.LexiconQuery import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# gather some pattern including our word, out of annotated corpora\n",
    "\n",
    "dfs_all_corpora = pd.DataFrame()\n",
    "\n",
    "for one_corpus in ['zeebrieven', 'gysseling']:\n",
    "    print('querying '+one_corpus+'...')\n",
    "    c = create_corpus(one_corpus).word(some_word).detailed_context(True).search()\n",
    "    df_corpus = c.kwic() \n",
    "    \n",
    "    # store the results\n",
    "    dfs_all_corpora = pd.concat( [dfs_all_corpora, df_corpus] )\n",
    "\n",
    "\n",
    "# get a tagger trained with our corpus data\n",
    "tagger = get_tagger(dfs_all_corpora, pos_key = 'pos') \n",
    "\n",
    "# Use the trained tagger to tag unknown sentences\n",
    "# The input must be like: tagger.tag(['today','is','a','beautiful','day'])\n",
    "\n",
    "sentence = 'Het is mooi weer, dus gaan we in het bos lopen'\n",
    "tagged_sentence = tagger.tag( sentence.split() )\n",
    "\n",
    "print(tagged_sentence)\n",
    "\n",
    "\n",
    "# Know we can lemmatize each occurence of our lemma in the new sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search in corpus and filter on metadata <a class=\"anchor\" id=\"corpus-filter-metadata\"></a>\n",
    "First, we request all available metadata fields of the corpus. Then, we issue a search query, and request all metadata fields for the result. Finally, we filter on metadata values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:33:21.734400Z",
     "start_time": "2019-03-06T16:33:19.568Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.metadata import get_available_metadata\n",
    "from chaininglib.utils.dfops import df_filter, property_freq\n",
    "from chaininglib.ui.dfui import display_df\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "\n",
    "\n",
    "corpus_name=\"zeebrieven\"\n",
    "query=r'[lemma=\"boek\"]'\n",
    "# Request all metadata fields from corpus\n",
    "fields = get_available_metadata(corpus_name)\n",
    "# Perform query and ask all metadata\n",
    "c = create_corpus(corpus_name).pattern(query).extra_fields_doc(fields[\"document\"]).search()\n",
    "df_corpus = c.kwic()\n",
    "\n",
    "# Filter on year: > 1700\n",
    "df_filter_year = df_corpus[df_corpus[\"witnessYear_from\"].astype('int32') > 1700] \n",
    "display_df(df_filter_year, labels=\"After 1700\")\n",
    "\n",
    "# Filter on sender birth place Amsterdam\n",
    "condition = df_filter(df_corpus[\"afz_geb_plaats\"], pattern=\"Amsterdam\")\n",
    "df_filter_place = df_corpus[ condition ]\n",
    "display_df(df_filter_place, labels=\"Sender born in Amsterdam\")\n",
    "\n",
    "\n",
    "# Group by birth place\n",
    "df = property_freq(df_corpus,\"afz_loc_plaats\")\n",
    "display_df(df, labels=\"Most frequent sender locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing h-dropping  <a class=\"anchor\" id=\"visualizing-h-dropping\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:33:21.735977Z",
     "start_time": "2019-03-06T16:33:19.573Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.search.metadata import get_available_metadata\n",
    "from chaininglib.ui.dfui import display_df\n",
    " \n",
    "corpus_to_search=\"zeebrieven\"\n",
    "group_by_column = 'afz_geb_plaats'\n",
    "\n",
    "fields = get_available_metadata(corpus_to_search)\n",
    "\n",
    "df_corpus1 = create_corpus(corpus_to_search).pattern(r'[lemma=\"h[aeo].*\" & word=\"h[aeo].*\"]').extra_fields_doc(fields[\"document\"]).search().kwic()\n",
    "df_corpus2 = create_corpus(corpus_to_search).pattern(r'[lemma=\"h[aeo].*\" & word=\"[aeo].*\"]').extra_fields_doc(fields[\"document\"]).search().kwic()\n",
    "\n",
    "print('Draw charts showing geographic differences between normal language and h-dropping')\n",
    "\n",
    "display_df( df_corpus1[['lemma 0', group_by_column]].groupby(group_by_column).count().sort_values(ascending=False,by=['lemma 0']).head(25), labels=\"normal\", mode='chart') \n",
    "display_df( df_corpus2[['lemma 0', group_by_column]].groupby(group_by_column).count().sort_values(ascending=False,by=['lemma 0']).head(25), labels=\"h-dropping\", mode='chart')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate lexicon from several corpora <a class=\"anchor\" id=\"lexicon-several-corpora\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:33:21.745722Z",
     "start_time": "2019-03-06T16:33:19.598Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.ui.dfui import display_df\n",
    "from chaininglib.process.corpus import extract_lexicon\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "\n",
    "dfs_all_corpora = []\n",
    "for one_corpus in get_available_corpora(exclude=[\"nederlab\"]):\n",
    "    print('querying '+one_corpus+'...')\n",
    "    c = create_corpus(one_corpus).pos(\"NOU\").detailed_context(True).search()\n",
    "    df_corpus = c.kwic() \n",
    "    # store the results\n",
    "    dfs_all_corpora.append(df_corpus)\n",
    "\n",
    "    \n",
    "# extract lexicon and show the result\n",
    "extracted_lexicon = extract_lexicon(dfs_all_corpora, posColumnName=\"pos\") # For FCS: posColumnName=universal_dependency\n",
    "display(extracted_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon search <a class=\"anchor\" id=\"lexicon-search\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the cell below to show the UI, and fill in your search query in the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:33:21.737060Z",
     "start_time": "2019-03-06T16:33:19.576Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5df084c69af4dda9512e7d2f591a3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='boek', description='<b>Word:</b>'), Dropdown(description='<b>Lexicon:</b>', index=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from chaininglib.ui.search import create_lexicon_ui\n",
    "\n",
    "#from chaininglib import ui\n",
    "searchWordField, lexiconField = create_lexicon_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Click the cell below and press Run to perform the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:33:21.737959Z",
     "start_time": "2019-03-06T16:33:19.578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[F                                                                    "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_word</th>\n",
       "      <th>query_pos</th>\n",
       "      <th>wordform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boek</td>\n",
       "      <td></td>\n",
       "      <td>boeck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boek</td>\n",
       "      <td></td>\n",
       "      <td>boecke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boek</td>\n",
       "      <td></td>\n",
       "      <td>boecken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boek</td>\n",
       "      <td></td>\n",
       "      <td>boecxkens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boek</td>\n",
       "      <td></td>\n",
       "      <td>boek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>boek</td>\n",
       "      <td></td>\n",
       "      <td>boeken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>boek</td>\n",
       "      <td></td>\n",
       "      <td>boekske</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>boek</td>\n",
       "      <td></td>\n",
       "      <td>boeksken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>boek</td>\n",
       "      <td></td>\n",
       "      <td>boekxken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>boek</td>\n",
       "      <td></td>\n",
       "      <td>boexke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>boek</td>\n",
       "      <td></td>\n",
       "      <td>boexken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>boek</td>\n",
       "      <td></td>\n",
       "      <td>boick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>boek</td>\n",
       "      <td></td>\n",
       "      <td>bouck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>boek</td>\n",
       "      <td></td>\n",
       "      <td>boucken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>boek</td>\n",
       "      <td></td>\n",
       "      <td>bouken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_word query_pos   wordform\n",
       "0        boek                boeck\n",
       "1        boek               boecke\n",
       "2        boek              boecken\n",
       "3        boek            boecxkens\n",
       "4        boek                 boek\n",
       "5        boek               boeken\n",
       "6        boek              boekske\n",
       "7        boek             boeksken\n",
       "8        boek             boekxken\n",
       "9        boek               boexke\n",
       "10       boek              boexken\n",
       "11       boek                boick\n",
       "12       boek                bouck\n",
       "13       boek              boucken\n",
       "14       boek               bouken"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615e1106870647bd9bfaff04b4b0d55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Sla uw resultaten op:'), Text(value='mijn_resultaten.csv'), Button(button_style='w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from chaininglib.search.LexiconQuery import *\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "search_word = searchWordField.value\n",
    "lexicon_name = lexiconField.value\n",
    "# USER: can replace this by own custom query\n",
    "lex = create_lexicon(lexicon_name).lemma(search_word).search()\n",
    "df_lexicon = lex.kwic()\n",
    "display_df(df_lexicon)\n",
    "#df_columns_list = list(df_lexicon.columns.values)\n",
    "#df_lexicon_in_columns = df_lexicon[df_columns_list]\n",
    "#display(df_lexicon_in_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus + lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve synonyms from DiaMaNT, look up in Gysseling <a class=\"anchor\" id=\"synonyms-diamant-gysseling\"></a>\n",
    "* Below cell searches for term \"boek\" in DiaMaNT, and looks up all variants in Gysseling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:33:21.739111Z",
     "start_time": "2019-03-06T16:33:19.580Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.search.LexiconQuery import *\n",
    "from IPython.core.display import display, HTML\n",
    "from chaininglib.search.corpusQueries import corpus_query\n",
    "from chaininglib.process.lexicon import get_diamant_synonyms\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "search_word = \"boek\"\n",
    "lexicon_name = \"diamant\"\n",
    "corpus= \"gysseling\"\n",
    "\n",
    "# First, lookup synonyms in DiaMaNT\n",
    "lq = create_lexicon(lexicon_name).lemma(search_word).search()\n",
    "df_lexicon = lq.kwic()\n",
    "\n",
    "syns = get_diamant_synonyms(df_lexicon)\n",
    "syns.add(search_word) # Also add search word itself\n",
    "display(HTML('Synoniemen voor <b>' + search_word + '</b>: ' + \", \".join(syns)))\n",
    "\n",
    "# Search for all synonyms in corpus\n",
    "## Create queries: search by lemma\n",
    "syns_queries = [corpus_query(lemma=syn) for syn in syns]\n",
    "\n",
    "## Search for all synonyms in corpus\n",
    "df = pd.DataFrame()\n",
    "for one_pattern in syns_queries:\n",
    "    cq = create_corpus(corpus).pattern(one_pattern).search()\n",
    "    df = df.append(cq.kwic())\n",
    "display_df(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Build a frequency list of the lemma of some corpus output <a class=\"anchor\" id=\"freq-lemma-corpus\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:33:21.740311Z",
     "start_time": "2019-03-06T16:33:19.583Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.process.corpus import *\n",
    "from chaininglib.ui.dfui import *\n",
    "\n",
    "# do some corpus search\n",
    "print('This can take a few seconds... please wait!')\n",
    "\n",
    "corpus_to_search=\"zeebrieven\"\n",
    "df_corpus = create_corpus(corpus_to_search).detailed_context(True).pos(\"NOU.*\").search().kwic()\n",
    "\n",
    "# compute and display a table of the frequencies of the lemmata\n",
    "\n",
    "freq_df = get_frequency_list(df_corpus)\n",
    "display_df(freq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T15:46:20.519833Z",
     "start_time": "2019-01-16T15:46:20.516208Z"
    }
   },
   "source": [
    "### Find occurences of attributive adjectives not ending with -e, even though they are preceeded by a definite article <a class=\"anchor\" id=\"adjective-e\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:45:26.276294Z",
     "start_time": "2019-03-08T15:44:57.804270Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.search.LexiconQuery import *\n",
    "from chaininglib.utils.dfops import df_filter\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "corpus_to_search=\"chn-extern\"\n",
    "lexicon_to_search=\"molex\"\n",
    "\n",
    "# CORPUS: get [article + attributive adjective + nouns] combinations in which the adjective does not end with -e\n",
    "print('Get occurences of attributive adjectives not ending with -e')\n",
    "cq = create_corpus(corpus_to_search).pattern(r'[lemma=\"de|het\"][word=\"^g(.+)[^e]$\" & pos=\"AA.*\"][pos=\"NOU.*\"]')\n",
    "df_corpus = cq.search().kwic()\n",
    "display(df_corpus)\n",
    "\n",
    "# LEXICON: get adjectives the lemma of which does not end with -e\n",
    "lq = create_lexicon(lexicon_to_search).lemma('^g(.+)[^e]$').pos('ADJ').search()\n",
    "df_lexicon = lq.search().kwic()\n",
    "\n",
    "# LEXICON: get adjectives having a final -e in definite attributive use\n",
    "print('Filtering lexicon results')\n",
    "final_e_condition = df_filter(df_lexicon[\"wordform\"], 'e$')\n",
    "df_lexicon_form_e = df_lexicon[ final_e_condition ]\n",
    "\n",
    "# RESULT: get the records out of our first list in which the -e-less-adjectives match the lemma form of our last list\n",
    "print('List of attributive adjectives not ending with -e even though they should have a final -e:')\n",
    "e_forms = list(df_lexicon_form_e.lemma)\n",
    "no_final_e_condition = df_filter(df_corpus[\"word 1\"], pattern=set(e_forms), method=\"isin\")\n",
    "result_df = df_corpus[ no_final_e_condition ]\n",
    "display_df( result_df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look up inflected forms and spelling variants for a given lemma in a corpus <a class=\"anchor\" id=\"inflected-spelling-corpus\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T11:22:41.350098Z",
     "start_time": "2019-03-08T11:22:40.316249Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.ui.dfui import display_df\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.search.LexiconQuery import *\n",
    "\n",
    "# Corpus Gysseling and lexicon mnwlex are from same period: 1250-1550\n",
    "lexicon_to_search=\"mnwlex\"\n",
    "corpus_to_search=\"gysseling\"\n",
    "\n",
    "##############################################\n",
    "# TODO  zelfde met meerdere lemmata en gegroepeerd \n",
    "##############################################\n",
    "\n",
    "lemma_to_look_for=\"denken\"\n",
    "\n",
    "# LEXICON: Search for the inflected forms of a lemma in a morphosyntactic lexicon\n",
    "lq = create_lexicon(lexicon_to_search).lemma(lemma_to_look_for).search()\n",
    "df_lexicon = lq.kwic()\n",
    "display_df(df_lexicon)\n",
    "\n",
    "# Put all inflected forms into a list\n",
    "inflected_wordforms = list(df_lexicon.wordform)\n",
    "\n",
    "# CORPUS: Look up the inflected forms in a (possibly unannotated) corpus\n",
    "# beware: If the corpus is not annotated, all we can do is searching for the inflected words\n",
    "#         But if the corpus is lemmatized, we have to make sure we're retrieving correct data by specifying the lemma as well\n",
    "annotated_corpus = True\n",
    "query = r'[lemma=\"'+lemma_to_look_for+r'\" & word=\"'+r\"|\".join(inflected_wordforms)+r'\"]' if annotated_corpus else r'[word=\"'+r\"|\".join(inflected_wordforms)+r'\"]'\n",
    "cq = create_corpus(corpus_to_search).pattern(query).search()\n",
    "df_corpus = cq.kwic() \n",
    "display_df(df_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus frequency list of lemmata from lexicon with given lemma <a class=\"anchor\" id=\"corpus-frequency-lemma-pos\"></a>\n",
    "Build a function with which we can gather all lemmata of a lexicon, and build a frequency list of those lemmata in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T16:41:28.120587Z",
     "start_time": "2019-03-08T16:41:10.730546Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.LexiconQuery import *\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.process.corpus import get_frequency_list\n",
    "from chaininglib.ui.dfui import display_df\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# build a function as required. We will run it afterwards\n",
    "\n",
    "def get_frequency_list_given_a_corpus(lexicon, pos, corpus):\n",
    "    \n",
    "    # LEXICON: get a lemmata list to work with\n",
    "\n",
    "    # query the lexicon for lemma with a given part-of-speech\n",
    "    lq = create_lexicon(lexicon).pos(pos).search()\n",
    "    df_lexicon = lq.kwic()\n",
    "\n",
    "    # Put the results into an array, so we can loop through the found lemmata\n",
    "    lexicon_lemmata_arr = [w.lower() for w in df_lexicon[\"writtenForm\"]][-200:]\n",
    "    # Instantiate a DataFrame, in which we will gather all single lemmata occurences\n",
    "    df_full_list = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # CORPUS: loop through the lemmata list, query the corpus with each lemma, and count the results\n",
    "\n",
    "    # It's a good idea to query more than one lemma at at the time,\n",
    "    # but not too many, otherwise the server will get overloaded!\n",
    "    nr_of_lemmata_to_query_atonce = 100\n",
    "\n",
    "    # loop over lemma list \n",
    "    for i in range(0, len(lexicon_lemmata_arr), nr_of_lemmata_to_query_atonce):\n",
    "        \n",
    "        print('Lemmata processed: '+str(i)+'/'+str(len(lexicon_lemmata_arr)))\n",
    "        \n",
    "        # slice to small array of lemmata to query at once\n",
    "        small_lemmata_arr = lexicon_lemmata_arr[i : i+nr_of_lemmata_to_query_atonce] \n",
    "\n",
    "        # join set of lemmas to send them in a query all at once\n",
    "        # beware: single quotes need escaping\n",
    "        lemmata_list = \"|\".join(small_lemmata_arr).replace(\"'\", \"\\\\\\\\'\")\n",
    "        cq = create_corpus(corpus).pattern(r'[lemma=\"' + lemmata_list + r'\"]').search()\n",
    "        df_corpus = cq.kwic()\n",
    "        \n",
    "        # add the results to the full list\n",
    "        if \"lemma 0\" in df_corpus.columns:\n",
    "            df_full_list = pd.concat( [df_full_list, df_corpus[\"lemma 0\"]] )     \n",
    "        \n",
    "\n",
    "    # make sure the columnswith that contains the lemmata is same as given to get_frequency_list function\n",
    "    column_name=\"lemma\"\n",
    "    df_full_list.columns = [column_name]\n",
    "\n",
    "    # we're done with querying, build the frequency list now\n",
    "    print('Done.')\n",
    "    freq_df = get_frequency_list(df_full_list, column_name=column_name)\n",
    "    \n",
    "\n",
    "    return freq_df\n",
    "\n",
    "    \n",
    "# run it!\n",
    "lexicon=\"molex\"\n",
    "# TODO: Maybe too much too ask all nouns? Maybe take random sample?\n",
    "corpus_to_search=\"chn-extern\"\n",
    "pos=\"NOU.*\"\n",
    "\n",
    "freq_df = get_frequency_list_given_a_corpus(lexicon, pos, corpus_to_search)\n",
    "\n",
    "display_df(freq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a frequency table of some corpus, based on lemma list of a given lexicon <a class=\"anchor\" id=\"freqtable-lemmalist\"></a>\n",
    "In this case study, we compare lemma frequencies for corpora from different time periods: CHN extern (contemporary Dutch Antilles & Suriname) and Letters as Loot (sailors' letters, 17th and 18th century).\n",
    "\n",
    "*For this case study, you need to run the previous case study first, because it generates a function we need here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T15:56:59.673548Z",
     "start_time": "2019-03-08T15:56:38.978853Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.utils.dfops import get_rank_diff\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "# For this case study, you need to run the previous case study first, because it generates a function we need here\n",
    "\n",
    "# Use lexica and corpora from same period\n",
    "base_lexicon1=\"molex\"\n",
    "corpus_to_search1=\"chn-extern\"\n",
    "\n",
    "base_lexicon2=\"molex\"\n",
    "corpus_to_search2=\"zeebrieven\"\n",
    "\n",
    "# ADJ gives interesting comparison\n",
    "\n",
    "pos=\"ADJ.*\"\n",
    "\n",
    "# build frequency tables of two corpora\n",
    "\n",
    "df_frequency_list1 = get_frequency_list_given_a_corpus(base_lexicon1, pos, corpus_to_search1)\n",
    "# sort and display\n",
    "df_top25_descending = df_frequency_list1.sort_values(ascending=False,by=['token count']).head(25)\n",
    "df_top25_ascending =  df_frequency_list1.sort_values(ascending=True, by=['rank']).head(25)\n",
    "display_df( df_top25_descending[['lemmas', 'token count']].set_index('lemmas'), labels='df1 chart '+corpus_to_search1, mode='chart' )\n",
    "\n",
    "df_frequency_list2 = get_frequency_list_given_a_corpus(base_lexicon2, pos, corpus_to_search2)\n",
    "# sort and display\n",
    "df_top25_descending = df_frequency_list2.sort_values(ascending=False,by=['token count']).head(25)\n",
    "df_top25_ascending =  df_frequency_list2.sort_values(ascending=True, by=['rank']).head(25)\n",
    "display_df( df_top25_descending[['lemmas', 'token count']].set_index('lemmas'), labels='df2 chart '+corpus_to_search2, mode='chart' )\n",
    "\n",
    "\n",
    "# TODO: lemmata tonen die in 1 of 2 ontbreken\n",
    "\n",
    "# compute the rank diff of lemmata in frequency tables\n",
    "\n",
    "# sort and display\n",
    "df_rankdiffs = get_rank_diff(df_frequency_list1, df_frequency_list2, index='lemmas')\n",
    "\n",
    "display_df(df_rankdiffs.sort_values(by=['rank_diff']).head(25), labels='Differences in ranks')\n",
    "\n",
    "df_top25_descending = df_rankdiffs.sort_values(ascending=False, by=['rank_diff']).head(25)\n",
    "display_df( df_top25_descending['rank_diff'], labels='chart large diff', mode='chart' )\n",
    "\n",
    "df_top25_ascending = df_rankdiffs.sort_values(ascending=True, by=['rank_diff']).head(25)\n",
    "display_df( df_top25_ascending['rank_diff'], labels='chart small diff', mode='chart' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search corpus for wordforms of lemma not included in lexicon <a class=\"anchor\" id=\"corpus-wordforms-not-lexicon\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T17:18:03.988453Z",
     "start_time": "2019-03-08T17:08:43.962019Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.LexiconQuery import *\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "# Let's build a function to do the job:\n",
    "# The function will require a lexicon name and a part-of-speech to limit the search to, and the name of a corpus to be searched.\n",
    "# It will return a Pandas DataFrame associating lemmata to their paradigms ('known_wordforms' column) and\n",
    "# missing wordforms found in the corpus ('unknown_wordforms' column).\n",
    "\n",
    "def get_missing_wordforms(lexicon_name, lexicon_postag, corpus, corpus_postag):    \n",
    "    \n",
    "    print('Finding missing wordforms in a lexicon can take some time...');\n",
    "    \n",
    "    # LEXICON: \n",
    "    # get a lemmata list having a given part-of-speech\n",
    "    # MoLex is a convenient source to get a list of lemmata\n",
    "    \n",
    "    lq = create_lexicon(\"molex\").pos(lexicon_postag).search()\n",
    "    df_lexicon = lq.kwic()\n",
    "    \n",
    "    # Put the results into an array, so we can loop through the list of lemmata\n",
    "    lexicon_lemmata_arr = [w.lower() for w in df_lexicon[\"writtenForm\"]][-50:]\n",
    "    \n",
    "    # Test array, instead of querying Molex\n",
    "    #lexicon_lemmata_arr = [\"denken\", \"doen\", \"hebben\", \"maken\"]\n",
    "    \n",
    "    # Prepare the output:\n",
    "    # instantiate a DataFrame for storing lemmata and mssing wordforms\n",
    "    df_enriched_lexicon = pd.DataFrame(index=lexicon_lemmata_arr, columns=['lemma', 'pos', 'known_wordforms', 'unknown_wordforms'])\n",
    "    df_enriched_lexicon.index.name = 'lemmata'\n",
    "    \n",
    "    # CORPUS: \n",
    "    # loop through the lemmata list, query the corpus for each lemma, \n",
    "    # and compute paradigms differences between both\n",
    "\n",
    "    \n",
    "    # loop through the lemmata list\n",
    "    # and query the corpus for occurances of the lemmata\n",
    "    \n",
    "    # It's a good idea to work with more than one lemma at the time (speed)!\n",
    "    nr_of_lemmata_to_query_atonce = 100\n",
    "    \n",
    "    for i in range(0, len(lexicon_lemmata_arr), nr_of_lemmata_to_query_atonce):\n",
    "        \n",
    "        # slice to small array of lemmata to query at once\n",
    "        small_lemmata_arr = lexicon_lemmata_arr[i : i+nr_of_lemmata_to_query_atonce]\n",
    "        \n",
    "        # join set of lemmata to send them in a query all at once\n",
    "        # beware: single quotes need escaping\n",
    "        lemmata_list = \"|\".join(small_lemmata_arr).replace(\"'\", \"\\\\\\\\'\")\n",
    "        print(\"Querying lemmata %i-%i of %i from corpus.\" % (i, i+nr_of_lemmata_to_query_atonce, len(lexicon_lemmata_arr) ))\n",
    "        cq = create_corpus(corpus).pattern(r'[lemma=\"' + lemmata_list + r'\" & pos=\"'+corpus_postag+'\"]').search()\n",
    "        df_corpus = cq.kwic()\n",
    "        \n",
    "        # if the corpus gave results,\n",
    "        # query the lexicon for the same lemmata\n",
    "        # and compare the paradigms!\n",
    "        \n",
    "        if (len(df_corpus)>0):\n",
    "            small_lemmata_set = set(small_lemmata_arr)\n",
    "            for one_lemma in small_lemmata_set: \n",
    "                \n",
    "                # look up the known wordforms in the lexicon\n",
    "                ql = create_lexicon(lexicon_name).lemma(one_lemma).search()\n",
    "                df_known_wordforms = ql.kwic()\n",
    "                \n",
    "                # we have a lexicon paradigm to compare, do the job now\n",
    "                if (len(df_known_wordforms) != 0):\n",
    "                    \n",
    "                    # gather the lexicon wordforms in a set\n",
    "                    known_wordforms = set( df_known_wordforms['wordform'].str.lower() )\n",
    "                    \n",
    "                    # gather the corpus wordforms (of the same lemma) in a set too\n",
    "                    corpus_lemma_filter = (df_corpus['lemma 0'] == one_lemma)\n",
    "                    corpus_wordforms = set( (df_corpus[ corpus_lemma_filter ])['word 0'].str.lower() )\n",
    "                    \n",
    "                    # Now compute the differences:\n",
    "                    # gather in a set all the corpus wordforms that cannot be found in the lexicon wordforms \n",
    "                    unknown_wordforms = corpus_wordforms.difference(known_wordforms)\n",
    "\n",
    "                    # If we found some missing wordforms, add the results to the output!\n",
    "                    \n",
    "                    if (len(unknown_wordforms) !=0):                        \n",
    "                        # The index of our results will be a key consisting of lemma + part-of-speech\n",
    "                        # Part-of-speech is needed to distinguish homonyms with different grammatical categories.\n",
    "                        # Of course, we need to take glosses into account too to do a truely correct job\n",
    "                        # But we didn't do it here\n",
    "                        key = one_lemma + lexicon_postag\n",
    "                        df_enriched_lexicon.at[key, 'lemma'] = one_lemma\n",
    "                        df_enriched_lexicon.at[key, 'pos'] = lexicon_postag\n",
    "                        df_enriched_lexicon.at[key, 'known_wordforms'] = known_wordforms\n",
    "                        df_enriched_lexicon.at[key, 'unknown_wordforms'] = unknown_wordforms\n",
    "                \n",
    "    # return non-empty results, t.i. cases in which we found some wordforms\n",
    "    return df_enriched_lexicon[ df_enriched_lexicon['unknown_wordforms'].notnull() ]\n",
    "\n",
    "\n",
    "# Run the function!\n",
    "#\n",
    "# ask the lexicon which wordforms it knows, and try to find new unknown wordforms in the corpus\n",
    "\n",
    "lexicon_name=\"mnwlex\"\n",
    "corpus_to_search=\"zeebrieven\"\n",
    "\n",
    "# beware: lexicon and corpus may have different parts-of-speech sets in use\n",
    "df = get_missing_wordforms(lexicon_name, \"VERB\", corpus_to_search, \"VRB\")\n",
    "\n",
    "# After such a heavy process, it's a good idea to save the results\n",
    "\n",
    "df.to_csv( \"missing_wordforms.csv\", index=False)\n",
    "\n",
    "display_df(df, labels='Missing wordforms')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treebanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treebank search <a class=\"anchor\" id=\"treebank-search\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:33:21.746920Z",
     "start_time": "2019-03-06T16:33:19.600Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.TreebankQuery import *\n",
    "\n",
    "\n",
    "print (\"search...\")\n",
    "\n",
    "tbq = create_treebank().pattern(\"xquery //node[@cat='pp' and node[@cat='ap' and node[@cat='np']]]\").search()\n",
    "\n",
    "print (\"get XML...\")\n",
    "\n",
    "xml = tbq.xml()\n",
    "print(xml)\n",
    "\n",
    "print (\"get trees and their string representations...\")\n",
    "\n",
    "trees = tbq.trees()\n",
    "\n",
    "for tree in trees:\n",
    "    display(tree.toString())\n",
    "\n",
    "df = tbq.kwic()\n",
    "    \n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which kind of nouns are used in a prepositional complement of the verb *geven* ? <a class=\"anchor\" id=\"treebank-objects-geven\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T16:33:21.748403Z",
     "start_time": "2019-03-06T16:33:19.603Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.TreebankQuery import *\n",
    "\n",
    "\n",
    "print (\"search...\")\n",
    "\n",
    "tbq = create_treebank().pattern(r'xquery //node[node[@rel=\"hd\" and @pt=\"ww\" and @root=\"geven\"] and node[@rel=\"obj1\" and @pt=\"n\"]]').search()\n",
    "\n",
    "\n",
    "print (\"get list of nouns which are part of an PP, as argument of predicate 'geven'...\")\n",
    "\n",
    "trees = tbq.trees()\n",
    "\n",
    "list_of_nouns = []\n",
    "for tree in trees:\n",
    "    nouns = tree.extract(['pp', 'np'])\n",
    "    list_of_nouns = list_of_nouns + nouns\n",
    "    \n",
    "\n",
    "display(list_of_nouns)\n",
    "    \n",
    "df = tbq.kwic(align_lemma='geven')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
