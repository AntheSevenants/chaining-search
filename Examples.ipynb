{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples Chaining search\n",
    "This notebook contains a number of examples of chaining linguistic resources: corpora, lexica and treebanks. Try the examples, or copy the code and customize the examples in the [Sandbox](Sandbox.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of examples\n",
    "### Corpora\n",
    " * [Corpus search](#corpus-search)\n",
    " * [Frequency of *zeker*+verb and *vast*+verb compared](#freq-puur-zuiver)\n",
    " * [Train a POS tagger on an annotated corpus](#pos-tagger)\n",
    " * [Search in corpus and filter on metadata](#corpus-filter-metadata)\n",
    " * [Visualizing h-dropping](#visualizing-h-dropping)\n",
    " * [Generate lexicon from several corpora](#lexicon-several-corpora)\n",
    "\n",
    "### Lexica\n",
    " * [Lexicon search](#lexicon-search)\n",
    "\n",
    "### Corpus + lexicon\n",
    " * [Retrieve synonyms from DiaMaNT, look up in Gysseling](#synonyms-diamant-gysseling)\n",
    " * [Build a frequency list of the lemma of some corpus output](#freq-lemma-corpus)\n",
    " * [Find occurences of attributive adjectives not ending with -e, even though they are preceeded by a definite article](#adjective-e)\n",
    " * [Look up inflected forms and spelling variants for a given lemma in a corpus](#inflected-spelling-corpus)\n",
    " * [Corpus frequency list of lemmata from lexicon with given lemma](#corpus-frequency-lemma-pos)\n",
    " * [Build a frequency table of some corpus, based on lemmata of a given lexicon](#freqtable-lemmalist)\n",
    " * [Search corpus for wordforms of lemma not included in lexicon](#corpus-wordforms-not-lexicon)\n",
    " \n",
    "### Treebanks\n",
    " * [Treebank search](#treebank-search)\n",
    " * [Which objects of verb *geven* occur?](#treebank-objects-geven)\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-04T10:51:07.839335Z",
     "start_time": "2019-03-04T10:51:07.835114Z"
    }
   },
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus search <a class=\"anchor\" id=\"corpus-search\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the cell below to show the UI, and fill in your search query\n",
    "* Choose one of the corpora:\n",
    "  * **zeebrieven**: The Brieven als Buit (Letters as Loot) corpus, consisting of 17th and 18th century letters from Dutch sailors\n",
    "  * **gysseling**: Corpus Gysseling, 13th century Dutch\n",
    "  * **chn-extern**: Externally accessible part of the Corpus Hedendaags Nederlands. Corpus of contemporary Dutch from the Dutch Antilles and Suriname, retrieved from newspapers and websites.\n",
    "  * **opus**: OPUS corpus of Dutch subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T10:23:23.333806Z",
     "start_time": "2019-03-21T10:23:22.741162Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.ui.search import create_corpus_ui\n",
    "from chaininglib.ui.dfui import display_df, get_uploader\n",
    "\n",
    "# Create corpus UI, creates references to field contents\n",
    "corpusQueryField, corpusField = create_corpus_ui()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Click the cell below and press Run to perform the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T10:32:08.854468Z",
     "start_time": "2019-03-21T10:32:07.837675Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.CorpusQuery import *\n",
    "\n",
    "#from chaininglib import search\n",
    "query= corpusQueryField.value\n",
    "corpus_name = corpusField.value\n",
    "df_corpus = create_corpus(corpus_name).pattern(query).search().kwic()\n",
    "#df_corpus = load_dataframe('mijn_resultaten.csv')\n",
    "display_df(df_corpus, labels=\"Results\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of *zeker*+verb and *vast*+verb compared <a class=\"anchor\" id=\"freq-puur-zuiver\"></a>\n",
    "* Below cell searches for *zeker*+verb and for *vast*+verb in the Letters as Loot (zeebrieven) corpus\n",
    "* Compare frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T10:50:19.982075Z",
     "start_time": "2019-03-21T10:50:19.176456Z"
    }
   },
   "outputs": [],
   "source": [
    "#from chaininglib import search\n",
    "from IPython.core.display import display, HTML\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.ui.dfui import display_df\n",
    "from chaininglib.utils.dfops import column_difference\n",
    "\n",
    "corpus_name = \"zeebrieven\"\n",
    "\n",
    "# Word 1: puur\n",
    "word1= \"zeker\"\n",
    "cq1 = create_corpus(corpus_name).pattern(r'[lemma=\"' + word1 + r'\"][pos=\"VRB.*\"]')\n",
    "df_corpus1 = cq1.search().kwic()\n",
    "display_df(df_corpus1, word1)\n",
    "\n",
    "# Word 2: zuiver\n",
    "word2 = \"vast\"\n",
    "cq2 = create_corpus(corpus_name).pattern(r'[lemma=\"' + word2 + r'\"][pos=\"VRB.*\"]')\n",
    "df_corpus2 = cq2.search().kwic()\n",
    "display_df(df_corpus2, word2)\n",
    "\n",
    "# Compute difference\n",
    "diff_left, diff_right, intersec = column_difference(df_corpus1[\"word 1\"], df_corpus2[\"word 1\"])\n",
    "# Elements of 1 that are not in 2\n",
    "display(HTML('Werkwoorden voor <b>' + word1 + '</b> niet in <b>' + word2 + '</b>: ' + \", \".join(diff_left)))\n",
    "# Elements of 2 that are not in 1\n",
    "display(HTML('Werkwoorden voor <b>' + word2 + '</b> niet in <b>' + word1 + '</b>: ' + \", \".join(diff_right)))\n",
    "# Elements both in 1 and 2\n",
    "display(HTML('Werkwoorden zowel voor <b>' + word1 + '</b> als voor <b>' + word2 + '</b>: ' + \", \".join(intersec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a POS tagger on an annotated corpus <a class=\"anchor\" id=\"pos-tagger\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T10:31:40.134049Z",
     "start_time": "2019-03-21T10:31:38.212645Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.ui.dfui import display_df\n",
    "from chaininglib.process.corpus import get_tagger\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.search.LexiconQuery import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# gather some pattern including our word, out of annotated corpora\n",
    "\n",
    "dfs_all_corpora = pd.DataFrame()\n",
    "some_lemma = \"lopen\"\n",
    "\n",
    "for one_corpus in ['zeebrieven', 'gysseling']:\n",
    "    print('querying '+one_corpus+'...')\n",
    "    c = create_corpus(one_corpus).lemma(some_lemma).detailed_context(True).search()\n",
    "    df_corpus = c.kwic() \n",
    "    \n",
    "    # store the results\n",
    "    dfs_all_corpora = pd.concat( [dfs_all_corpora, df_corpus] )\n",
    "\n",
    "\n",
    "# get a tagger trained with our corpus data\n",
    "tagger = get_tagger(dfs_all_corpora, pos_key = 'pos') \n",
    "\n",
    "# Use the trained tagger to tag unknown sentences\n",
    "# The input must be like: tagger.tag(['today','is','a','beautiful','day'])\n",
    "\n",
    "sentence = 'Het is mooi weer, dus gaan we in het bos lopen'\n",
    "tagged_sentence = tagger.tag( sentence.split() )\n",
    "\n",
    "print(tagged_sentence)\n",
    "\n",
    "\n",
    "# Know we can lemmatize each occurence of our lemma in the new sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search in corpus and filter on metadata <a class=\"anchor\" id=\"corpus-filter-metadata\"></a>\n",
    "First, we request all available metadata fields of the corpus. Then, we issue a search query, and request all metadata fields for the result. Finally, we filter on metadata values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T10:50:27.492793Z",
     "start_time": "2019-03-21T10:50:26.879452Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.metadata import get_available_metadata\n",
    "from chaininglib.utils.dfops import df_filter, property_freq\n",
    "from chaininglib.ui.dfui import display_df\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "\n",
    "\n",
    "corpus_name=\"zeebrieven\"\n",
    "query=r'[lemma=\"boek\"]'\n",
    "# Request all metadata fields from corpus\n",
    "fields = get_available_metadata(corpus_name)\n",
    "# Perform query and ask all metadata\n",
    "c = create_corpus(corpus_name).pattern(query).extra_fields_doc(fields[\"document\"]).search()\n",
    "df_corpus = c.kwic()\n",
    "\n",
    "# Filter on year: > 1700\n",
    "df_filter_year = df_corpus[df_corpus[\"witnessYear_from\"].astype('int32') > 1700] \n",
    "display_df(df_filter_year, labels=\"After 1700\")\n",
    "\n",
    "# Filter on sender birth place Amsterdam\n",
    "condition = df_filter(df_corpus[\"afz_geb_plaats\"], pattern=\"Amsterdam\")\n",
    "df_filter_place = df_corpus[ condition ]\n",
    "display_df(df_filter_place, labels=\"Sender born in Amsterdam\")\n",
    "\n",
    "\n",
    "# Group by birth place\n",
    "df = property_freq(df_corpus,\"afz_loc_plaats\")\n",
    "display_df(df, labels=\"Most frequent sender locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing h-dropping  <a class=\"anchor\" id=\"visualizing-h-dropping\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T10:50:57.910848Z",
     "start_time": "2019-03-21T10:50:31.246859Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.search.metadata import get_available_metadata\n",
    "from chaininglib.ui.dfui import display_df\n",
    " \n",
    "corpus_to_search=\"zeebrieven\"\n",
    "group_by_column = 'afz_geb_plaats'\n",
    "\n",
    "fields = get_available_metadata(corpus_to_search)\n",
    "\n",
    "df_corpus1 = create_corpus(corpus_to_search).pattern(r'[lemma=\"h[aeo].*\" & word=\"h[aeo].*\"]').extra_fields_doc(fields[\"document\"]).search().kwic()\n",
    "df_corpus2 = create_corpus(corpus_to_search).pattern(r'[lemma=\"h[aeo].*\" & word=\"[aeo].*\"]').extra_fields_doc(fields[\"document\"]).search().kwic()\n",
    "\n",
    "print('Draw charts showing geographic differences between normal language and h-dropping')\n",
    "\n",
    "display_df( df_corpus1[['lemma 0', group_by_column]].groupby(group_by_column).count().sort_values(ascending=False,by=['lemma 0']).head(25), labels=\"normal\", mode='chart') \n",
    "display_df( df_corpus2[['lemma 0', group_by_column]].groupby(group_by_column).count().sort_values(ascending=False,by=['lemma 0']).head(25), labels=\"h-dropping\", mode='chart')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate lexicon from several corpora <a class=\"anchor\" id=\"lexicon-several-corpora\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T11:18:40.973343Z",
     "start_time": "2019-03-21T11:17:44.001751Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.ui.dfui import display_df\n",
    "from chaininglib.process.corpus import extract_lexicon\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "\n",
    "dfs_all_corpora = []\n",
    "for one_corpus in get_available_corpora(exclude=[\"nederlab\"]):\n",
    "    print('querying '+one_corpus+'...')\n",
    "    c = create_corpus(one_corpus).pos(\"NOU\").detailed_context(True).search()\n",
    "    df_corpus = c.kwic() \n",
    "    # store the results\n",
    "    dfs_all_corpora.append(df_corpus)\n",
    "\n",
    "    \n",
    "# extract lexicon and show the result\n",
    "extracted_lexicon = extract_lexicon(dfs_all_corpora, posColumnName=\"pos\") # For FCS: posColumnName=universal_dependency\n",
    "display(extracted_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon search <a class=\"anchor\" id=\"lexicon-search\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the cell below to show the UI, and fill in your search query in the UI\n",
    "* Choose one of the lexica:\n",
    "  * **anw**: Algemeen Nederlands Woordenboek, dictionary of contemporary Dutch\n",
    "  * **celex**: Lexical database of the Dutch language\n",
    "  * **diamant**: DiaMaNT, diachronous semantic lexicon of the Dutch language. Contains synonym relations extracted from all Dutch historical dictionaries.\n",
    "  * **duelme**\n",
    "  * **molex**: A lexicon of modern Dutch, containing spelling variants per lemma\n",
    "  * **mnwlex**: Lexicon service access to Middelnederlands Woordenboek, a lexicon of Middle Dutch (ca. 1250 - 1550)\n",
    "  * **lexicon_service_db**: Lexicon service access to a lexicon of early modern and modern Dutch (1500 - 1976)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T11:07:27.446045Z",
     "start_time": "2019-03-21T11:07:27.428984Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from chaininglib.ui.search import create_lexicon_ui\n",
    "\n",
    "#from chaininglib import ui\n",
    "searchWordField, lexiconField = create_lexicon_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Click the cell below and press Run to perform the given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T11:17:44.000298Z",
     "start_time": "2019-03-21T11:17:43.638576Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.LexiconQuery import *\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "search_word = searchWordField.value\n",
    "lexicon_name = lexiconField.value\n",
    "# USER: can replace this by own custom query\n",
    "lex = create_lexicon(lexicon_name).lemma(search_word).search()\n",
    "df_lexicon = lex.kwic()\n",
    "display_df(df_lexicon)\n",
    "#df_columns_list = list(df_lexicon.columns.values)\n",
    "#df_lexicon_in_columns = df_lexicon[df_columns_list]\n",
    "#display(df_lexicon_in_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus + lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve synonyms from DiaMaNT, look up in Gysseling <a class=\"anchor\" id=\"synonyms-diamant-gysseling\"></a>\n",
    "* Below cell searches for term \"boek\" in DiaMaNT, and looks up all variants in Gysseling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T11:14:56.092757Z",
     "start_time": "2019-03-21T11:14:46.126920Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.search.LexiconQuery import *\n",
    "from IPython.core.display import display, HTML\n",
    "from chaininglib.search.corpusQueries import corpus_query\n",
    "from chaininglib.process.lexicon import get_diamant_synonyms\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "search_word = \"boek\"\n",
    "lexicon_name = \"diamant\"\n",
    "corpus= \"gysseling\"\n",
    "\n",
    "# First, lookup synonyms in DiaMaNT\n",
    "lq = create_lexicon(lexicon_name).lemma(search_word).search()\n",
    "df_lexicon = lq.kwic()\n",
    "\n",
    "syns = get_diamant_synonyms(df_lexicon)\n",
    "syns.add(search_word) # Also add search word itself\n",
    "display(HTML('Synoniemen voor <b>' + search_word + '</b>: ' + \", \".join(syns)))\n",
    "\n",
    "# Search for all synonyms in corpus\n",
    "## Create queries: search by lemma\n",
    "syns_queries = [corpus_query(lemma=syn) for syn in syns]\n",
    "\n",
    "## Search for all synonyms in corpus\n",
    "df = pd.DataFrame()\n",
    "for one_pattern in syns_queries:\n",
    "    cq = create_corpus(corpus).pattern(one_pattern).search()\n",
    "    df = df.append(cq.kwic())\n",
    "display_df(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Build a frequency list of the lemma of some corpus output <a class=\"anchor\" id=\"freq-lemma-corpus\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T11:10:04.501764Z",
     "start_time": "2019-03-21T11:09:17.928456Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.process.corpus import *\n",
    "from chaininglib.ui.dfui import *\n",
    "\n",
    "# do some corpus search\n",
    "print('This can take a few seconds... please wait!')\n",
    "\n",
    "corpus_to_search=\"zeebrieven\"\n",
    "df_corpus = create_corpus(corpus_to_search).detailed_context(True).pos(\"NOU.*\").search().kwic()\n",
    "\n",
    "# compute and display a table of the frequencies of the lemmata\n",
    "\n",
    "freq_df = get_frequency_list(df_corpus)\n",
    "display_df(freq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T15:46:20.519833Z",
     "start_time": "2019-01-16T15:46:20.516208Z"
    }
   },
   "source": [
    "### Find occurences of attributive adjectives not ending with -e, even though they are preceeded by a definite article <a class=\"anchor\" id=\"adjective-e\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T11:15:32.231218Z",
     "start_time": "2019-03-21T11:14:56.094365Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.search.LexiconQuery import *\n",
    "from chaininglib.utils.dfops import df_filter\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "corpus_to_search=\"chn-extern\"\n",
    "lexicon_to_search=\"molex\"\n",
    "\n",
    "# CORPUS: get [article + attributive adjective + nouns] combinations in which the adjective does not end with -e\n",
    "print('Get occurences of attributive adjectives not ending with -e')\n",
    "cq = create_corpus(corpus_to_search).pattern(r'[lemma=\"de|het\"][word=\"^g(.+)[^e]$\" & pos=\"AA.*\"][pos=\"NOU.*\"]')\n",
    "df_corpus = cq.search().kwic()\n",
    "display(df_corpus)\n",
    "\n",
    "# LEXICON: get adjectives the lemma of which does not end with -e\n",
    "lq = create_lexicon(lexicon_to_search).lemma('^g(.+)[^e]$').pos('ADJ').search()\n",
    "df_lexicon = lq.search().kwic()\n",
    "\n",
    "# LEXICON: get adjectives having a final -e in definite attributive use\n",
    "print('Filtering lexicon results')\n",
    "final_e_condition = df_filter(df_lexicon[\"wordform\"], 'e$')\n",
    "df_lexicon_form_e = df_lexicon[ final_e_condition ]\n",
    "\n",
    "# RESULT: get the records out of our first list in which the -e-less-adjectives match the lemma form of our last list\n",
    "print('List of attributive adjectives not ending with -e even though they should have a final -e:')\n",
    "e_forms = list(df_lexicon_form_e.lemma)\n",
    "no_final_e_condition = df_filter(df_corpus[\"word 1\"], pattern=set(e_forms), method=\"isin\")\n",
    "result_df = df_corpus[ no_final_e_condition ]\n",
    "display_df( result_df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look up inflected forms and spelling variants for a given lemma in a corpus <a class=\"anchor\" id=\"inflected-spelling-corpus\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T11:15:32.702930Z",
     "start_time": "2019-03-21T11:15:32.232593Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.ui.dfui import display_df\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.search.LexiconQuery import *\n",
    "\n",
    "# Corpus Gysseling and lexicon mnwlex are from same period: 1250-1550\n",
    "lexicon_to_search=\"mnwlex\"\n",
    "corpus_to_search=\"gysseling\"\n",
    "\n",
    "##############################################\n",
    "# TODO  zelfde met meerdere lemmata en gegroepeerd \n",
    "##############################################\n",
    "\n",
    "lemma_to_look_for=\"denken\"\n",
    "\n",
    "# LEXICON: Search for the inflected forms of a lemma in a morphosyntactic lexicon\n",
    "lq = create_lexicon(lexicon_to_search).lemma(lemma_to_look_for).search()\n",
    "df_lexicon = lq.kwic()\n",
    "display_df(df_lexicon)\n",
    "\n",
    "# Put all inflected forms into a list\n",
    "inflected_wordforms = list(df_lexicon.wordform)\n",
    "\n",
    "# CORPUS: Look up the inflected forms in a (possibly unannotated) corpus\n",
    "# beware: If the corpus is not annotated, all we can do is searching for the inflected words\n",
    "#         But if the corpus is lemmatized, we have to make sure we're retrieving correct data by specifying the lemma as well\n",
    "annotated_corpus = True\n",
    "query = r'[lemma=\"'+lemma_to_look_for+r'\" & word=\"'+r\"|\".join(inflected_wordforms)+r'\"]' if annotated_corpus else r'[word=\"'+r\"|\".join(inflected_wordforms)+r'\"]'\n",
    "cq = create_corpus(corpus_to_search).pattern(query).search()\n",
    "df_corpus = cq.kwic() \n",
    "display_df(df_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus frequency list of lemmata from lexicon with given lemma <a class=\"anchor\" id=\"corpus-frequency-lemma-pos\"></a>\n",
    "Build a function with which we can gather all lemmata of a lexicon, and build a frequency list of those lemmata in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T11:15:49.920956Z",
     "start_time": "2019-03-21T11:15:32.705184Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.LexiconQuery import *\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.process.corpus import get_frequency_list\n",
    "from chaininglib.ui.dfui import display_df\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# build a function as required. We will run it afterwards\n",
    "\n",
    "def get_frequency_list_given_a_corpus(lexicon, pos, corpus):\n",
    "    \n",
    "    # LEXICON: get a lemmata list to work with\n",
    "\n",
    "    # query the lexicon for lemma with a given part-of-speech\n",
    "    lq = create_lexicon(lexicon).pos(pos).search()\n",
    "    df_lexicon = lq.kwic()\n",
    "\n",
    "    # Put the results into an array, so we can loop through the found lemmata\n",
    "    lexicon_lemmata_arr = [w.lower() for w in df_lexicon[\"writtenForm\"]][-200:]\n",
    "    # Instantiate a DataFrame, in which we will gather all single lemmata occurences\n",
    "    df_full_list = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # CORPUS: loop through the lemmata list, query the corpus with each lemma, and count the results\n",
    "\n",
    "    # It's a good idea to query more than one lemma at at the time,\n",
    "    # but not too many, otherwise the server will get overloaded!\n",
    "    nr_of_lemmata_to_query_atonce = 100\n",
    "\n",
    "    # loop over lemma list \n",
    "    for i in range(0, len(lexicon_lemmata_arr), nr_of_lemmata_to_query_atonce):\n",
    "        \n",
    "        print('Lemmata processed: '+str(i)+'/'+str(len(lexicon_lemmata_arr)))\n",
    "        \n",
    "        # slice to small array of lemmata to query at once\n",
    "        small_lemmata_arr = lexicon_lemmata_arr[i : i+nr_of_lemmata_to_query_atonce] \n",
    "\n",
    "        # join set of lemmas to send them in a query all at once\n",
    "        # beware: single quotes need escaping\n",
    "        lemmata_list = \"|\".join(small_lemmata_arr).replace(\"'\", \"\\\\\\\\'\")\n",
    "        cq = create_corpus(corpus).pattern(r'[lemma=\"' + lemmata_list + r'\"]').search()\n",
    "        df_corpus = cq.kwic()\n",
    "        \n",
    "        # add the results to the full list\n",
    "        if \"lemma 0\" in df_corpus.columns:\n",
    "            df_full_list = pd.concat( [df_full_list, df_corpus[\"lemma 0\"]] )     \n",
    "        \n",
    "\n",
    "    # make sure the columnswith that contains the lemmata is same as given to get_frequency_list function\n",
    "    column_name=\"lemma\"\n",
    "    df_full_list.columns = [column_name]\n",
    "\n",
    "    # we're done with querying, build the frequency list now\n",
    "    print('Done.')\n",
    "    freq_df = get_frequency_list(df_full_list, column_name=column_name)\n",
    "    \n",
    "\n",
    "    return freq_df\n",
    "\n",
    "    \n",
    "# run it!\n",
    "lexicon=\"molex\"\n",
    "# TODO: Maybe too much too ask all nouns? Maybe take random sample?\n",
    "corpus_to_search=\"chn-extern\"\n",
    "pos=\"NOU.*\"\n",
    "\n",
    "freq_df = get_frequency_list_given_a_corpus(lexicon, pos, corpus_to_search)\n",
    "\n",
    "display_df(freq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a frequency table of some corpus, based on lemma list of a given lexicon <a class=\"anchor\" id=\"freqtable-lemmalist\"></a>\n",
    "In this case study, we compare lemma frequencies for corpora from different time periods: CHN extern (contemporary Dutch Antilles & Suriname) and Letters as Loot (sailors' letters, 17th and 18th century).\n",
    "\n",
    "*For this case study, you need to run the previous case study first, because it generates a function we need here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T11:16:57.899540Z",
     "start_time": "2019-03-21T11:15:49.922813Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.utils.dfops import get_rank_diff\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "# For this case study, you need to run the previous case study first, because it generates a function we need here\n",
    "\n",
    "# Use lexica and corpora from same period\n",
    "base_lexicon1=\"molex\"\n",
    "corpus_to_search1=\"chn-extern\"\n",
    "\n",
    "base_lexicon2=\"molex\"\n",
    "corpus_to_search2=\"zeebrieven\"\n",
    "\n",
    "# ADJ gives interesting comparison\n",
    "\n",
    "pos=\"ADJ.*\"\n",
    "\n",
    "# build frequency tables of two corpora\n",
    "\n",
    "df_frequency_list1 = get_frequency_list_given_a_corpus(base_lexicon1, pos, corpus_to_search1)\n",
    "# sort and display\n",
    "df_top25_descending = df_frequency_list1.sort_values(ascending=False,by=['token count']).head(25)\n",
    "df_top25_ascending =  df_frequency_list1.sort_values(ascending=True, by=['rank']).head(25)\n",
    "display_df( df_top25_descending[['lemmas', 'token count']].set_index('lemmas'), labels='df1 chart '+corpus_to_search1, mode='chart' )\n",
    "\n",
    "df_frequency_list2 = get_frequency_list_given_a_corpus(base_lexicon2, pos, corpus_to_search2)\n",
    "# sort and display\n",
    "df_top25_descending = df_frequency_list2.sort_values(ascending=False,by=['token count']).head(25)\n",
    "df_top25_ascending =  df_frequency_list2.sort_values(ascending=True, by=['rank']).head(25)\n",
    "display_df( df_top25_descending[['lemmas', 'token count']].set_index('lemmas'), labels='df2 chart '+corpus_to_search2, mode='chart' )\n",
    "\n",
    "\n",
    "# TODO: lemmata tonen die in 1 of 2 ontbreken\n",
    "\n",
    "# compute the rank diff of lemmata in frequency tables\n",
    "\n",
    "# sort and display\n",
    "df_rankdiffs = get_rank_diff(df_frequency_list1, df_frequency_list2, index='lemmas')\n",
    "\n",
    "display_df(df_rankdiffs.sort_values(by=['rank_diff']).head(25), labels='Differences in ranks')\n",
    "\n",
    "df_top25_descending = df_rankdiffs.sort_values(ascending=False, by=['rank_diff']).head(25)\n",
    "display_df( df_top25_descending['rank_diff'], labels='chart large diff', mode='chart' )\n",
    "\n",
    "df_top25_ascending = df_rankdiffs.sort_values(ascending=True, by=['rank_diff']).head(25)\n",
    "display_df( df_top25_ascending['rank_diff'], labels='chart small diff', mode='chart' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search corpus for wordforms of lemma not included in lexicon <a class=\"anchor\" id=\"corpus-wordforms-not-lexicon\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T11:17:09.521210Z",
     "start_time": "2019-03-21T11:16:57.901052Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.LexiconQuery import *\n",
    "from chaininglib.search.CorpusQuery import *\n",
    "from chaininglib.ui.dfui import display_df\n",
    "\n",
    "# Let's build a function to do the job:\n",
    "# The function will require a lexicon name and a part-of-speech to limit the search to, and the name of a corpus to be searched.\n",
    "# It will return a Pandas DataFrame associating lemmata to their paradigms ('known_wordforms' column) and\n",
    "# missing wordforms found in the corpus ('unknown_wordforms' column).\n",
    "\n",
    "def get_missing_wordforms(lexicon_name, lexicon_postag, corpus, corpus_postag):    \n",
    "    \n",
    "    print('Finding missing wordforms in a lexicon can take some time...');\n",
    "    \n",
    "    # LEXICON: \n",
    "    # get a lemmata list having a given part-of-speech\n",
    "    # MoLex is a convenient source to get a list of lemmata\n",
    "    \n",
    "    lq = create_lexicon(\"molex\").pos(lexicon_postag).search()\n",
    "    df_lexicon = lq.kwic()\n",
    "    \n",
    "    # Put the results into an array, so we can loop through the list of lemmata\n",
    "    lexicon_lemmata_arr = [w.lower() for w in df_lexicon[\"writtenForm\"]][-50:]\n",
    "    \n",
    "    # Test array, instead of querying Molex\n",
    "    #lexicon_lemmata_arr = [\"denken\", \"doen\", \"hebben\", \"maken\"]\n",
    "    \n",
    "    # Prepare the output:\n",
    "    # instantiate a DataFrame for storing lemmata and mssing wordforms\n",
    "    df_enriched_lexicon = pd.DataFrame(index=lexicon_lemmata_arr, columns=['lemma', 'pos', 'known_wordforms', 'unknown_wordforms'])\n",
    "    df_enriched_lexicon.index.name = 'lemmata'\n",
    "    \n",
    "    # CORPUS: \n",
    "    # loop through the lemmata list, query the corpus for each lemma, \n",
    "    # and compute paradigms differences between both\n",
    "\n",
    "    \n",
    "    # loop through the lemmata list\n",
    "    # and query the corpus for occurances of the lemmata\n",
    "    \n",
    "    # It's a good idea to work with more than one lemma at the time (speed)!\n",
    "    nr_of_lemmata_to_query_atonce = 100\n",
    "    \n",
    "    for i in range(0, len(lexicon_lemmata_arr), nr_of_lemmata_to_query_atonce):\n",
    "        \n",
    "        # slice to small array of lemmata to query at once\n",
    "        small_lemmata_arr = lexicon_lemmata_arr[i : i+nr_of_lemmata_to_query_atonce]\n",
    "        \n",
    "        # join set of lemmata to send them in a query all at once\n",
    "        # beware: single quotes need escaping\n",
    "        lemmata_list = \"|\".join(small_lemmata_arr).replace(\"'\", \"\\\\\\\\'\")\n",
    "        print(\"Querying lemmata %i-%i of %i from corpus.\" % (i, i+nr_of_lemmata_to_query_atonce, len(lexicon_lemmata_arr) ))\n",
    "        cq = create_corpus(corpus).pattern(r'[lemma=\"' + lemmata_list + r'\" & pos=\"'+corpus_postag+'\"]').search()\n",
    "        df_corpus = cq.kwic()\n",
    "        \n",
    "        # if the corpus gave results,\n",
    "        # query the lexicon for the same lemmata\n",
    "        # and compare the paradigms!\n",
    "        \n",
    "        if (len(df_corpus)>0):\n",
    "            small_lemmata_set = set(small_lemmata_arr)\n",
    "            for one_lemma in small_lemmata_set: \n",
    "                \n",
    "                # look up the known wordforms in the lexicon\n",
    "                ql = create_lexicon(lexicon_name).lemma(one_lemma).search()\n",
    "                df_known_wordforms = ql.kwic()\n",
    "                \n",
    "                # we have a lexicon paradigm to compare, do the job now\n",
    "                if (len(df_known_wordforms) != 0):\n",
    "                    \n",
    "                    # gather the lexicon wordforms in a set\n",
    "                    known_wordforms = set( df_known_wordforms['wordform'].str.lower() )\n",
    "                    \n",
    "                    # gather the corpus wordforms (of the same lemma) in a set too\n",
    "                    corpus_lemma_filter = (df_corpus['lemma 0'] == one_lemma)\n",
    "                    corpus_wordforms = set( (df_corpus[ corpus_lemma_filter ])['word 0'].str.lower() )\n",
    "                    \n",
    "                    # Now compute the differences:\n",
    "                    # gather in a set all the corpus wordforms that cannot be found in the lexicon wordforms \n",
    "                    unknown_wordforms = corpus_wordforms.difference(known_wordforms)\n",
    "\n",
    "                    # If we found some missing wordforms, add the results to the output!\n",
    "                    \n",
    "                    if (len(unknown_wordforms) !=0):                        \n",
    "                        # The index of our results will be a key consisting of lemma + part-of-speech\n",
    "                        # Part-of-speech is needed to distinguish homonyms with different grammatical categories.\n",
    "                        # Of course, we need to take glosses into account too to do a truely correct job\n",
    "                        # But we didn't do it here\n",
    "                        key = one_lemma + lexicon_postag\n",
    "                        df_enriched_lexicon.at[key, 'lemma'] = one_lemma\n",
    "                        df_enriched_lexicon.at[key, 'pos'] = lexicon_postag\n",
    "                        df_enriched_lexicon.at[key, 'known_wordforms'] = known_wordforms\n",
    "                        df_enriched_lexicon.at[key, 'unknown_wordforms'] = unknown_wordforms\n",
    "                \n",
    "    # return non-empty results, t.i. cases in which we found some wordforms\n",
    "    return df_enriched_lexicon[ df_enriched_lexicon['unknown_wordforms'].notnull() ]\n",
    "\n",
    "\n",
    "# Run the function!\n",
    "#\n",
    "# ask the lexicon which wordforms it knows, and try to find new unknown wordforms in the corpus\n",
    "\n",
    "lexicon_name=\"mnwlex\"\n",
    "corpus_to_search=\"zeebrieven\"\n",
    "\n",
    "# beware: lexicon and corpus may have different parts-of-speech sets in use\n",
    "df = get_missing_wordforms(lexicon_name, \"VERB\", corpus_to_search, \"VRB\")\n",
    "\n",
    "# After such a heavy process, it's a good idea to save the results\n",
    "\n",
    "df.to_csv( \"missing_wordforms.csv\", index=False)\n",
    "\n",
    "display_df(df, labels='Missing wordforms')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treebanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treebank search <a class=\"anchor\" id=\"treebank-search\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T11:17:29.617434Z",
     "start_time": "2019-03-21T11:17:09.522677Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.TreebankQuery import *\n",
    "\n",
    "\n",
    "print (\"search...\")\n",
    "tbq = create_treebank(\"cgn\").pattern(\"//node[@cat='pp' and node[@cat='ap' and node[@cat='np']]]\").search()\n",
    "\n",
    "print (\"get XML...\")\n",
    "\n",
    "xml = tbq.xml()\n",
    "print(xml)\n",
    "\n",
    "print (\"get trees and their string representations...\")\n",
    "\n",
    "trees = tbq.trees()\n",
    "\n",
    "for tree in trees:\n",
    "    display(tree.toString())\n",
    "\n",
    "df = tbq.kwic()\n",
    "    \n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which kind of nouns are used in a prepositional complement of the verb *geven* ? <a class=\"anchor\" id=\"treebank-objects-geven\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T11:17:43.637212Z",
     "start_time": "2019-03-21T11:17:29.618818Z"
    }
   },
   "outputs": [],
   "source": [
    "from chaininglib.search.TreebankQuery import *\n",
    "\n",
    "\n",
    "print (\"search...\")\n",
    "\n",
    "tbq = create_treebank(\"cgn\").pattern('//node[node[@rel=\"hd\" and @pt=\"ww\" and @root=\"geven\"] and node[@rel=\"obj1\" and @pt=\"n\"]]').search()\n",
    "\n",
    "\n",
    "print (\"get list of nouns which are part of an PP, as argument of predicate 'geven'...\")\n",
    "\n",
    "trees = tbq.trees()\n",
    "\n",
    "list_of_nouns = []\n",
    "for tree in trees:\n",
    "    nouns = tree.extract(['pp', 'np'])\n",
    "    list_of_nouns = list_of_nouns + nouns\n",
    "    \n",
    "\n",
    "display(list_of_nouns)\n",
    "    \n",
    "df = tbq.kwic(align_lemma='geven')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
